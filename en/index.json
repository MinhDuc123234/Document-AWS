[
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AI-Powered Cloud Solutions \u0026amp; Amazon Bedrock Workshop” Event Objectives Introduce Foundation Models and the difference between traditional ML and Generative AI Provide practical knowledge of Prompt Engineering and RAG Explore AWS AI Services and their real-world applications Present Amazon Bedrock AgentCore for building scalable AI agents Share career advice and the importance of building real AI products for portfolios Speakers Lam Tuan Kiet – Sr. DevOps Engineer, FPT Software Dang Hoang Hieu Nghi – AI Engineer, Reonova Cloud Dinh Le Hoang Anh – Cloud Engineer Trainee, FCJ FPT Company Representative – Insights on enterprise adoption of AI and cloud-based product development Key Highlights Understanding Foundation Models vs Traditional ML Traditional ML models: Single-task, require labeled data, limited generalization Foundation Models: Trained on large unlabeled datasets, self-supervised, support multiple tasks, enabled by services like Amazon Bedrock Bedrock supports multiple models including OpenAI and DeepSeek Prompt Engineering Techniques Zero-shot prompting: Minimal instructions → simple, concise outputs Few-shot prompting: Provide examples to guide the model Chain-of-Thought prompting: Encourage step-by-step reasoning for improved accuracy Example discussed: The question “What is 10 + 10?” can generate ambiguous interpretations without proper context Retrieval Augmented Generation (RAG) Retrieves relevant information from data sources before generating answers Enhances accuracy, reduces hallucination, and supports custom enterprise data Embeddings convert text into vectors; AWS Titan Text Embeddings supports over 100 languages Demonstration of RAG workflow in practice AWS AI Services Overview Rekognition – Image/video analysis Translate – Auto language detection \u0026amp; translation Textract – Extract text \u0026amp; document structure Transcribe – Speech-to-text Polly – Text-to-speech Comprehend – NLP insights, sentiment analysis Kendra – Intelligent search over documents Lookout Family – Detect anomalies in metrics, equipment, and vision Personalize – Personalized recommendations Pipecat – Pipeline framework for AI agents All services are accessible via simple API calls Amazon Bedrock AgentCore Platform for developing AI agents without heavy DevOps requirements Addresses challenges in scaling, memory management, identity handling, and tool integration Key mechanisms include: Runtime, Memory, Identity, Gateway, Code Interpreter, Browser Tool, Observability Designed for building production-ready AI assistants and workflow automation Key Takeaways Design Mindset Building real projects is crucial — not just academic exercises Enterprises increasingly focus on developing AI-powered cloud products Understanding business needs is key to creating meaningful AI solutions Technical Architecture Foundation Models offer versatility beyond traditional ML Prompt engineering directly influences model accuracy and reliability RAG improves contextual accuracy by combining prompts with enterprise data AWS AI Services accelerate development and reduce operational overhead Modernization Strategy Use Bedrock models for scalable, multi-purpose AI features Integrate embeddings and RAG for enterprise-grade applications Adopt AgentCore to simplify the process of deploying complex AI agents Applying to Work Build small prototypes using Amazon Bedrock and AWS AI Services Experiment with prompt engineering techniques to improve outcomes Apply RAG to enhance internal chatbot or automation systems Use AgentCore to create AI agents capable of multi-step decision-making Add completed AI projects to portfolio/CV as recommended by speakers Event Experience Attending the “AI-Powered Cloud Solutions \u0026amp; Amazon Bedrock Workshop” provided extensive insights into modern AI development and cloud-based architectures.\nLearning from industry speakers Experts shared practical knowledge and clarified differences between traditional ML and modern Foundation Models Real-world examples helped illustrate how enterprises adopt AI at scale Hands-on technical exposure Demonstrations of embeddings, RAG, and prompt engineering Understanding when to use Zero-shot, Few-shot, or Chain-of-Thought prompts Clear explanations of how AWS AI services integrate into product workflows Leveraging modern tools Exposure to the Bedrock ecosystem and its multi-model capabilities Understanding AgentCore for building and scaling AI agents Learning how to use APIs for quick integration in real projects Networking and discussions Encouragement from speakers to build actual AI products for CVs Discussions reinforced the growing demand for cloud-based AI solutions in companies Lessons learned Foundation Models enhance flexibility compared to traditional ML RAG is essential for building AI systems with factual grounding Real product-building experience is extremely valuable for career growth Event Photo Figure 1\rFigure 2\rFigure 3\rOverall, the workshop offered strong technical foundations, practical insights, and motivation to build real-world AI products using AWS technologies.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AWS Cloud, AI \u0026amp; Innovation Summit” Event Objectives Present national strategies for cloud expansion and digital transformation Strengthen the U.S.–Vietnam partnership in technology and innovation Share insights on AI, blockchain, and ecosystem development shaping Vietnam’s future Highlight AWS initiatives in talent development, cloud accessibility, and responsible AI Provide hands-on technical knowledge in AI-driven development and AI security Speakers Vietnam Government Representative U.S. Ambassador to Vietnam Eric Elock – CEO for Vietnam, Laos, Cambodia \u0026amp; Myanmar Chloe Phung – CEO, U2U Erik – AWS Leadership Jaime Valless – AWS Leadership AWS Technical Experts – Afternoon technical sessions Key Highlights National strategy on cloud infrastructure and digital transformation Government emphasized expanding cloud computing and digital systems as the foundation for Industry 4.0 Ensures security, safety, and information protection across all digital services Enables open collaboration for public sector, private enterprises, and foreign investors Cloud is positioned as a strategic enabler for economic growth and national modernization U.S.–Vietnam relationship and technological development The U.S. Ambassador highlighted the 30-year partnership between the two nations Technology companies like AWS serve as a bridge accelerating co-development Emphasis on mutual economic benefits and long-term cooperation Innovation through banking support and blockchain ecosystems – Eric Elock The banking sector plays a key role in supporting IT modernization U2U builds an ecosystem enabling businesses and users to interact through blockchain Demonstrates how cloud + blockchain shape new digital economic models AI shaping Vietnam’s future – Chloe Phung Two years ago, partners said U2U’s ideas were “impossible,” but they have become reality Vietnam is not only catching up with global AI trends but helping shape the revolution Real impacts of AI in Vietnam Education:\n60% of Vietnamese students use EdTech apps AI breaks language barriers and boosts learning engagement Economy:\nOver 765 AI startups, ranking 2nd in ASEAN Expected GDP contribution: $120–130 billion Social impact:\nHospitals using AI reduced examination time to 5 minutes per case AI improves traffic management, energy views, and coastal protection Technology examples Nubila – AI-powered weather prediction Staex – Successfully deployed 1,000+ IoT devices across Asia and Europe AI \u0026amp; blockchain synergy Generative AI reduces development time from weeks to hours or days Blockchain becomes more accessible for beginners with AI assistance AI helps businesses and policymakers make better daily decisions Appreciation extended to AWS for enabling the ecosystem AWS initiatives for Vietnam – Erik AWS has trained over 100,000 cloud builders in Vietnam Working to make cloud services more accessible nationwide Introduced the FJC 6-month program, providing secure job pathways Highlighted AWS culture as a key differentiator Where culture meets innovation – Jaime Valless Humanity is at a unique moment where AI will transform all industries AI transformation is not only about technology but also skills, people, culture, and responsibility Encouraged continuous learning and responsible AI usage AWS supports secure deployment with multi-model access and protection mechanisms Use case example Nearmap: Uses AI models to help customers make faster decisions by automating repetitive tasks, allowing humans to focus on creativity and critical thinking Key Takeaways Design Mindset Cloud, AI, and blockchain together create powerful new innovation opportunities Collaboration between government, enterprise, and global partners accelerates digital transformation AI should be deployed responsibly with strong security and human oversight Technical Architecture AI accelerates development cycles through automated coding and testing IoT and weather-modeling use cases show how AI scales across industries Generative AI lowers the barriers to understanding complex systems like blockchain Modernization Strategy Adopt cloud + AI + blockchain for high-impact transformation Invest in continuous learning and cloud-AI skill development Maintain responsible AI practices: access control, hallucination prevention, secure prompts, data protection Applying to Work Explore AI-driven development workflows using AWS tools Apply responsible-AI practices: user tracking, human-in-the-loop, prompt protection, data validation Consider RAG for secure and accurate enterprise AI outputs Experiment with AWS services like Amazon Q and QuickSight for faster prototyping and dashboarding Event Experience Attending the “AWS Cloud, AI \u0026amp; Innovation Summit” provided valuable insights into how cloud, AI, and blockchain are shaping Vietnam’s digital transformation.\nLearning from industry leaders Government, international representatives, and AWS leadership shared strategic visions for Vietnam Real-world examples helped illustrate how AI can transform education, economy, and society Hands-on technical exposure Afternoon session included AWS SageMaker, AI-driven SDLC, and AI application security Demonstrated a full AI-enhanced development lifecycle: Inception: ideation, requirement definition Construction: domain modeling, code generation, testing, IaC deployment Operation: production deployment, incident management Security practices for AI applications Addressed AI risks such as hallucination, data poisoning, prompt security, and access control Highlighted architectures for secure AI deployments, supply-chain validation, user tracking, and using RAG Leveraging AWS tools Amazon Q and QuickSight simplify dashboard creation and workflow automation AI assists with coding, documentation, and refactoring, allowing developers to stay in control while accelerating delivery Lessons learned AI and cloud enable efficiency and innovation across multiple industries Strong security, responsible practices, and human oversight are essential Vietnam has significant momentum and potential in AI and digital transformation Event Photo Figure 1\rFigure 2\rFigure 3\rOverall, the summit provided strategic insights, technical knowledge, and real-world examples demonstrating how AI, cloud computing, and blockchain are shaping Vietnam’s future.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Trellix achieves 35% cost savings and strengthens security with Amazon OpenSearch Service By Leeneksh Dubey, Harsh Bansal, and Prashant Agrawal, September 19, 2025 | Amazon OpenSearch Service, Customer Solutions, Intermediate (200).\nThis is a guest post by Leeneksh Dubey, Cloud Engineer at Trellix, in collaboration with AWS.\nTrellix, a leading provider of cybersecurity solutions, emerged in 2022 from the merger of McAfee Enterprise and FireEye. Serving more than 40,000 enterprise customers worldwide, Trellix delivers one of the most comprehensive and open AI-powered security platforms for enterprises. Its capabilities help organizations build resilience against advanced threats through automated detection, investigation, and response.\nToday, security teams face an increasingly complex threat landscape while the volume of security and application logs continues to grow rapidly. Due to limited resources and staffing, many teams struggle to investigate security data effectively, increasing the risk of new threats going undetected. Trellix addresses these challenges by unifying endpoint, network, cloud, and email security tools into a single AI-assisted platform. Automated threat detection, investigation, and response enables security teams to identify and remediate potential threats faster while reducing operational complexity.\nTo address exponential log growth across a multi-tenant, multi-Region infrastructure, Trellix used Amazon OpenSearch Service, Amazon OpenSearch Ingestion, and Amazon Simple Storage Service (Amazon S3) to modernize its logging platform. Previously, Trellix faced multiple challenges operating self-managed Elasticsearch clusters on Amazon Elastic Compute Cloud (Amazon EC2). Migrating to managed OpenSearch Service significantly improved operational efficiency. This strategy enables Trellix to process terabytes of security data per day across multiple AWS Regions, while delivering a 35% reduction in storage costs as of Q3 2024. Moving to a managed service also saves up to 10 hours of infrastructure maintenance per week, allowing developers to focus more on higher-value work.\nIn this post, we share how Trellix leveraged AWS solutions to improve performance, availability, and scalability while reducing operational costs.\nSolution overview Trellix’s innovative log management solution, built on AWS services, addresses the challenge of processing large volumes of security data across multiple Regions. This enterprise-grade architecture demonstrates how organizations can effectively manage security logs at scale while optimizing costs. The solution focuses on three key business challenges: efficient long-term log retention, scalable distribution for analytics and alerting, and cost-optimized storage across a multi-Region footprint. The architecture is illustrated in the diagram below, showing how Trellix manages security logs at scale while optimizing costs.\nFigure 1\rTrellix’s AWS-based security log management solution implements an end-to-end data workflow that seamlessly handles collection, processing, storage, and analysis. In the following sections, we dive deeper into the six steps of this workflow.\nStep 1: Upload data to Amazon S3\nThe solution begins with data collection, leveraging Amazon S3’s globally distributed infrastructure and high scalability. Raw security and application logs are collected from multiple deployments across different Regions, allowing Trellix to maintain data sovereignty while enabling low-latency access across jurisdictions. These logs are then processed by Trellix’s internal tooling, where they are enriched using proprietary security logic. The enriched dataset is stored back in Amazon S3, forming a secure, scalable foundation for downstream security analytics and processing.\nStep 2: Trigger Amazon SNS notifications from S3 events\nAfter the data is enriched and successfully stored in Amazon S3, the system initiates an event-driven automation chain. Amazon S3 is configured to publish event notifications to an Amazon Simple Notification Service (Amazon SNS) topic whenever new data is uploaded. Amazon SNS acts as a notification hub, efficiently fan-out distributing these events to subscribed services or endpoints. This approach keeps the architecture decoupled and flexible, enabling multiple consumers to receive real-time alerts as soon as new data appears.\nStep 3: Queue messages in Amazon SQS\nNext, SNS notifications are routed to Amazon Simple Queue Service (Amazon SQS), which provides a durable, scalable queuing layer between data producers and consumers. The queue serves as a buffer, reliably and asynchronously delivering event metadata to downstream processing components. Using Amazon SQS adds message retention and fault tolerance—especially valuable during high load or failures—allowing OpenSearch Ingestion to process incoming data in a controlled and resilient manner.\nStep 4: Automated data processing with OpenSearch Ingestion OpenSearch Ingestion continuously polls the SQS queue for new messages indicating that data is ready in Amazon S3. When it receives these messages, it uses built-in integrations to retrieve the data directly from Amazon S3. After retrieval, the ingestion pipeline performs the required transformations before forwarding the data to the OpenSearch Service domain. To optimize cost and performance, Trellix selected OR1 instances for its OpenSearch deployment. These instances provide a high memory-to-vCPU ratio and are specifically optimized for indexing- and search-intensive workloads, making them ideal for large-scale log analytics.\nStep 5: Log lifecycle management with Index State Management To optimize storage usage and manage data lifecycle, Trellix implemented Index State Management (ISM) policies in OpenSearch Service. These policies automate the lifecycle of ingested log data by moving it through defined phases based on age and access patterns. Initially, logs remain in the hot tier for up to 24 hours, enabling immediate access for real-time security analytics. After this period, logs are automatically moved to UltraWarm storage, a more cost-effective option that still supports querying. Finally, once the configured retention period ends, ISM deletes the data from the system. This fully automated lifecycle approach balances performance, compliance, and cost optimization.\nStep 6: Comprehensive monitoring and visualization By using Amazon CloudWatch for end-to-end monitoring—combined with Trellix’s internal automation via OpenSearch public APIs for custom monitoring—this solution delivers full observability through integrated visualization tools. OpenSearch Dashboards enables security teams to analyze logs and perform powerful searches to investigate security events and identify potential threats. In addition, the solution uses Amazon Managed Grafana to create custom dashboards that track both the health of the data pipeline and the performance of the OpenSearch cluster.\nThis dual-visualization approach provides real-time security event monitoring and analysis, comprehensive infrastructure performance metrics, automated alerts for faster threat response, customizable dashboards for different security operations needs, and a unified view across multi-Region deployments. Together, these tools form a robust observability framework that helps Trellix maintain a strong security posture while supporting optimal performance across its global infrastructure.\nKey benefits Trellix’s adoption of OpenSearch Service as a logging solution delivered three major benefits that significantly transformed its cybersecurity operations.\n1. Simplified log management architecture\nTrellix streamlined its security operations by deploying a unified log management architecture, eliminating the complexity of maintaining multiple disconnected tools. By using OpenSearch Ingestion—a fully managed, serverless data pipeline—Trellix simplified real-time security data processing. Integration with Managed Grafana adds a unified visualization layer, enabling security teams to focus on threat detection rather than infrastructure management.\n2. Scalability and resilience\nOpenSearch Service enabled Trellix to achieve unprecedented scalability and resilience. Trellix’s architecture uses OpenSearch Ingestion pipelines to handle log-volume spikes across Regional deployments. OpenSearch Ingestion supports dynamic scaling with automatic resource optimization, making it easier to manage capacity as data grows. This capability helps Trellix maintain consistent performance even during spikes in security events. The solution also implements a robust Multi-AZ deployment strategy to maximize resilience and ensure service continuity. In self-healing tests, the architecture demonstrated recovery in under 9 minutes when a node was restarted—showing strong business continuity even during node failures. Automatic failover reduces disruption to security operations, ensuring continuous monitoring of customers’ security posture. In addition, the solution uses automated Amazon S3 backups combined with hourly snapshots to enable point-in-time recovery. Each Region maintains additional copies of customer data, creating a layered data-protection strategy to ensure integrity and availability of critical security information.\n3. Easier scaling with optimized cost\nExponential growth in security data processing required a solution that can scale flexibly while maintaining cost efficiency. The combination of Amazon S3 and OpenSearch Service with UltraWarm storage provided the foundation for this scalable architecture. UltraWarm is a fully managed intermediate storage tier for OpenSearch Service that changes how Trellix manages large volumes of security data across multiple Regions. The solution leverages UltraWarm’s innovative architecture, using Amazon S3 for durable storage while maintaining fast query performance for security analytics. A key benefit of UltraWarm’s Amazon S3-based architecture is eliminating the need for index replicas, which significantly reduces cluster size and related costs while maintaining data durability. An intelligent log classification framework underpins Trellix’s data management strategy by routing incoming data based on security priority. This systematic approach enables efficient routing of P2 and P3 log sources, optimized processing flows for different security priorities, reduced load on the primary SIEM infrastructure, and customized handling based on customer requirements. This deployment is particularly valuable for security log analytics, where historical data analysis is essential for threat detection and compliance. As a result, Trellix achieved significant operational and financial outcomes: priority-based routing and tiered storage reduced storage and compute costs by 35%, maintained high performance in security operations, supported efficient retention and analysis of large volumes of historical data, and reinforced Trellix’s commitment to comprehensive security monitoring while optimizing operational spend. This demonstrates how AWS services can help organizations reduce costs without compromising security or operational effectiveness.\nNext steps With this solution successfully deployed, Trellix is positioned to explore additional AWS capabilities and emerging technologies to further enhance its cybersecurity operations:\nIntegrate AWS ML/AI services to analyze petabytes of security log data Implement ML-based anomaly detection in OpenSearch Service Use security analytics plugins for advanced threat detection Deploy custom configurations and prebuilt security rules Conclusion Trellix successfully modernized its log management infrastructure in collaboration with AWS, deploying a sophisticated architecture to address the challenge of processing terabytes of security data per day across multiple Regions. By using OpenSearch Service with UltraWarm nodes and integrating Amazon S3, the solution delivered meaningful performance improvements, including faster log ingestion and more streamlined operations management. The architecture’s innovative tiered storage approach, combined with optimized retention policies, reduced storage costs by 35% while still meeting compliance requirements.\nThis transformation positions Trellix to handle growing data volumes and evolving security challenges efficiently, while demonstrating how strategic use of cloud services can simultaneously improve performance, reduce cost, and increase operational efficiency.\nAbout the authors Leeneksh Dubey (Leeneksh) is a Cloud Engineer at Trellix, specializing in designing scalable and highly fault-tolerant cloud infrastructure systems on AWS. He has deep experience in data, analytics, and artificial intelligence (AI), including end-to-end solution design, deployment automation, and cost optimization. His goal is to build secure, high-performance environments that support the company’s cybersecurity product portfolio. Harsh Bansal (Harsh) is an Analytics and AI Solutions Architect at Amazon Web Services (AWS). He works closely with customers to help them migrate to the cloud and optimize clusters to improve performance and reduce cost. Prior to AWS, Harsh helped customers use OpenSearch and Elasticsearch for a wide range of search and log analytics needs. Prashant Agrawal (Prashant) is a Sr. Search Specialist Solutions Architect for Amazon OpenSearch Service. He partners with customers to migrate workloads to the cloud and optimize existing clusters for better performance and cost efficiency. Before joining AWS, Prashant helped many customers deploy OpenSearch and Elasticsearch for search and log analytics use cases. Outside of work, he enjoys traveling and exploring new places—in short, he loves the “Eat → Travel → Repeat” lifestyle. "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Data access authorization features in RAG systems By Riggs Goodman III, September 18, 2025. | In Advanced (300), Amazon Bedrock, Best Practices, Generative AI, Security, Identity, \u0026amp; Compliance\nOrganizations today are increasingly using large language models (LLMs) to deliver new forms of customer interaction through generative AI–powered chatbots, virtual assistants, and intelligent search capabilities. To enhance these interactions, organizations are adopting Retrieval-Augmented Generation (RAG) to combine proprietary data, domain expertise, and internal documents in order to provide more accurate and context-aware responses. With RAG, LLMs use an external knowledge repository—typically backed by a vector store—to incorporate specific knowledge data before generating a response.\nOur customers tell us they’re concerned that adding extra context to prompts could result in sensitive information being leaked to principals (users or applications) that may exist in some of these tools, or from unstructured data in the knowledge base. As discussed in previous posts (Part 1, Part 2), LLMs should be treated as untrusted entities because they don’t enforce access controls (authorization) as part of their responses. A reasonable mental model for organizations is to assume that any data sent to an LLM in a prompt could be returned to the principal. With tools (APIs the LLM can call to interact with external resources), you can pass the principal’s identity token into the tool to determine what the principal is allowed to access and which actions are permitted. Capabilities across many vector databases—including metadata filters and synchronizing identity information between the data source and the knowledge base—can improve knowledge base retrieval results and provide basic filtering. However, this does not provide strong, data-source–based authorization with the data source acting as the single source of truth, which some customers are looking for.\nIn this post, I present an architectural pattern to provide strong authorization mechanisms for knowledge base retrieval results, along with an example implementation using Amazon S3 Access Grants with Amazon Bedrock Knowledge Bases. I also share considerations for implementing similar architectural patterns with other data sources.\nOverview of using RAG RAG architectures share many similarities with search engines, but there are also some important differences. Although both use indexed data sources to find relevant information, their approaches to data access differ. Search engines provide links to information sources, requiring users to access the original data source directly based on the permissions granted to them. This process is illustrated in Figure 1.\nFigure 1\rFigure 1: A principal (in this example, the User) accessing the data source after the search engine returns results.\nUnlike search engines, RAG systems return results from the vector database directly through the LLM, bypassing access checks at the original data source. Although metadata filtering can help control access, this approach has two key challenges. First, vector databases sync only periodically, so access changes in the data source aren’t updated immediately. Second, with complex identity permissions—where a principal can belong to hundreds of groups—accurately filtering results becomes very difficult. This makes metadata filtering insufficient for organizations that require stricter authorization mechanisms. This process is illustrated in Figure 2.\nFigure 2\rFigure 2: An application accessing data in a vector database.\nTo implement strong authorization for data access in a knowledge base, verify access directly at the data source rather than relying on intermediary systems. Using the search engine example, authorization is verified when retrieving the actual results from the data source, not during the initial search. For vector databases, the generative AI application validates access by sending an authorization request to the data source before retrieving data. This approach ensures that the data source—where the authoritative access control rules are maintained—determines whether the principal can access specific objects. This real-time authorization check means access changes are reflected immediately when accessing the data source. This authorization pattern is similar to how AWS Lake Formation manages access to structured data. Lake Formation evaluates permissions when a principal requests access to a database or table, granting or denying access based on the permissions defined for that principal. You can implement similar authorization controls for vector database results before providing that context to large language models.\nLet’s examine an example solution using S3 Access Grants with Amazon Bedrock Knowledge Bases in a real-world scenario.\nSolution overview: S3 Access Grants with Bedrock Knowledge Bases In the following example, you have an organization, ACME, that wants to build a generative AI chatbot for its employees. There are multiple teams in the organization (Marketing, Sales, HR, and IT) working on projects across the organization. You have five users (the principals accessing the application) with the following group permissions: Alice: Marketing Team Bob: Sales Team, Project A Team Carol: HR Team, Project B Team Dave: IT Support, Project C Team Eve: Marketing Team Each principal will be able to access the corresponding project folder (for example, /projects/projectA) or department folder (for example, departments/marketing/). Marketing will also have access to everything under the projects prefix (/projects/*) unless the files are considered highly confidential. To mark Project B files as highly confidential, you add metadata tags to the objects in the Project B prefix with classification = 'highly confidential'. Figure 3 shows the relationship between principals and access to different folders in the data source. For example, only Carol can access highly confidential data in the Project B folder.\n* Figure 3: Organization group permissions *\rTo grant each principal access to objects in the knowledge base, you use Amazon S3 Access Grants. You can learn how to set up S3 Access Grants in Part 1 or Part 2 of this blog series.\nIn AWS IAM Identity Center, you add each user to their respective groups. For example, Bob is added to both the Sales Team and the Project A Team groups, as shown in Figure 3.\nEach prefix (for example, projectA/, marketing/) contains a single file that provides the team’s status. In addition, for Project B, you also add the status.txt.metadata.json file to tag the object as highly confidential because it’s an HR project.\nProject B status is as follows: Project B = Compensation Update STATUS = YELLOW Project completion = 50% Notes: we are tracking behind schedule. Need to pull more resources to get it completed by next month. And the metadata.json file will contain the following:\n{ \u0026ldquo;metadataAttributes\u0026rdquo; : { \u0026ldquo;classification\u0026rdquo; : \u0026ldquo;highly confidential\u0026rdquo; } } After the knowledge base and S3 Access Grants are configured, you can start testing authorization for chunks in the knowledge base. The application workflow is described as follows (as illustrated in Figure 4):\nThe user logs in to the generative AI application using their identity provider (IdP) (steps 1a, 1b, and 1c).\nThe generative AI application exchanges tokens with IAM Identity Center and assumes a role on behalf of the user (step 2).\nThe generative AI application calls S3 Access Grants to retrieve the list of grants the user is allowed to use (step 3).\nThe user submits a query to the generative AI application (step 4).\nThe generative AI application sends the query to the knowledge base (step 5).\nThe generative AI application compares the chunks returned from the knowledge base with the access scopes granted to the user (step 6).\nOnly the scopes the user is authorized to access are forwarded to the LLM to generate a response (step 7).\nThe generative AI application repeats steps 5–7 until you want to refresh the grant list (repeat step 4) or when the token expires (repeat steps 3 and 4).\n*** Figure 4: Application flow for authorizing data access from the knowledge base ***\rThe grant scopes are shown in the following table:\nGrant scope Grant ID s3:// amzn-s3-demo-bucket/departments/sales/* edbd7575-0ba8-4837-8df1-07fe5d89f973 (sales group) s3:// amzn-s3-demo-bucket/departments/it/* a8f1d390-10d1-7037-7b27-c9fcf0b04441 (it group) s3:// amzn-s3-demo-bucket/departments/marketing/* 28f1e3c0-8081-70fe-6b4f-531ae370e7fd (marketing group) s3:// amzn-s3-demo-bucket/departments/hr/* 38f11380-d011-70fb-261b-aa50d7edc1d5 (hr group) s3:// amzn-s3-demo-bucket/projects/projectA/* c84173b0-b071-70c5-3207-dadc1e6f76a9 (project A group) s3:// amzn-s3-demo-bucket/projects/projectB/* 2871d3c0-6001-7073-baaf-62717f56b8d0 (project B group) s3:// amzn-s3-demo-bucket/projects/projectC/* f8a183b0-f001-707b-aa8e-1826ca04595e (project C group) s3:// amzn-s3-demo-bucket/projects/* 28f1e3c0-8081-70fe-6b4f-531ae370e7fd (marketing group) In this example, you can use Bob’s role to illustrate how chunk authorization works. When you call the knowledge base without performing any data authorization steps, you receive a response like the following when asking, “What is the status of my project?” For each object in the data source, you also include metadata as a *.metadata.json file, which the knowledge base uses to assign specific key/value pairs to each object. This is where you set Project A and Project C to confidential and Project B to highly confidential, as mentioned earlier. You pass this filter as part of the request to the Bedrock knowledge base using RetrievalFilter in retrievalConfiguration. The following code shows the response from the Bedrock knowledge base:\n{ \u0026#34;ResponseMetadata\u0026#34;: { ... }, \u0026#34;retrievalResults\u0026#34;: [ { \u0026#34;content\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Project A status is as follows: Project A = Sales Strategy STATUS = GREEN Project completion = 80% Notes: we are on track to complete the project by end of month\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;TEXT\u0026#34; }, \u0026#34;location\u0026#34;: { \u0026#34;s3Location\u0026#34;: { \u0026#34;uri\u0026#34;: \u0026#34;s3://amzn-s3-demo-bucket/projects/projectA/status.txt\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;S3\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;x-amz-bedrock-kb-source-uri\u0026#34;: \u0026#34;s3://amzn-s3-demo-bucket/projects/projectA/status.txt\u0026#34;, \u0026#34;classification\u0026#34;: \u0026#34;confidential\u0026#34;, \u0026#34;x-amz-bedrock-kb-chunk-id\u0026#34;: \u0026#34;1%3A0%3AnTT-15UBTG7d8qG4nL6p\u0026#34;, \u0026#34;x-amz-bedrock-kb-data-source-id\u0026#34;: \u0026#34;CIUUDCONV2\u0026#34; }, \u0026#34;score\u0026#34;: 0.558023 }, { \u0026#34;content\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Project C status is as follows: Project C = Infrastucture Update STATUS = RED Project completion = 30% Notes: ROI is not meeting expectations, rethinking strategy with project\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;TEXT\u0026#34; }, \u0026#34;location\u0026#34;: { \u0026#34;s3Location\u0026#34;: { \u0026#34;uri\u0026#34;: \u0026#34;s3://amzn-s3-demo-bucket/projects/projectC/status.txt\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;S3\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;x-amz-bedrock-kb-source-uri\u0026#34;: \u0026#34;s3://amzn-s3-demo-bucket/projects/projectC/status.txt\u0026#34;, \u0026#34;classification\u0026#34;: \u0026#34;confidential\u0026#34;, \u0026#34;x-amz-bedrock-kb-chunk-id\u0026#34;: \u0026#34;1%3A0%3AnDT-15UBTG7d8qG4mb78\u0026#34;, \u0026#34;x-amz-bedrock-kb-data-source-id\u0026#34;: \u0026#34;CIUUDCONV2\u0026#34; }, \u0026#34;score\u0026#34;: 0.52052265 } ] } Data from Project B is not included in the output because it is labeled as highly confidential. Data from Project C is included, even though Bob should not have access to it—so let’s walk through how to authorize Bob correctly. In the following steps and with the provided Python sample code, I’ll show how to call each function shown in the code block below. You can use this code as part of an application to validate authorization for the data returned from the Bedrock knowledge base.\n# Execute the workflow # 1. Assume a role to access S3 client_s3_oidc = assume_role( args.client_id, args.grant_type, args.assertion, args.role_arn, args.role_session_name, args.provider_arn ) # 2. Retrieve the S3 scopes granted to the caller scopes = get_caller_grant_scopes(client_s3_oidc, args.account) # 3. Filter chunks based on the caller\u0026#39;s grants authorized, not_authorized = check_grant_scopes(chunks, scopes) Step 1: The user logs in to the generative AI application using an IdP When Bob first accesses the generative AI application, the application redirects him through a single sign-on (SSO) flow to authenticate with his IdP. Bob receives a signed identity token from the IdP that asserts his identity. An example of Bob’s identity token is shown as follows:\n{ \u0026ldquo;sub\u0026rdquo;: \u0026ldquo;sub\u0026rdquo;, \u0026ldquo;email\u0026rdquo;: \u0026ldquo;bob@example.com\u0026rdquo;, \u0026ldquo;aud\u0026rdquo;: \u0026ldquo;bob\u0026rdquo;, \u0026ldquo;iss\u0026rdquo;: \u0026ldquo;https://tokens.identity-solutions.example.com\u0026rdquo;, \u0026ldquo;exp\u0026rdquo;: 1744219319, \u0026ldquo;iat\u0026rdquo;: 1744218719, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;bob\u0026rdquo; } Step 2: Exchange the token with IAM Identity Center After Bob is authenticated and passes his token to the generative AI application, the application exchanges the IdP identity token for an IAM Identity Center identity token and obtains temporary credentials on Bob’s behalf. You create a Python function named assume_role that takes various variables, allowing Bob to assume a role in AWS:\nclient_id: A unique identifier string for the client or application. This value is the ARN (Amazon Resource Name) of the application configured with OAuth grants. grant_type: The OAuth grant type; in this example, it’s JWT Bearer. role_arn: The ARN of the role to assume. role_session_name: An identifier for the assumed role session. provider_arn: The ARN of the context provider from which the trusted context assertion is created. client_assertion: A JSON Web Token (JWT) issued by a trusted token issuer. In the sample Python function shown below, you perform the following steps:\nOpen two boto3 clients: sso-oidc (to create the token with IAM) and sts (to assume a temporary role for Bob). Use client_id, grant_type, and client_assertion to call create_token_with_iam, generating an IAM Identity Center token and storing it in token_response. Inside token_response, extract sts:identity_context, which is required to assume the role for Bob. With identity_context, pass this context into assume_role along with role_arn, role_session_name, and provider_arn to obtain temporary credentials for Bob. Finally, return a boto3 s3-control client using Bob’s temporary credentials to validate his access through S3 Access Grants. def assume_role(client_id, grant_type, client_assertion, role_arn, role_session_name, provider_arn): \u0026#34;\u0026#34;\u0026#34; Assume an IAM role using SSO/OIDC authentication and return an S3 control client. Args: client_id: The ID of the OIDC client grant_type: The type of grant being requested client_assertion: The client assertion token role_arn: ARN of the role to assume role_session_name: Name for the temporary session provider_arn: ARN of the identity provider Returns: boto3.client: An S3 control client with temporary credentials \u0026#34;\u0026#34;\u0026#34; client_oidc = boto3.client(\u0026#39;sso-oidc\u0026#39;) client_sts = boto3.client(\u0026#39;sts\u0026#39;) try: # Get ID token from IAM using SSO OIDC token_response = client_oidc.create_token_with_iam( clientId=client_id, grantType=grant_type, assertion=client_assertion ) # Extract identity context from token id_token = jwt.decode(token_response[\u0026#39;idToken\u0026#39;], options={\u0026#39;verify_signature\u0026#39;: False}) identity_context = id_token[\u0026#39;sts:identity_context\u0026#39;] # Assume role using identity context temp_credentials = client_sts.assume_role( RoleArn=role_arn, RoleSessionName=role_session_name, ProvidedContexts=[{ \u0026#39;ProviderArn\u0026#39;: provider_arn, \u0026#39;ContextAssertion\u0026#39;: identity_context }] ) # Create and return S3 control client with temporary credentials creds = temp_credentials[\u0026#39;Credentials\u0026#39;] return boto3.client( \u0026#39;s3control\u0026#39;, region_name=\u0026#39;us-west-2\u0026#39;, aws_access_key_id=creds[\u0026#39;AccessKeyId\u0026#39;], aws_secret_access_key=creds[\u0026#39;SecretAccessKey\u0026#39;], aws_session_token=creds[\u0026#39;SessionToken\u0026#39;] ) except ClientError as e: print(f\u0026#39;Error: {e}\u0026#39;) sys.exit(1) Step 3: Retrieve the caller grant scopes Next, you need to retrieve what Bob is allowed to access in the data source using S3 Access Grants. In this example, you validate what Bob is authorized to access at the data-source level, rather than checking each individual S3 object.\nTo retrieve the list of prefixes Bob is allowed to access, you perform the following steps in the get_caller_grant_scopes function:\nPass in the s3control client returned by assume_role, along with the account associated with S3 Access Grants. Using Bob’s temporary role, call list_caller_access_grants. This returns the list of access grants Bob has. For example, when you call this function for Bob, you receive a response from list_caller_access_grants like the following, showing that Bob has access to the sales and projectA prefixes:\n{ \u0026ldquo;ResponseMetadata\u0026rdquo;: { \u0026hellip; }, \u0026ldquo;CallerAccessGrantsList\u0026rdquo;: [ { \u0026ldquo;Permission\u0026rdquo;: \u0026ldquo;READ\u0026rdquo;, \u0026ldquo;GrantScope\u0026rdquo;: \u0026ldquo;s3:// amzn-s3-demo-bucket/departments/sales/*\u0026rdquo;, \u0026ldquo;ApplicationArn\u0026rdquo;: \u0026ldquo;ALL\u0026rdquo; }, { \u0026ldquo;Permission\u0026rdquo;: \u0026ldquo;READ\u0026rdquo;, \u0026ldquo;GrantScope\u0026rdquo;: \u0026ldquo;s3:// amzn-s3-demo-bucket/projects/projectA/*\u0026rdquo;, \u0026ldquo;ApplicationArn\u0026rdquo;: \u0026ldquo;ALL\u0026rdquo; } ] } You add the scopes into an array and return it to the application. An example implementation is shown below. Note: you remove the * from the access grant because each chunk’s URI is a full path, not just a prefix.\ndef get_caller_grant_scopes(client, account): \u0026#34;\u0026#34;\u0026#34; Retrieve the S3 access scopes granted to a caller. Args: client: S3 control client with assumed role credentials account: AWS account ID Returns: List of S3 path prefixes the caller is authorized to access \u0026#34;\u0026#34;\u0026#34; try: # Get list of access grants for the caller response = client.list_caller_access_grants(AccountId=account) # Extract S3 path prefixes and remove trailing wildcards scopes = [grant[\u0026#39;GrantScope\u0026#39;].replace(\u0026#39;*\u0026#39;,\u0026#39;\u0026#39;) for grant in response[\u0026#39;CallerAccessGrantsList\u0026#39;]] return scopes except ClientError as e: print(f\u0026#39;Error: {e}\u0026#39;) sys.exit(1) At this point, you have the list of grant scopes Bob is allowed to access in the data source. This information can now be used to compare against the chunks returned from the knowledge base in order to validate access before sending the final prompt (with additional context) to the LLM.\nStep 4: Check caller grant scopes The final step is to compare the chunks returned from the knowledge base with the list of grants that Bob is allowed to access.\nTo do this, you define the check_grant_scopes function and pass in both arguments: the chunks and the scopes Bob is authorized to access.\nThe chunks variable is an array of dictionaries; you iterate through it and validate each chunk against the list of scopes, as shown in the example code below.\nIterate through each chunk passed into the function. For each chunk, check whether the chunk location starts with a prefix in the S3 access grants. If there’s a match, add the chunk (along with the matching scope) to the authorized chunks list. If there’s no match, add the chunk to the not_authorized chunks list. The function returns two lists—one for authorized chunks and one for not authorized chunks—so you can easily see which data Bob should not be able to access.\ndef check_grant_scopes(chunks, scopes): \u0026#34;\u0026#34;\u0026#34; Check which chunks a user is authorized to access based on their granted scopes. Args: chunks: List of dictionaries containing content chunks with \u0026#39;location\u0026#39; keys scopes: List of authorized S3 path prefixes the user has access to Returns: tuple: (authorized_chunks, unauthorized_chunks) \u0026#34;\u0026#34;\u0026#34; authorized = [] not_authorized = [] # If user has no scopes, they are not authorized for any chunks if not scopes: return [], chunks # Check each chunk against available scopes for chunk in chunks: location = chunk[\u0026#39;location\u0026#39;] authorized_scope = next((scope for scope in scopes if location.startswith(scope)), None) if authorized_scope: chunk[\u0026#39;scope\u0026#39;] = authorized_scope authorized.append(chunk) else: not_authorized.append(chunk) return authorized, not_authorized When you run the function above for Bob along with the chunks returned from the knowledge base, you get the list of authorized and not authorized chunks as shown in the following example. # Authorized: [ { \u0026#34;content\u0026#34;: \u0026#34;Project A status is as follows: Project A = Sales Strategy STATUS = GREEN Project completion = 80% Notes: we are on track to complete the project by end of month\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;s3://amzn-s3-demo-bucket/projects/projectA/status.txt\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;s3://amzn-s3-demo-bucket/projects/projectA/\u0026#34; } ] # Not Authorized: [ { \u0026#34;content\u0026#34;: \u0026#34;Project C status is as follows: Project C = Infrastucture Update STATUS = RED Project completion = 30% Notes: ROI is not meeting expectations, rethinking strategy with project\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;s3://amzn-s3-demo-bucket/projects/projectC/status.txt\u0026#34; } ] Considerations for the solution\nWhen implementing an authorization architecture for RAG systems, there are several important factors to understand because they affect security, performance, and scalability. Considering these factors carefully helps ensure your system maintains strong security controls while optimizing performance and remaining flexible across multiple data sources. Key considerations include:\nIn this example, you used S3 Access Grants as a way to validate access. However, this architecture can be applied to other data sources as well, as long as the data source URI is returned by the knowledge base and an API is available to validate the principal’s access—similar to the get_caller_grant_scopes function described earlier.\nUsing S3 Access Grants provides an authorization mechanism for principals to access the data source. However, you can still apply additional access control policies per bucket by adding key/value tags or data source–level policies, if needed. In this way, a principal can be denied access to a bucket even if S3 Access Grants allows it.\nTo support this, you can add metadata to the vector database so the system can filter queries into the knowledge base, as shown in the earlier example.\nJust as data in the knowledge base can become stale between resyncs, the list of authorized scopes can also become inaccurate over time. You should determine how frequently you refresh the grant list (step 3 in Figure 4) and the lifetime of the assumed-role session (step 2 in Figure 4).\nDepending on which chunks the principal is allowed to access and what the knowledge base returns, some chunks may be removed before being sent to the LLM. From a security perspective, this is desirable because it ensures users cannot access data they are not authorized to see.\nConclusion\nIn this post, I introduced an architectural pattern to implement strong authorization mechanisms for results returned from a knowledge base. You learned the importance of strict authorization in a knowledge base and how to implement it using Amazon S3 Access Grants.\nFinally, we reviewed real code examples that demonstrate how the mechanism works when combining Amazon Bedrock Knowledge Bases with S3 Access Grants—allowing you to control data access in a way that is safer, more flexible, and more effective in RAG systems.\nTo learn more about security in generative AI systems, see additional posts on the AWS Security Blog and AWS blog posts covering generative AI on the AWS Blog.\nIf you have feedback about this post, please leave a comment in the Comments section below.\nIf you have questions related to this post, please contact AWS Support for assistance.\nAbout the authors Riggs Goodman III Riggs is a Principal Partner Solution Architect at AWS. His current focus is security and networking for AI, providing technical guidance, architectural patterns, and direction to customers and partners building AI workloads on AWS. Internally at AWS, Riggs focuses on driving the overall technical strategy and innovation across AWS service teams to address customer and partner challenges. .\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Announcing multi-account ingestion for Amazon OpenSearch Service By David Venable, September 19, 2025 | Amazon OpenSearch Service, Announcements, Intermediate (200)\nAmazon OpenSearch Ingestion is a powerful data ingestion pipeline that AWS customers use for a variety of purposes, such as observability, analytics, and zero-ETL search. Many customers today push logs, traces, and metrics from their applications into OpenSearch Ingestion to store and analyze this data.\nToday, we’re excited to announce that OpenSearch Ingestion pipelines now support cross-account ingestion for push-based data sources such as HTTP and OpenTelemetry (OTel). Organizations can now use this feature to easily share data across teams. For example, many organizations have centralized observability teams—these teams can now create OpenSearch Ingestion pipelines and share them with other teams in their organization. You can also use this feature to ingest data into Amazon OpenSearch Service domains or Amazon OpenSearch Serverless collections in other accounts.\nPreviously, sharing OpenSearch Ingestion pipelines across accounts required teams to use virtual private cloud (VPC) features to share access. For example, teams could use VPC peering, which isn’t always feasible, or AWS Transit Gateway. The new cross-account ingestion features in OpenSearch Ingestion can simplify your deployment and reduce the cost of sharing pipelines.\nSolution overview\nLet’s look at how to share a pipeline from a central logging account with two other development accounts (A and B). The central logging account can create an OpenSearch Ingestion pipeline using a push-based source such as HTTP. After creating the pipeline, a member of the central logging team can grant access to other teams. They can use a resource policy that grants permissions to the other two team accounts to create pipeline endpoints. After making this change, the OpenSearch Ingestion pipeline becomes available for other teams to use.\nThe following diagram illustrates this architecture.\nFigure 1\rIn the following sections, we illustrate how to implement this solution.\nPrerequisites First, the central logging account must have a VPC with two options enabled:\nenableDnsSupport must be set to true enableDnsHostnames must be set to true The central logging account must also create a push-based OpenSearch Ingestion pipeline within that VPC. This can be a pipeline that receives logs from FluentBit or OpenTelemetry telemetry.\nThe development accounts connecting to the pipeline must also have VPCs in the same Region with the same DNS options enabled:\nenableDnsSupport must be set to true enableDnsHostnames must be set to true Create a resource policy As the pipeline owner, you can create a resource policy that allows the two development accounts to create pipeline endpoints for your pipeline.\nThe following is an example resource policy for this scenario:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: [ \u0026#34;000000000000\u0026#34;, \u0026#34;999999999999\u0026#34; ] }, \u0026#34;Action\u0026#34;: \u0026#34;osis:CreatePipelineEndpoint\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:osis:us-west-2:123456789012:pipeline/central-logging\u0026#34; } ] } The OpenSearch Ingestion console makes it simple to create these policies, as shown in the following screenshot.\nFigure 2\rCreate a pipeline endpoint Now that the central logging account has shared permissions on their pipeline, the development accounts can create a pipeline endpoint. A pipeline endpoint is a connection from a VPC to an OpenSearch Ingestion pipeline.\nThe development accounts are responsible for creating pipeline endpoints in the VPCs they want to connect from. They create the endpoint in the required subnets and provide a security group. This security group must have an inbound rule that allows HTTPS access on port 443 from any sources the development accounts need in order to ingest logs.\nDevelopment team A can create a pipeline endpoint using a command similar to the following:\naws --region us-west-2 osis create-pipeline-endpoint \\ --pipeline-arn arn:aws:osis:us-west-2:123456789012:pipeline/central-logging \\ --vpc-options \u0026#39;{\u0026#34;SubnetIds\u0026#34;:[\u0026#34;subnet-123456789012345678\u0026#34;,\u0026#34;subnet-012345678912345678\u0026#34;],\u0026#34;SecurityGroupIds\u0026#34;:[\u0026#34;sg-123456789012345678\u0026#34;]}\u0026#39; Development team A can also use the OpenSearch Ingestion console to create the pipeline endpoint.\nFigure 3\rAfter making this change, development team A’s VPC will have a pipeline endpoint. This pipeline endpoint now allows data ingestion into the central logging pipeline. Now, Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon Elastic Container Service (Amazon ECS) tasks, Kubernetes pods, and other compute workloads running in the VPC can ingest their log data into the pipeline using tools such as FluentBit.\nAt the same time, or later, development team B can also create a pipeline endpoint. This team will create an endpoint for their own VPC.\nAfter this step, the pipeline will have two pipeline endpoints, so both teams can ingest their log data into the central logging VPC.\nClean up After a pipeline endpoint is created, any account can delete it. The development teams in our scenario can use the DeletePipelineEndpoint API to delete the endpoint from their account. In addition, if the central logging account needs to remove a pipeline endpoint connection from the pipeline, they can use the RevokePipelineEndpointConnections API. Both options are available in the OpenSearch Ingestion console.\nAfter the pipeline endpoints are deleted, the central logging team can also delete the pipeline if they no longer need it.\nConclusion The new pipeline endpoint feature for OpenSearch Ingestion simplifies how you can share pipelines for cross-account ingestion. This can help teams take advantage of the powerful features of OpenSearch Ingestion and unlock new possibilities for teams or organizations using multiple accounts and VPCs. The new pipeline endpoint feature is now available in the AWS Regions where OpenSearch Ingestion is supported.\nTo get started with cross-account ingestion in OpenSearch Ingestion, refer to the OpenSearch Ingestion documentation or try creating your first cross-account pipeline in the OpenSearch Ingestion console.\n.\nAbout the author David Venable David is a Senior Software Engineer at Amazon Web Services (AWS), currently working in OpenSearch observability. He is also a maintainer of the Data Prepper project. "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyễn Minh Đức\nPhone Number: 0768803969\nEmail: minhducn8870@gmail.com\nUniversity: FPT University HCM\nMajor: Artificial Intelligence\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: AI Engineer Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "ONLINE PLATFORM FOR TRACKING AND FORECASTING HURRICANE TRAJECTORY In this workshop, we will present how we created an online platform that allows internet users to freely check, track, and even predict the path of ongoing storms in the West Pacific region. This platform helps users better prepare for upcoming natural disasters and reduces the potential damage they may cause.\nThe platform provides two main functionalities:\nShowing Recent Storms – Allows users to view the path, intensity, wind speed, and other characteristics of recent storms in the West Pacific region. Predicting Hurricane Trajectories – Allows users to input past storm locations (latitude and longitude; at least 9 data points) to obtain predictions about the storm’s near-future movement, intensity changes, and potential path. Following the flow of this workshop, we will discuss the datasets, pre-processing steps, model-training pipeline, and the process of building the online platform using AWS services. We will also demonstrate our proposed augmentation techniques—Stepwise Temporal Fading Augmentation (STFA) and Plausible Geodesic Bearing Augmentation (PGBA)—along with the use of physics-informed machine learning. These approaches enhance the realism of the training data and significantly improve prediction accuracy for storm trajectories, lifetime estimates, and total travel distance.\nFigure 1 : Model pipeline\rOnce the model-training process is completed, we move to building the online platform using a serverless architecture. This architecture is cost-efficient, scalable, and easy to maintain/deploy—making it an ideal choice for our project. Below are the main AWS services used:\nAWS Lambda – Executes the ML models and handles backend logic Amazon S3 – Stores static files, trained models, and storm data Amazon API Gateway – Routes user requests to the appropriate Lambda functions depending on whether they are viewing recent storms or running predictions Amazon CloudFront – Speeds up content delivery through edge locations AWS Secrets Manager – Stores API keys and other sensitive information … – Additional supporting services as needed Figure 2 : Platform Architecture\r"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Explore topic project and key idea for it. Learn about AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/09/2025 11/09/2025 3 - Research about project predict Hurricane path by reading Scholarly 10/09/2025 13/09/2025 - Academic Lesson 1: Tropical Cyclone Track Forecasting Using Fused Deep Learning (S. Giffard-Roisin et al., 2020) (link) - Academic Lesson 2: Forecasting Tropical Cyclone Tracks in the Northwestern Pacific Based on Deep-Learning Model (Wang, L. et al., 2023) (link) 4 - Find data source for project 14/9/2025 16/9/2025 4 - Investigate about AWS services by watching Youtube from link of Mail received 15/9/2025 15/9/2025 Week 1 Achievements: Understanding each other members.\nGathering basic knowledges for this project:\nWhat tropical cyclones (hurricanes/typhoons) are and how they’re structured (eye, eyewall, rainbands). How they form in nature: warm SSTs (~26–27 °C+), moisture, pre-existing disturbance, Coriolis, and low vertical wind shear. Energy source \u0026amp; lifecycle: latent-heat release, intensification, peak, decay/extratropical transition. Understanding AWS service groups:\nCompute — EC2, Lambda, ECS/EKS Storage — S3, EBS, EFS Networking \u0026amp; CDN — VPC, Route 53, CloudFront Key concepts learned of AWS service:\nRegions, Availability Zones, and global services. IAM users, roles, and least privilege. VPC design, routing, and security groups. Cost allocation with tags and budgets "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Advance the storm modeling project by refining data structures and analysis approach. Strengthen AWS Cloud knowledge through hands-on practice with core services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Ongoing Progress in Storm Analysis and Trajectory Modeling Project +Storm Data Analysis + Trajectory Modeling \u0026amp; Classification + Predictive Modeling Development 15/09/2025 21/09/2025 2 - Collaboration with the Software Engineering team on integrating the model and the software 16/09/2025 18/09/2025 3 - Continue exploring AWS cloud services: + Amazon S3 – learned how to set up buckets and manage data storage. + Amazon EC2 – learned how to launch instances, configure security groups, and connect via SSH. 17/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Gained deeper understanding of AWS fundamentals\nSWorked with Amazon S3 and EC2 services:\nCreated and managed S3 buckets for storm data Launched EC2 instances and configured security groups Generated and used key pairs for secure accessView EC2 service Connected via SSH and set up the working environment Linked S3 with EC2 for data workflow integration Progress on Storm Modeling Project\nExplored different modeling approaches and how they can be applied to the project data Reviewed academic research to identify formulas and methods relevant Defined key metrics (speed, heading, distance, lifetime) and clarified how they will be derived from raw data. Coordinated with the Software Engineering team:\nDefined integration points between model and software Clarified responsibilities of each side "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Clean and prepare storm trajectory data for modeling. Create essential features for trajectory prediction. Build and evaluate baseline prediction models. Learn AWS Lambda basics for future integration. Align API design and data flow with the Software Engineering team. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Ongoing work on storm dataset preprocessing + Clean historical storm tracks + Standardize timestamps and remove inconsistent records 22/09/2025 24/09/2025 - 2 - Feature engineering for storm trajectory model: compute Δlat, Δlon, speed, heading 23/09/2025 25/09/2025 - 3 - Develop baseline trajectory prediction models + Persistence model + Simple regression benchmark + Evaluate MAE \u0026amp; distance error 24/09/2025 26/09/2025 - 4 - Review AWS Lambda in detail - Study handler, runtime behavior, and cold start - Check Lambda → API Gateway integration model 25/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Coordinated with Software Engineering team on API design and data flow 26/09/2025 26/09/2025 - Week 3 Achievements: Completed preprocessing for the storm dataset.\nBuilt key trajectory features:\nΔlat, Δlon Forward speed Heading vector Developed baseline prediction models.\nEvaluated early performance:\nPersistence model benchmark Simple regression MAE \u0026amp; distance-error Improved understanding of AWS Lambda and its workflow:\nHandler structure Runtime behavior Basic API Gateway integration Coordinated with the Software Engineering team on API structure.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Ensure the storm dataset is consistent and ready for modeling. Refine and improve key trajectory feature calculations. Run small exploratory tests to observe early model behavior. Learn essential AWS components for future integration (Lambda, API Gateway, CloudWatch). Align with the SE team on prediction output format and upcoming integration steps. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Reviewed storm dataset to ensure consistency 29/09/2025 30/09/2025 - 2 - Continued refining feature extraction + Adjust Δlat/Δlon calculations + Re-check movement speed formatting 30/09/2025 01/10/2025 - 3 - Conducted small exploratory prediction tests 01/10/2025 02/10/2025 - 4 - Studied AWS documentation related to project + Lambda execution flow + API Gateway trigger basics + CloudWatch log usage 02/10/2025 02/10/2025 AWS Docs 5 - Discussed prediction output format and next steps with SE team 03/10/2025 03/10/2025 - Week 4 Achievements: Reviewed and validated the storm dataset to ensure stable input for later modeling. Refined several key trajectory features after rechecking coordinate changes. Performed small exploratory prediction tests to understand data behavior. Studied AWS documentation relevant to the project: Lambda execution flow How API Gateway connects to Lambda Basic CloudWatch log usage Discussed API structure and future integration steps with the Software Engineering team. "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Improve and refine the existing feature extraction logic for storm data. Run additional baseline tests to observe how input variations affect prediction accuracy. Strengthen AWS foundational knowledge, focusing on Lambda, API Gateway, and S3 interactions. Define an initial JSON structure for model prediction outputs. Sync with the team to align progress and clarify current challenges. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Reviewed and adjusted parts of the feature extraction logic 06/10/2025 07/10/2025 - 2 - Conducted additional small baseline prediction tests + Tried slight variations in input + Observed changes in basic error metrics 07/10/2025 08/10/2025 - 3 - Continued reading AWS documentation + Lambda concurrency basics + API Gateway request flow + S3 object retrieval patterns 08/10/2025 08/10/2025 AWS Docs 4 - Drafted a simple JSON structure for prediction output 09/10/2025 09/10/2025 - 5 - Participated in team sync meeting to share updates and clarify current challenges 10/10/2025 10/10/2025 - Week 5 Achievements: Refined the overall quality of the prepared storm features.\nRan a few lightweight trials to observe how small input changes affect prediction behavior:\nCompared short test runs Noted the patterns from updated error values Expanded understanding of AWS through additional documentation:\nHow Lambda handles concurrent executions Typical request flow inside API Gateway Basic approaches to retrieving objects from S3 Sketched an initial idea for how the model’s output could be structured.\nJoined a team discussion to align expectations and share progress updates.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Identify and compare suitable model options for storm trajectory prediction. Explore and analyze the strengths and limitations of LSTM, RNN, and Transformer-based approaches. Draft an initial modeling plan covering inputs, structure, and a simple local training/testing flow. Continue building AWS familiarity through small experiments with S3, CloudWatch, and Lambda. Run early trial tests to understand initial model behavior and validate basic logic. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Reviewed suitable model candidates for trajectory prediction 13/10/2025 13/10/2025 - 2 - Continued exploring modeling approaches + Compared LSTM, simple RNN, and a light Transformer idea + Noted pros/cons for each method 14/10/2025 16/10/2025 - 3 - Drafted an initial plan for how the model might be structured + Identified input requirements + Outlined a small training/test flow (local only) 16/10/2025 17/10/2025 - 4 - Studied AWS docs related to project workflow + Reviewed basic S3 folder organization + Observed CloudWatch log examples + Tried a small Lambda test 17/10/2025 18/10/2025 AWS Docs 5 - Performed small trial runs on early model logic 18/10/2025 18/10/2025 - Week 6 Achievements: Gained a clearer understanding of which model directions are most practical.\nTook time to compare several modeling ideas:\nLSTM and RNN variations A simple Transformer concept Notes on strengths and weaknesses Outlined an early modeling plan to guide future work.\nContinued learning AWS basics involved in the project:\nS3 organization patterns Using CloudWatch for simple log checks Small Lambda test attempts Ran a few lightweight model trials to observe how the logic behaves in early form.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Validate the storm dataset and ensure it is ready for prototype modeling. Start building a simple prototype model to test core logic and data flow. Strengthen understanding of AWS components relevant to future deployment, especially Lambda Layers and dependency packaging. Run early prototype tests to observe initial model behavior. Document findings and identify areas needing refinement for upcoming iterations. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Reviewed and checked the storm input dataset for modeling 20/10/2025 21/10/2025 - 2 - Began building a simple prototype model + Set up basic model structure + Prepared input tensors + Wrote initial forward logic 21/10/2025 23/10/2025 - 3 - Continued studying AWS documentation + Lambda Layers basics + Packaging Python dependencies + Intro to CloudWatch logs 23/10/2025 23/10/2025 AWS Docs 4 - Ran small test cases on the prototype to observe early behavior 24/10/2025 24/10/2025 - 5 - Summarized progress and documented areas that need refinement for modeling 25/10/2025 25/10/2025 - Week 7 Achievements: Gained a clearer understanding of the dataset after reviewing the modeling inputs.\nMade progress on the prototype model:\nSet up an initial structure Tested basic forward logic Identified areas that need refinement Improved knowledge of AWS components related to model deployment:\nUnderstanding of Lambda Layers Basic handling of Python dependencies Familiarity with CloudWatch logging flow Observed how the early prototype behaves through simple test cases.\nDocumented current findings and outlined what should be improved in the next iterations.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Finalize preprocessing steps to ensure the dataset is fully ready for training. Review and refine the prototype model structure to prepare it for the upcoming training phase. Strengthen understanding of AWS environment setup, including dependency packaging and logging workflows. Validate model stability through controlled prototype tests before training. Document remaining issues and outline the final steps required to begin initial training. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Revisited preprocessing steps to make sure the cleaned data is ready for use in a training pipeline + Verified coordinate normalization across samples + Checked for missing or inconsistent values 27/10/2025 28/10/2025 - 2 - Reviewed prototype model structure to ensure it can move toward a trainable version + Inspected tensor shapes in every layer + Noted sections needing clearer logic before training + Ensured forward pass stability 28/10/2025 30/10/2025 - 3 - Studied AWS documentation on preparing environments for ML workloads + Looked into dependency packaging + Checked how future training logs could be viewed via CloudWatch 30/10/2025 30/10/2025 AWS Docs 4 - Ran several controlled prototype tests to confirm the model behaves consistently with updated preprocessing - Compared outputs before and after adjustments to verify readiness for a first training attempt 31/10/2025 31/10/2025 - 5 - Documented all findings, listed remaining issues, and outlined what must be finalized before starting any initial training steps 01/11/2025 01/11/2025 - Week 8 Achievements: Improved the preprocessing pipeline to ensure the dataset is stable and suitable for upcoming training steps.\nGained a clearer view of how the current model behaves after several refinements and confirmed that the forward pass is now more stable.\nPrepared better for the training stage:\nIdentified what parts of the model need clearer structure Checked all tensor dimensions inside the model Ensured no unexpected shape mismatches remain Understood more about AWS requirements for ML environments, especially around dependency preparation and logging.\nCompiled a list of items that must be completed before starting the first phase of model training.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Finalize and verify the training dataset to ensure consistent input quality. Run the first full training cycle and observe initial learning behavior. Analyze training logs to identify early issues and unstable patterns. Evaluate trained model predictions against baseline performance. Outline key areas that need fine-tuning for the next stage of model improvement. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Prepared final training dataset and verified formatting consistency + Rechecked normalization ranges + Ensured no missing segments 04/11/2025 05/11/2025 - 2 - Started full training run for the prototype model + Ran several epochs to observe initial convergence + Logged early behaviors for analysis + Noted unstable loss patterns 05/11/2025 07/11/2025 - 3 - Processed training logs and reviewed loss progression to identify issues needing further adjustment 06/11/2025 06/11/2025 - 4 - Performed stability tests after training - Compared trained predictions with baseline outputs 08/11/2025 09/11/2025 - 5 - Summarized initial training results and outlined what should be optimized in the next fine-tuning phase 09/11/2025 10/11/2025 - Week 9 Achievements: Completed the first full training run of the model and observed how the loss curve behaved across multiple epochs.\nIdentified several patterns from the training logs that will guide fine-tuning in the next stage.\nEarly loss fluctuations Sections where predictions diverged Notes on areas needing structural refinement Understood better how the trained model reacts to different input samples and how far it currently is from expected real-world behavior.\nCompared post-training predictions with the baseline and recorded where improvements were noticeable and where the model still struggled.\nPrepared a list of fine-tuning points to work on next.\nAdjustment of internal logic Re-evaluation of preprocessing effects Minor architectural tweaks for the next iteration "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Understanding the project scope, reading proposals, and reviewing related storm trajectory prediction approaches\nWeek 3: Storm dataset preprocessing, feature engineering, baseline models, and learning Lambda fundamentals\nWeek 4: Refining features, running exploratory prediction tests, and studying Lambda–API Gateway–CloudWatch basics\nWeek 5: Improving feature extraction, running additional baseline tests, and reviewing Lambda concurrency + S3 patterns\nWeek 6: Comparing model candidates (LSTM/RNN/Transformer), drafting modeling plan, and basic AWS practice (S3/CloudWatch/Lambda)\nWeek 7: Building a prototype model, testing early behavior, and learning Lambda Layers \u0026amp; dependency packaging\nWeek 8: Preparing for training: stabilizing preprocessing, validating model structure, and reviewing AWS ML environment setup\nWeek 9: Running the first full training, analyzing logs, and evaluating trained predictions vs baseline\nWeek 10: Fine-tuning the model, post-tuning evaluation, and completing remaining AWS learning for deployment readiness\nWeek 11: Error analysis, integrating the trained model into an API, and testing deployment on the web interface\nWeek 12: Final documentation, technical report, final slides, end-to-end demo validation, and project handover runbook\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/5-workshop/5.2-data-preparation/",
	"title": "Data Preparation",
	"tags": [],
	"description": "",
	"content": "Data Collection and Preparation Data is a critical component of our project. It not only powers the machine learning model but is also directly displayed to end users so they can monitor the most recent storms in the West Pacific region. Because of this dual purpose—model training and real-time visualization—we carefully investigated multiple reliable and authoritative sources before selecting a single dataset that met all of our requirements: Hurricane Data from NOAA.\nThe National Oceanic and Atmospheric Administration (NOAA) is a scientific agency under the U.S. Department of Commerce. NOAA provides highly accurate, research-grade environmental data, including global weather observations, satellite imagery, and tropical cyclone records. With decades of investment in advanced technologies such as geostationary satellites, ocean buoys, radar systems, and climate monitoring networks, NOAA is widely regarded as one of the most trustworthy providers of hurricane information in the world.\nFor this project, we specifically use data from the International Best Track Archive for Climate Stewardship (IBTrACS)—a NOAA-led project and the world’s most comprehensive tropical cyclone dataset. IBTrACS consolidates and unifies historical and modern storm-track data from multiple meteorological agencies (e.g., JTWC, JMA, CMA, NHC). By merging these sources into a single consistent format, it improves inter-agency comparability and ensures researchers worldwide have access to the best available storm-track information.\nThe version of the dataset we use contains 226,153 rows of hurricane observations. Each row includes a variety of valuable fields such as:\nsid – storm ID number – storm number basin / subbasin – regional classification nature – storm type (e.g., tropical storm, typhoon) iso_time – timestamp lat / lon – storm center coordinates … and many additional meteorological properties However, for our machine learning model, we focus only on four key columns: sid, iso_time, lat, and lon. These form the essential time-series trajectory used to predict storm movement.\nThe dataset spans storms recorded from 1870 up to 2025, filtered to include only those within the West Pacific region—our geographical focus. The raw dataset can be accessed publicly here: https://data.humdata.org/dataset/vnm-ibtracs-tropical-storm-tracks#\nCleaning and Physics-Informed Feature Engineering One advantage of IBTrACS is that it is already well-maintained and consistent. Only minimal preprocessing is needed, primarily the removal of missing values.\nAfter cleaning, we apply our first step of physics-informed machine learning—a technique that injects physical knowledge directly into the data pipeline. From the latitude–longitude coordinates, we compute two additional features using the Haversine formula:\nDistance between consecutive storm points Bearing (direction of movement) These features are physically meaningful: they represent real-world movement patterns instead of arbitrary transformations. They enrich the dataset by giving the model more context about the storm’s momentum and direction, thereby improving learning efficiency and prediction accuracy.\nFigure 1 : Dataset Description\rData for Display The data used for display on the platform is different from the data used for training, even though both originate from NOAA. The training dataset is static and historical, but the display dataset must always reflect the current storm conditions.\nTo achieve this, we implement a scheduled AWS Lambda function that automatically retrieves the latest storm-track updates at the end of each day. This ensures that the platform always presents the most recent and accurate information to users.\nThe processed display data is stored as a JSON file in an Amazon S3 bucket. When a user accesses the website:\nThe frontend sends a request to API Gateway API Gateway triggers the appropriate Lambda function The Lambda function fetches the JSON from S3 The resulting data is returned to the user for visualization This pipeline guarantees real-time, serverless, cost-effective data delivery.\nThe live dataset used for display can be accessed here: https://ncics.org/ibtracs/\nFigure 2 : Web to crawl data\r"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "ONLINE PLATFORM FOR TRACKING AND FORECASTING HURRICANE TRAJECTORY Geodesic-Aware Deep Learning for Hurricane Trajectory Prediction: A Physics-Informed and Augmentation-Driven Approach 1. Executive Summary Time-series data serves as one of the most fundamental representations of information in modern scientific and industrial applications. It is essential for understanding dynamic processes such as economic trends, energy consumption patterns, and meteorological changes over time. In particular, weather forecasting heavily relies on time-series data to predict future atmospheric conditions, hurricane trajectories, and seasonal anomalies based on historical records.\nWith the rapid progress in deep learning and neural network research, our project aims to develop an advanced forecasting model capable of accurately predicting the future path, intensity, and total travel distance of moving storms within the next few days. The model’s predictions can support early warning systems, allowing authorities and residents in affected regions to take precautionary measures well before a hurricane reaches their area.\nTo move beyond the limitations of current technologies and existing research, this study introduces several novel techniques and algorithms, including two new augmentation methods for geodesic time-series data and a spatial encoding mechanism designed to enhance the predictive performance of convolutional computing. The final system will be integrated into a cloud-based, serverless architecture on AWS, ensuring scalability, high availability, and cost efficiency for real-time storm tracking and analysis.\nThe system architecture leverages several AWS services to form a fully managed data processing and deployment pipeline. AWS Lambda functions serve as the backbone for serverless computation, automatically triggered by Amazon EventBridge to crawl and process new storm data from open meteorological sources on a scheduled basis. Processed data is stored securely in Amazon S3, while AWS CodePipeline and CodeBuild automate the continuous integration and deployment of new model versions. The trained model is hosted and exposed via Amazon API Gateway, enabling lightweight, real-time inference requests from the online forecasting platform. All system activities are monitored through Amazon CloudWatch, providing operational visibility, fault detection, and performance metrics.\nThe first proposed method, Stepwise Temporal Fading Augmentation (STFA), is a new time-series augmentation framework that models the natural decline in the influence of past observations. Unlike traditional approaches based on random perturbation or noise injection, STFA applies fading weights to earlier time steps while preserving recent information. This process generates realistic and diverse synthetic sequences, improving model robustness and generalization. The technique will be evaluated on hurricane trajectory prediction tasks that rely on sequential latitude–longitude data.\nThe second proposed technique, Plausible Geodesic Bearing Augmentation (PGBA), introduces an augmentation strategy based on the feasible range of storm bearings and distances. By analyzing the geodesic bearing and distance between consecutive storm locations, PGBA defines a realistic motion boundary within which new synthetic trajectories are generated. This approach enhances the model’s capacity to capture natural spatial variability and directional uncertainty in storm movements.\nAdditionally, this study explores a spatial–temporal representation of time-series data, enabling the application of convolutional neural networks (CNNs) to capture both spatial and temporal dependencies. This representation leverages the strengths of convolutional computing to model local interactions across space and time. It will serve as a baseline for comparison with Temporal Convolutional Network (TCN) models trained using the proposed augmentation methods.\nTraditional neural networks, whether used for sequential or image-based modeling, primarily learn statistical patterns from data. However, in many real-world physical systems, such purely data-driven models may fail to adhere to natural constraints, such as gravitational or geodesic relationships. To address this limitation, we incorporate the principles of Physics-Informed Machine Learning (PIML) into our approach. Specifically, geodesic distance and bearing are derived from the latitude–longitude data and integrated into the model’s training process as physically meaningful features. Furthermore, we employ the Haversine formula—which computes the spherical distance between two points—as an auxiliary loss term, complementing standard error metrics such as MSE, RMSE, MAE, and MAPE.\nBy combining the proposed augmentation methods, physics-informed learning principles, and a serverless deep learning infrastructure powered by AWS services, this research aims to develop a scalable, robust, and accurate framework for hurricane trajectory forecasting. The resulting system not only advances the state of geodesic time-series modeling but also demonstrates the practicality of deploying AI-driven environmental prediction systems as resilient, cloud-native applications that enhance preparedness and safety in hurricane-prone regions.\n2. Problem Statement What’s the Problem? To develop a reliable platform for tracking and issuing alerts on future hurricane movements, it is essential to construct a machine learning model that is both accurate and capable of producing dependable predictions. The tracking component can be addressed by continuously collecting and updating data from publicly available meteorological sources. However, these datasets are often geographically constrained and contain redundant or incomplete information.\nIn contrast, the predictive component presents greater complexity. Achieving accurate time-series forecasting in this context typically faces two primary challenges: (1) the limited diversity and coverage of available data, and (2) the absence of physical grounding, which restricts the model’s ability to reflect the underlying geophysical dynamics of hurricane behavior.\nData scarcity: Many time-series forecasting tasks suffer from limited training data. While there are various augmentation methods, few approaches directly focus on the declining importance of past values over time.\nPhysics ignorance: Most neural networks only learn from raw data, without considering real-world physical constraints. In trajectory prediction tasks (e.g., hurricanes), this often leads to unrealistic predictions.\nWe aim to:\nDevelop a new time-series augmentation method (STFA and PGBA) to improve robustness and generalization. Incorporate physics-based constraints into model training, bridging the gap between data-driven learning and real-world dynamics. Testing the strength of convolution network (convol2D) in term of forecasting trajectory Creating an online platform that provide latest information about currents storm and precisely predictions on their trajectory The Solution A - Stepwise Temporal Fading Augumentation STFA generates synthetic time-series sequences by gradually reducing the influence of earlier values. Unlike random noise injection, it systematically applies stepwise fading multipliers across bands of older data.\nLet a univariate sequence be:\n$$ X = [x_0, x_1, \\ldots, x_{T-1}] $$\nwhere $T$ is the sequence length of $X$.\nParameters:\n$n$: number of most steps to remain unchanged. $S$: number of step-bands to apply fading, each band is assigned a constant multiplier. $L = T - n$: length of the fading region. $k = \\frac{L}{S}$: values per band. $I_b$: index set of the $b$-th band. \\[ I_b = {\\ {i \\mid L - b \\cdot k ;\\leq; i ;\\leq; L - (b-1)\\cdot k - 1} ,} \\]\nTransformation:\nWe denote the augmented series as:\n$$ X = [x_0, \\ldots, x_{T-1}] $$\nwith the transformation rules:\n$$ x_t = \\begin{cases} x_t, \u0026amp; t \\in {T-n, \\ldots, T-1}, \\\\ m_b , x_t, \u0026amp; t \\in I_b, \\\\ m_{S+1} , x_t, \u0026amp; t \u0026lt; \\min(I_S), \\end{cases} $$\nwhere multipliers $m_b \\in (0,1)$ decrease monotonically from recent to older bands.\nThis formulation preserves the fidelity of recent history while exerting stronger control on the long-range influence of the sequence. The augmentation forces the model to focus on robust patterns beyond the raw data, while increasing diversity according to the chosen parameters.\nB - Plausible Geodesic Bearing Augmentation The Plausible Geodesic Bearing Augmentation (PGBA) technique enhances the realism and control of synthetic trajectory generation in geospatial time-series forecasting tasks. Unlike conventional random perturbation methods, PGBA introduces stochasticity that remains plausible within the physical constraints of the underlying motion. The generated trajectories are derived from the geometric relationships of past observations rather than from purely random steps, resulting in smoother paths and meaningful variability in the training dataset. This technique apply on every four locations in a data sequence.\nPGBA serves as a complementary augmentation to STFA, enriching the diversity of training samples while preserving the original dynamical structure. Its objective is to create redundant yet physically consistent trajectories that capture potential variations in storm movements or similar geospatial phenomena.\nCore Mechanism\nConsider a storm trajectory represented by a sequence of $n$ ordered geographical points:\n$$ P = [P_1, P_2, \\ldots, P_n] $$\nWe split this sequence into small blocks of 4 points, with $P_i$ as the starting point of each block, defined by its latitude and longitude:\n$$ P_i = (\\phi_i, \\lambda_i) $$\nHere, $\\phi_i$ and $\\lambda_i$ denote latitude and longitude in radians, respectively.\nThe geodesic distance $d_i$ and bearing $\\theta_i$ between consecutive points $P_i$ and $P_{i+1}$ are defined as:\n$$ d_i = \\text{Distance}(P_i, P_{i+1}), \\qquad \\theta_i = \\text{Bearing}(P_i, P_{i+1}) $$\nTo introduce plausible variability, PGBA perturbs the bearing by adding a small, uniformly distributed random noise $\\epsilon_i$:\n$$ \\theta_i^{\\text{aug}} = \\theta_i + \\epsilon_i, \\qquad \\epsilon_i \\sim \\text{Uniform}(-\\delta, \\delta) $$\nwhere $\\delta$ is a tunable angular bound controlling the range of deviation.\nThe first two points always remain unchanged and are used to compute distance and bearing. The subsequent augmented point is then computed using the geodesic destination formula, keeping the distance constant while allowing the bearing to vary within the range of the random noise:\n$$ P_{i+2}^{\\text{aug}} = \\text{Destination}(P_i, d_i, \\theta_i^{\\text{aug}}) $$\nThis process preserves the inter-point distance $d_i$ while slightly perturbing the direction to produce physically plausible deviations.\nMulti-Step Smoothing and Correction\nTo enhance spatial smoothness and generate curvilinear trajectories, PGBA applies a secondary correction at every fourth point. Let $P_{i+3}^{\\text{aug}}$ denote the fourth point. It is recomputed such that its bearing $\\theta_{i+3}^{\\text{corr}}$ minimizes the deviation from the original point $P_{i+3}$:\n$$ \\theta_{i+3}^{\\text{corr}} = \\arg\\min_{\\theta}, \\text{Distance}\\Big( \\text{Destination}(P_{i+2}^{\\text{aug}}, d_{i+2}, \\theta),; P_{i+3} \\Big) $$\nThen, the corrected augmented point is obtained as:\n$$ P_{i+3}^{\\text{aug}} = \\text{Destination}(P_{i+2}^{\\text{aug}}, d_{i+2}, \\theta_{i+3}^{\\text{corr}}) $$\nThis step ensures smooth transitions across multiple points while maintaining physical plausibility.\nNote that the first two and last locations in every geodesic time-series sequence always remain unchanged.\nC - Physics-Informed Machine Learning Neural network models such as RNNs, CNNs, and Transformers do not require explicit formulas or task-specific rules to perform well, provided that they are trained with sufficient data. For example, in machine translation tasks such as German-to-English translation using an RNN, no explicit grammar rules are provided during training. Nevertheless, the model is capable of producing coherent translations, which demonstrates one of the major strengths of deep learning: the ability to learn complex patterns directly from data. In contrast, traditional approaches—such as early versions of rule-based translation systems (e.g., Google Translate prior to the 2000s)—relied heavily on grammar rules and dictionaries. While precise, such systems often lacked flexibility and failed when encountering words with multiple meanings or when handling context-dependent structures.\nInspired by this, our goal is to combine the strengths of deep learning with human-defined formulas in order to achieve better performance. Specificly in this geography field, we will try to add the benefit from Haversine formula into training for distance and beering calculation between two location on a sphere. These provide the model with additional structure and inductive bias, guiding learning beyond purely statistical correlations.\nHaversine Formula\nFor distance calculation\nThe Haversine formula is used to calculate the great-circle distance between two points on the surface of a sphere — that is, the shortest path over the Earth’s surface.\n$$ d = 2r , \\arcsin!\\left( \\sqrt{ \\sin^2!\\left(\\frac{\\Delta \\varphi}{2}\\right) + \\cos(\\varphi_1)\\cos(\\varphi_2) \\sin^2!\\left(\\frac{\\Delta \\lambda}{2}\\right) } \\right) $$\nWhere:\n$\\varphi_1, \\lambda_1$ and $\\varphi_2, \\lambda_2$ are the latitudes and longitudes of the two points (in radians). $\\Delta \\varphi = \\varphi_2 - \\varphi_1$ $\\Delta \\lambda = \\lambda_2 - \\lambda_1$ $r$ is the Earth’s radius (≈ 6,371 km). In our framework, instead of relying solely on standard loss functions such as MSE, RMSE, or MAPE, we propose using the Haversine formula for distance calculation as the primary loss function. As the model outputs latitude and longitude coordinates for the next hurricane location, the Haversine formula directly measures the distance between predicted and ground-truth points. A distance close to 0 indicates a highly accurate prediction, while a large distance signals a significant error.\nFor beering calculation\nThe Bearing calculation is dereived from Haversine Formula gives the direction from one geographic point to another along the great-circle path:\n$$\\theta = \\text{atan2}!\\left(\\sin(\\Delta \\lambda)\\cos(\\varphi_2),, \\cos(\\varphi_1)\\sin(\\varphi_2) - \\sin(\\varphi_1)\\cos(\\varphi_2)\\cos(\\Delta \\lambda)\\right)$$\nWhere:\n$(\\varphi_1, \\lambda_1)$ is the start point. $(\\varphi_2, \\lambda_2)$ is the end point. $\\Delta \\lambda$ is the difference in longitude. The result $\\theta$ represents the initial bearing (azimuth) measured clockwise from true north.\nIn our implementation, we fully exploit the use of Haversine Formula to compute two additional features — “distance” and “bearing” — which are appended to the dataset.\nThese features provide the model with richer information about hurricane trajectories while maintaining the core objective of predicting the next geographic location.\nD - Short overview of misconceptions in Common Approaches to Sequence Modeling In the field of sequence modeling within deep learning, recurrent architectures such as RNNs, LSTMs, and GRUs are often considered the default solutions. This perception has led many practitioners and researchers to overlook alternative architectures, particularly convolutional neural networks (CNNs), which are traditionally associated with image processing tasks. Textbooks and courses frequently categorize tasks such as language modeling, translation, or other sequential predictions as the domain of recurrent networks, while convolutional networks are presented primarily in the context of spatial data like images. As a result, the potential of CNNs for sequence modeling is frequently underestimated or ignored.\nConvolutional networks offer several intrinsic advantages that make them well-suited for sequential data. Their inherent parallelism allows for significantly faster training compared to strictly sequential models. Additionally, CNNs are highly effective at capturing local spatial and temporal dependencies, a property that can be leveraged in time-series forecasting and other sequential tasks. Despite these benefits, CNNs are often misunderstood as being unsuitable for sequences due to their lack of explicit memory mechanisms and the absence of intrinsic temporal ordering.\nIn our study, we focus on hurricane trajectory prediction, where the data consists of time-series records of latitude and longitude coordinates. We demonstrate that this type of sequential data can be effectively modeled using CNNs, leveraging their computational efficiency and ability to capture local spatiotemporal patterns. To facilitate this, we encode the locations into a 2D matrix representation, enabling the convolutional network to more effectively extract and learn patterns from the data. Each entry in the matrix corresponds to a “pixel” of an image, an approach we refer to as Trajectory-as-Image. Our methodology employs a standard CNN as a baseline, which is then compared with more specialized sequence models, including TCNs, LSTMs, and RNNs, while incorporating various data augmentation techniques, including two novel methods proposed in this work.\nThrough systematic experimentation and evaluation, we aim to challenge the prevailing notion that convolutional architectures are ill-suited for sequential data. By highlighting the effectiveness of CNNs in sequence modeling, we hope to broaden the perspective of researchers and practitioners, encouraging them to explore convolutional computing as a viable and competitive approach in time-series forecasting and other sequential prediction tasks.\nBenefits and Return on Investment Performance Boost: STFA + PGBA generates structured synthetic sequences that enhance model robustness, reduce overfitting, and improve generalization on unseen storm trajectories.\nPhysics Awareness: Incorporating geographical principles such as distance and bearing increases interpretability and ensures physically consistent predictions.\nNew Research Direction: Establishes two novel paradigms for time-series augmentation based on temporal relevance fading, expanding the methodological toolkit for sequence learning.\nScalability and Reusability: The combined STFA + PGBA + PIML framework can be extended to other sequential forecasting domains such as energy demand, traffic flow, and financial trends.\nOverall Impact: By improving predictive stability and interpretability while maintaining scalability, the proposed approach delivers both scientific value and practical return on computational investment.\n3. Solution Architecture The online platform provides users with up-to-date information on recent storms and a powerful tool for hurricane trajectory predictions. Visitors can either view recent storm data or run predictions using the ML models. The results are displayed interactively on a map, showing storm location, time, and predicted path.\nThe platform is built using a serverless AWS architecture to reduce operational costs while maintaining scalability and reliability. Frontend content is hosted on Amazon S3 and delivered globally via CloudFront, ensuring low-latency access. Users’ requests are routed through API Gateway to Lambda functions, which handle prediction computations and data retrieval. Pre-trained ML models and recent storm datasets are securely stored in S3, with weekly updates managed automatically by EventBridge-triggered crawler Lambdas. Sensitive API keys are stored in Secrets Manager, and system performance is monitored through CloudWatch logs and metrics. IAM enforces least-privilege access for all services.\nThe predictive models leverage the proposed STFA (Stepwise Temporal Fading Augmentation) and PGBA (Plausible Geodesic Bearing Augmentation) techniques. These methods generate realistic synthetic time-series trajectories, preserving temporal relevance and spatial consistency, which significantly improves the robustness and accuracy of the models. By integrating STFA and PGBA, the platform delivers more precise hurricane path predictions, helping users better understand storm behavior and make informed decisions.\nThis architecture enables a responsive, cost-efficient, and secure platform where users can visualize real-time storm information and explore hurricane trajectory forecasts with interactive maps, supported by advanced augmentation methods to boost model performance.\nFigure 1 : Model pipeline\rFigure 2 : Platform Architecture\rAWS Services Used Amazon S3: Stores static frontend files, pre-trained ML models, and recent storm data. AWS Lambda: Runs prediction models, fetches storm data, and executes web crawling automation. Amazon API Gateway: Handles frontend requests for predictions and storm data. Amazon CloudFront: Delivers static content globally with low latency. Amazon Route 53: Routes user traffic to CloudFront. Amazon EventBridge: Schedules weekly data crawling. AWS Secrets Manager: Stores external API keys securely. Amazon CloudWatch: Monitors Lambda logs, performance metrics, and system health. AWS IAM: Assigns least-privilege access to all services. Component Design Frontend Layer: Hosted on S3 and delivered through CloudFront. Backend Layer: Handles prediction and data retrieval using API Gateway and Lambda. Data Storage: ML models stored in S3, recent storm data updated weekly by the crawler. Automation: EventBridge triggers a weekly Crawler Lambda to fetch external storm data. Security \u0026amp; Monitoring: Secrets stored in Secrets Manager, metrics and logs collected via CloudWatch. 4. Technical Implementation Implementation Phases This project has three main parts: building the prediction pipeline, setting up data crawling, and deploying the web platform. Each part follows four phases:\nDesign Architecture: Plan AWS serverless stack, Lambda functions, S3 structure. (Weak 1) Estimate Costs: Use AWS Pricing Calculator to assess feasibility and adjust design (Weak 1-2). Optimize Architecture: Fine-tune Lambda memory, S3 usage, and caching to reduce cost (Weak 2-4). Develop, Test, Deploy: Implement Lambda functions, event scheduling, ML model integration, and web frontend with Next.js (Weak 4-8). Technical Requirements\nML Models: Pre-trained trajectory models stored in S3 (.h5/.pth), loaded by prediction Lambda. Storm Data: Weekly updated JSON files, stored in S3, used for frontend display and prediction validation. Serverless Infrastructure: Lambda for prediction, fetching, and crawling; API Gateway for frontend requests; CloudFront/S3 for content delivery. Security: Secrets Manager for API keys, IAM for least-privilege access. Monitoring: CloudWatch for logging and Lambda Insights metrics. 5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Weak 1): Planning, research on external weather APIs, and ML model preparation.\nInternship (Weak 1-8):\nWeak 1-2: AWS study, architecture design, and cost estimation. Weak 2-4: Optimize architecture, configure serverless workflow, and integrate ML models. Weak 4-8: Implement Lambda functions, set up frontend, test system, and deploy to production. Post-Launch: Continuous data collection and monitoring for up to 1 year.\n6. Budget Estimation Region: ap-southeast-1 (Singapore)\nThe estimated monthly costs for running the hurricane prediction platform on AWS are as follows:\nA. Frontend \u0026amp; Content Delivery\nAmazon S3 (Static Files): Hosts 5 GB of frontend files (HTML, CSS, JS) and handles 10 GB of data transfer per month. Cost ≈ $0.54/month. Amazon CloudFront: Handles 50 GB of data transfer and up to 1 million requests (within free tier). Cost ≈ $6.00/month. Amazon Route 53: 1 hosted zone and 1 million DNS queries per month. Cost ≈ $0.90/month. AWS Certificate Manager (ACM): Provides TLS certificate for secure HTTPS access. Free of charge. Subtotal for Frontend \u0026amp; CDN: ≈ $7.4/month\nB. Backend (API \u0026amp; ML Processing)\nAmazon API Gateway: Handles 1 million HTTP API requests per month, each request approximately 1 MB in size. Cost ≈ $2.5/month. Lambda (Storm Prediction): Predicts hurricane trajectories using ML models. Runs ~1,000 times per day with 512 MB memory allocated and 1 GB ephemeral storage. Each execution lasts ~5 seconds. Cost ≈ $2.54/month. Lambda (Fetch Recent Storm Data): Fetches storm data from S3 for the frontend. Runs ~20,000 times per day with 512 MB memory and 512 MB ephemeral storage for 1 second per execution. Cost ≈ $0.00/month (covered by free tier). Subtotal for Backend: ≈ $4.54/month\nC. Automation \u0026amp; Data Crawling\nAmazon EventBridge: Schedules weekly crawling of storm data (1 cron trigger per day). Free under AWS free tier. Lambda (Web Crawler): Fetches data from external APIs weekly. Uses 128 MB memory and 512 MB ephemeral storage, ~30 seconds per execution. Cost ≈ $0.00/month (free tier). AWS Secrets Manager: Stores 5 API keys for secure access to external weather services. Cost ≈ $2.00/month. Subtotal for Automation \u0026amp; Data Crawling: ≈ $2.0/month\nD. Monitoring \u0026amp; Logging\nAmazon CloudWatch Logs: Collects logs from all Lambda functions and delivers to S3 with 1-month retention. Approximately 2 GB of logs per month. Cost ≈ $0.57/month. CloudWatch Metrics (Lambda Insights): Monitors 8 metrics across Lambda functions. First 10 metrics are free, and only record for predictions and crawl data. Cost ≈ $0.00/month. Subtotal for Monitoring \u0026amp; Logging: ≈ $0.57/month\nE. Storage \u0026amp; Data Transfer\nS3 (Model Bucket): Stores ML models (~1 GB) and handles ~60,000 GET requests per month. Cost ≈ $0.05/month. S3 (Recent Storms Bucket): Stores recent storm data (~1 GB) with ~60,000 GET requests and 30 PUT requests per month. Cost ≈ $0.27/month. F. Tooling \u0026amp; Migration\nAWS CodePipeline and CodeBuild for 10 minutes fixing ≈ $0.90/month. Subtotal for Storage \u0026amp; Data Transfer: ≈ $0.32/month\nTotal Estimated Monthly Cost\nFrontend \u0026amp; CDN: $7.4 Backend (API + ML): $4.54 Automation (Crawler + Secrets): $2.0 Monitoring \u0026amp; Logging: $0.57 Storage \u0026amp; Data Transfer: $0.32 Tooling \u0026amp; Migration: $0.9 TOTAL ≈ $15.73/month\n7. Risk Assessment Risk Matrix\nNetwork Outages: Medium impact, medium probability. Data Source Unavailability: Medium impact, low probability. ML Model Errors: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies\nNetwork: Cache recent storm data in S3 to allow frontend display during outages. Data Sources: Store historical storm data for fallback. ML Model: Regular model validation and testing. Cost: Monitor AWS usage and set budget alerts. Contingency Plans\nSwitch to manual updates if external API fails. Rollback to previous ML model using S3 versioning if new model fails. 8. Expected Outcomes Technical Improvements:\nReal-time hurricane trajectory predictions with visualized paths. Scalable serverless system capable of handling thousands of requests/day. Long-term Value:\nCentralized hurricane data for research and analysis. Framework reusable for other geospatial prediction tasks. Low monthly operational cost (\u0026lt; $20/month). "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Review initial training results to determine the direction for fine-tuning. Adjust key model components to improve forward-pass stability and prediction consistency. Perform fine-tuning training to achieve smoother and more reliable loss progression. Evaluate post-tuning predictions to verify improvements over the initial training phase. Complete the remaining AWS learning tasks needed for later deployment activities. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Reviewed initial training outputs to determine fine-tuning direction + Checked misaligned predictions + Marked cases showing unstable behavior 11/11/2025 12/11/2025 - 2 - Adjusted model components for fine-tuning + Tweaked activation/hidden configurations + Cleaned forward-pass logic + Updated preprocessing effects 12/11/2025 14/11/2025 - 3 - Performed fine-tuning training and monitored improved loss progression 14/11/2025 15/11/2025 - 4 - Conducted post-fine-tune evaluation tests - Compared updated predictions with Week 9 training results 15/11/2025 16/11/2025 - 5 - Completed final AWS learning tasks and wrapped up environment-related studies essential for later deployment 16/11/2025 17/11/2025 AWS Docs Week 10 Achievements: Improved the model through fine-tuning and noticed smoother loss reduction compared to the initial training phase.\nIdentified key improvements after adjusting several components of the model.\nClearer forward-pass consistency Less erratic prediction behavior Better alignment between predicted and expected trajectories Completed the remaining AWS learning required for upcoming deployment steps, including environment preparation and logging considerations.\nEvaluation after fine-tuning demonstrated more stable outputs across multiple storms, showing improved generalization.\nCompiled refined notes for the next phase, focusing on how the fine-tuned model behaves in edge-case conditions.\nOutlier performance checks Potential preprocessing adjustments Architectural elements worth revisiting later "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Review training metrics to better understand model behavior and identify areas needing deeper analysis. Evaluate model predictions across multiple storm samples to detect large deviations and recurring error patterns. Perform extended error analysis to understand weaknesses across different storm types. Begin integrating the trained model into the API layer and validate output formatting. Conduct initial deployment tests to ensure predictions display correctly on the web interface. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Reviewed overall training results to identify which metrics need deeper analysis - Checked loss trend across multiple epochs 18/11/2025 19/11/2025 - 2 - Evaluated model predictions on several storm samples + Compared predicted tracks with ground truth + Highlighted sequences with large deviation 19/11/2025 21/11/2025 - 3 - Conducted extended error analysis to detect recurring issues - Reviewed performance differences between storm types 21/11/2025 22/11/2025 - 4 - Started integrating the trained model into the API structure + Prepared inference function for API output + Validated response format for web consumption 22/11/2025 23/11/2025 - 5 - Performed initial deployment tests on the web interface by calling the model’s API endpoint - Checked if predictions render correctly on the web UI 23/11/2025 24/11/2025 - Week 11 Achievements: Gained a clearer understanding of the model’s behavior after reviewing training metrics and analyzing how the loss evolved across epochs.\nEvaluated the model on multiple storm samples and identified patterns in which types of storms caused the largest deviations.\nCases with turning trajectories Situations where speed changes abruptly Outliers with inconsistent track direction Performed a deeper error analysis that helped highlight recurring weaknesses and provided direction for upcoming refinements.\nRan several inference tests locally and confirmed that the output format is stable enough to be used by downstream components.\nSuccessfully began integrating the trained model into the API layer.\nPrepared the inference response logic Ensured the API returns consistent JSON output Checked compatibility with the web interface Verified that the initial API deployment worked on the web version and that predicted tracks could be rendered properly on the UI.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Finalize all project documentation for submission. Complete the technical report covering preprocessing, model evaluation, and system workflow. Prepare and refine the final presentation slides for the project demo. Conduct full end-to-end testing to ensure the deployed system works reliably. Organize the repository and create a clear runbook for project handover. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Began preparing the final project documentation + Wrote system overview section + Described model architecture and training summary 25/11/2025 26/11/2025 - 2 - Continued writing technical report for final submission - Completed sections on preprocessing, evaluation metrics, and error analysis 26/11/2025 27/11/2025 - 3 - Started building the final presentation slides + Added architecture diagrams + Included API flow and web integration details 27/11/2025 28/11/2025 - 4 - Performed final end-to-end demo testing on the deployed system - Checked API → web flow and confirmed predictions render correctly without issues 28/11/2025 29/11/2025 - 5 - Cleaned up the repository and wrote the project runbook for handover 29/11/2025 29/11/2025 - Week 12 Achievements: Completed the final project documentation with clear explanations of the system workflow and model behavior.\nFinished writing the technical report that includes preprocessing steps, model evaluation, and error analysis, helping the project become easier to review and maintain.\nCreated the final presentation slides for the closing demonstration.\nAdded diagrams of the model pipeline and AWS integration Included API response examples Highlighted improvements from earlier weeks Successfully validated the full end-to-end system through final demo testing, confirming that the API and web interface work reliably together.\nOrganized the repository and prepared the project runbook for handover.\nIncluded deployment instructions Documented rerun steps for preprocessing and inference Added notes for future improvements "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/5-workshop/5.3-ml-model/",
	"title": "Machine Learning Model",
	"tags": [],
	"description": "",
	"content": "MODEL TRAINING PROCESS This work presents the development of a predictive model designed to estimate a storm’s next geographical position using historical observational data from its previous path. To put it simply, we use a sequence of past latitude and longitude values to predict the next latitude and longitude in the future.\nFeature Engineering After completing the data preprocessing stage, we split the dataset into 70% training, 10% validation, and 20% testing. This is done by storm ID, ensuring that no storm appearing in the validation or test set is included in the training set. This prevents data leakage and ensures the reliability of evaluation results.\nFor model training, each input consists of a sequence of 4 consecutive time steps, where each step represents an observation interval of 3 hours. Thus, one input sequence covers a total of 9 hours of storm movement.\nFigure 1 : Dataset Distribution\rApplying Stepwise Temporal Fading Augmentation (STFA) To enhance the diversity of the training data, we apply our proposed method—Stepwise Temporal Fading Augmentation (STFA)—to 50% of the training set, selected based on unique storm IDs. The original sequences from these storms are replaced by their augmented counterparts, ensuring the final size of the training set remains unchanged (approximately 100% of the original size).\nAs introduced earlier in the proposal section, STFA modifies the older points in a sequence while keeping the most recent observations unchanged. For each 4-step sequence:\nThe latest 2 time steps remain the same The older 2 time steps are scaled using fading coefficients: [0.98,; 0.99] Although these values appear small, latitude and longitude are extremely sensitive features. Even a tiny change—such as from 6.7 to 6.8—can correspond to tens of kilometers of displacement in the real world. Therefore, using modest fading values is both logical and physically meaningful, ensuring the augmented data remains realistic.\nExample of STFA on a 4-Step Sequence Here is a simple example showing how a 4-step time-series sequence is transformed after applying STFA:\nRow Original (lat, lon) Augmented (lat, lon) Operation 1 [-6.8 , 107.5] [-6.66 , 105.35] multiplied by 0.98 2 [-7.0 , 107.1] [-6.93 , 106.03] multiplied by 0.99 3 [-7.3 , 106.7] [-7.3 , 106.7] unchanged 4 [-7.5 , 106.4] [-7.5 , 106.4] unchanged This process reduces the magnitude of older observations while keeping the recent ones intact. The augmentation introduces controlled variability, helping the model generalize better for trajectory forecasting.\nEarlier, we used physics-informed machine learning to compute distance and bearing between time steps using the Haversine formula. After STFA is applied, these values are recomputed based on the augmented coordinates. This guarantees that all physical features remain accurate and consistent with the updated trajectory.\nFigure 2 : Comparison of Augmentation Techniques on Storm Trajectories\rModel Setting 1.Physics-Informed Loss Our use of the Haversine formula does not end at feature engineering. In addition to generating distance and bearing values, we also incorporate the Haversine distance as a custom loss function, alongside traditional losses such as MSE, RMSE, and MAPE.\nBecause the Haversine formula computes the true geodesic distance between two geographical points, it serves as a natural metric for evaluating the error in the model’s predicted storm location. A larger Haversine distance indicates that the prediction is far from the true position, whereas a smaller value means the model is performing well. The optimal Haversine loss, ideally, is 0 km.\nThe formula was previously introduced in Section 2: Proposal, so we do not rewrite it here.\nExample:\nModel prediction: [-6.72, 107.1] True location: [-6.8, 107.5] Haversine loss: 45.06 km The value 45.06 km directly reflects the real-world positional error, making the loss physically interpretable.\nThis is what we refer to as physics-informed machine learning loss, where physical laws and domain knowledge guide the model’s optimization.\n2.Model Architect Sequence modeling tasks are traditionally handled by recurrent architectures such as RNNs, LSTMs, or GRUs. However, recent research has shown that convolution-based approaches can outperform these methods in many time-series applications.\nWhile CNNs do not inherently model sequence order, they excel at:\nextracting local spatial/temporal features parallel processing faster training stable gradients efficient scaling For our project, we adopt a convolution-based architecture as the foundation. Specifically, our main model is a Temporal Convolutional Network (TCN).\nTemporal Convolutional Network (TCN) A TCN uses 1D dilated convolutions, enabling the model to have a receptive field that expands over time, allowing it to “see” far into the past without requiring recurrence.\nExample (sequence = [a, b, c, d]):\nLayer 1 (dilation = 1): model sees [d] Layer 2 (dilation = 2): model sees [b, d] Layer 3 (dilation = 4): model can see [a, b, c, d] By stacking dilated convolutions, the network learns long-range dependencies while retaining all the benefits of fast convolution operations. Thus, TCNs combine memory of the past with computational efficiency, making them an ideal choice for storm trajectory forecasting.\n3. Model Hyperparameters Below are the key hyperparameters used in our training configuration:\nInput dimensions: 4 (latitude, longitude, distance, bearing) Hidden units: 1024 Number of TCN layers: 2 Learning rate: 1e-4 Epochs: 80 Optimizer: Adam Early stopping: patience = 6 (based on overall loss) 4. Composite Loss Function Our main training loss is a weighted combination of multiple components:\nMSE on latitude and longitude MSE on distance and bearing (auxiliary features) Haversine loss (physics-informed component) The contribution of each auxiliary loss is controlled by weighting factors:\nλ_aux = 0.5 (for distance and bearing MSE) λ_hav = 0.3 (for the Haversine loss) This design ensures that the model:\nlearns to minimize coordinate errors, respects physical displacement, and does not overfit to any single feature. By integrating physics-informed loss terms with data-driven learning, the model becomes more stable, consistent, and aligned with real-world storm dynamics.\nFigure 3 : Training Process\rEvaluation After the training process concludes and the model triggers early stopping, performing an evaluation on the test set is necessary to determine whether the model generalizes well and is ready for real-world deployment. We evaluate the model using Overall Loss, MSE, RMSE, MAPE, and Haversine distance to gain deeper insight into its performance:\nTotal Loss: 74.3849 MSE: 0.0832 RMSE: 0.2772 MAPE: 0.60% Haversine (km): 30.75 From these results, we observe that the average positional error is approximately 30 km from the true location. This level of error is acceptable because hurricanes are extremely large systems, often spanning hundreds to thousands of kilometers. The MSE of only 0.08 for latitude and longitude also indicates strong predictive accuracy, suggesting that the model can effectively estimate realistic future storm trajectories with minimal error.\nThese results further demonstrate that convolutional computations can perform very well on sequence-modeling tasks, and should be considered a strong alternative to traditional sequential architectures such as RNNs, LSTMs, or GRUs—not only for images but also for structured spatiotemporal data.\nWith the model validated, the next step is to upload the trained model to an Amazon S3 bucket and use an AWS Lambda function to load and execute it in response to user inference requests.\nThe following sections will focus on how we design and deploy our online prediction platform, making the hurricane trajectory model accessible for public use.\nFigure 4 : Evaluation Metrics\r"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Trellix achieved 35% cost savings and improved security thanks to Amazon OpenSearch Service This post introduces Trellix, a leading provider of cybersecurity solutions. Trellix emerged in 2022 from the merger of McAfee Enterprise and FireEye and serves more than 40,000 enterprise customers worldwide. Trellix offers a comprehensive, open, and familiar AI-powered security platform for enterprises. Their measures help organizations build resilience against advanced threats through automated detection, investigation, and response.\nBlog 2 - Data access control features in RAG systems This blog introduces how organizations are adopting the Retrieval-Augmented Generation (RAG) approach to combine proprietary data, domain expertise, and internal documents in order to deliver more accurate and context-aware responses. With RAG, LLMs use an external knowledge base backed by a vector store to incorporate specific knowledge data before generating an answer.\nBlog 3 - Announcing multi-account ingestion for Amazon OpenSearch Service This blog describes a setup with a central logging account and two other development accounts (A and B). The central logging account can create an OpenSearch Ingestion pipeline using a push-based source, such as HTTP. After creating the pipeline, a member of the central logging team can grant access to other teams. They can use a resource policy that grants the other two team accounts permission to create pipeline endpoints. After making this change, the OpenSearch Ingestion pipeline becomes available for the other teams to use.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.1-ai-model-integration/",
	"title": "AI-Model-Integration",
	"tags": [],
	"description": "",
	"content": "Actions to Integrate Hurricane Prediction Model from Lambda Overview Below here we will represent how did we integrate our model from lambda to use step-by-step.\nFiles Currently Using Mock Data 1. WeatherOverlay.tsx (IMPORTANT) Location: frontend/src/components/WeatherOverlay.tsx Mock data: Temperature and Wind overlay data Function: generateWeatherData() Required change: Replace with an API call to Lambda 2. WeeklyForecast.tsx Location: frontend/src/components/WeeklyForecast.tsx Mock data: mockForecast array Required change: Fetch from the backend API 3. windData.ts Location: frontend/src/lib/windData.ts Mock data: mockWindData Required change: Fetch from OpenWeatherMap or Lambda 4. WindFieldManager.ts Location: frontend/src/components/wind/WindFieldManager.ts Mock data: Fallback when there is no API key Status: OK — it already fetches from OpenWeatherMap; you just need to configure the API key How to Integrate the AI Model from Lambda Step 1: Add API endpoints for Storm Prediction In frontend/src/api/weatherApi.ts, add:\nexport interface StormPrediction { stormId: string; name: string; nameVi: string; currentPosition: { lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; }; historicalTrack: Array\u0026lt;{ lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; }\u0026gt;; forecastTrack: Array\u0026lt;{ lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; confidence?: number; // Confidence score from the AI model }\u0026gt;; } export const weatherApi = { // ... existing methods ... // Get storm predictions from the Lambda AI model getStormPredictions: async (): Promise\u0026lt;StormPrediction[]\u0026gt; =\u0026gt; { const response = await api.get\u0026lt;StormPrediction[]\u0026gt;(\u0026#39;/storms/predictions\u0026#39;); return response.data; }, // Get details for a specific storm getStormById: async (stormId: string): Promise\u0026lt;StormPrediction\u0026gt; =\u0026gt; { const response = await api.get\u0026lt;StormPrediction\u0026gt;(`/storms/${stormId}`); return response.data; }, }; Step 2: Update the Backend to call Lambda In the C# backend (backend/Controllers/WeatherController.cs), add an endpoint:\n[HttpGet(\u0026#34;storms/predictions\u0026#34;)] public async Task\u0026lt;IActionResult\u0026gt; GetStormPredictions() { try { // Call the Lambda function var lambdaClient = new AmazonLambdaClient(); var request = new InvokeRequest { FunctionName = \u0026#34;storm-prediction-function\u0026#34;, InvocationType = InvocationType.RequestResponse, Payload = \u0026#34;{}\u0026#34; // Or parameters if needed }; var response = await lambdaClient.InvokeAsync(request); using var reader = new StreamReader(response.Payload); var result = await reader.ReadToEndAsync(); return Ok(JsonSerializer.Deserialize\u0026lt;List\u0026lt;StormPrediction\u0026gt;\u0026gt;(result)); } catch (Exception ex) { return StatusCode(500, new { error = ex.Message }); } } Step 3: Update the Frontend to use the real API In frontend/src/pages/Index.tsx (or wherever storm data is fetched):\nimport { weatherApi } from \u0026#39;../api/weatherApi\u0026#39;; import { useQuery } from \u0026#39;@tanstack/react-query\u0026#39;; // Instead of using mock data const { data: storms, isLoading } = useQuery({ queryKey: [\u0026#39;storms\u0026#39;], queryFn: () =\u0026gt; weatherApi.getStormPredictions(), refetchInterval: 5 * 60 * 1000, // Refresh every 5 minutes }); Step 4: Configure Environment Variables Frontend (.env.production):\nVITE_API_BASE_URL=https://your-backend-api.com/api/weather Backend (appsettings.json):\n{ \u0026#34;AWS\u0026#34;: { \u0026#34;Region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;LambdaFunctionName\u0026#34;: \u0026#34;storm-prediction-function\u0026#34; } } Deployment Checklist Deploy the AI model to Lambda Test the Lambda function with sample input Add the API endpoint in the C# backend Test the backend endpoint Update weatherApi.ts with the new endpoints Replace mock data with real API calls Test the frontend with real data Update .env.production with the production URL Build and deploy the frontend Monitor logs and errors Notes Caching: You should cache results from Lambda to reduce cost Error handling: Handle Lambda timeouts or errors gracefully Loading states: Show loading indicators while fetching Fallback: You can keep mock data as a fallback when the API fails Files You Don’t Need to Change (Examples Only) These files are demos/examples and do not affect production:\n*.example.tsx */__tests__/* */GUIDE.md "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments. Event 1 Event Name: AI-Powered Cloud Solutions \u0026amp; Amazon Bedrock Workshop\nDate \u0026amp; Time: 08:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent Overview: The event introduced modern AI-powered cloud solutions using Amazon Bedrock, covering Foundation Models, prompt engineering, RAG, and AI agent development. Speakers demonstrated practical use cases with AWS AI Services and explained how enterprises adopt AI to modernize workflows and build scalable cloud-based products.\nKey Outcomes: I gained a clearer understanding of Foundation Models, prompt engineering techniques, and RAG workflows. I also learned how AWS tools like Bedrock, AgentCore, and AI Services accelerate development, enhance accuracy, and support building real AI-based products—valuable for both personal projects and future team contributions.\nEvent 2 Event Name: AWS Cloud, AI \u0026amp; Innovation Summit\nDate \u0026amp; Time: 09:00, September 19, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent Overview: The summit highlighted Vietnam’s cloud and AI development strategy, the growing partnership with the U.S., and real-world impacts of AI and blockchain across education, healthcare, and industry. Speakers also introduced AWS programs, responsible AI practices, and practical tools like SageMaker, Amazon Q, and QuickSight to support modern AI-driven development.\nKey Outcomes: I gained clearer insights into how cloud, AI, and blockchain accelerate national digital transformation, along with practical knowledge of AI development workflows, security considerations, and AWS services that can improve productivity and support future projects.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.2-fix-unicode-error-solution/",
	"title": "Fix-Unicode-Error",
	"tags": [],
	"description": "",
	"content": "Fixing UnicodeDecodeError in Lambda There is a series error we want to talk about during the process of development, and this section will talk about it.\nIssue UnicodeDecodeError: \u0026#39;utf-8\u0026#39; codec can\u0026#39;t decode byte 0x80 in position 64: invalid start byte Root Cause The PyTorch model uses the .pth extension (a binary file). The Python runtime also uses .pth files for path configuration (text files). If the model .pth is placed directly in LAMBDA_TASK_ROOT, Python may try to read it as text → causing the error. Applied Fix 1. Update Dockerfile Move the model into a subdirectory models/:\n# Copy model file to subdirectory to avoid Python .pth file confusion RUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ 2. Update app.py Update the model search paths:\npossible_paths = [ \u0026#39;/var/task/models/cropping_storm_7304_2l.pth\u0026#39;, # Primary location in Lambda \u0026#39;models/cropping_storm_7304_2l.pth\u0026#39;, tcn_path ] Rebuild \u0026amp; Deploy Steps Step 1: Build the Docker image cd storm_prediction docker build --provenance=false --platform linux/amd64 -t storm-prediction-model . Note: --provenance=false helps reduce image size for pushing to ECR.\nStep 2: Tag the image docker tag storm-prediction-model:latest 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Step 3: Login to ECR aws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com Step 4: Push to ECR docker push 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Step 5: Update Lambda AWS Console → Lambda → storm-prediction Open the Image tab Click Deploy new image Select the latest image Click Save Step 6: Test curl -X POST \u0026#34;https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 14.5, \u0026#34;lng\u0026#34;: 121.0}, {\u0026#34;lat\u0026#34;: 14.6, \u0026#34;lng\u0026#34;: 121.1}, {\u0026#34;lat\u0026#34;: 14.7, \u0026#34;lng\u0026#34;: 121.2}, {\u0026#34;lat\u0026#34;: 14.8, \u0026#34;lng\u0026#34;: 121.3}, {\u0026#34;lat\u0026#34;: 14.9, \u0026#34;lng\u0026#34;: 121.4}, {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 121.5}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 121.6}, {\u0026#34;lat\u0026#34;: 15.2, \u0026#34;lng\u0026#34;: 121.7}, {\u0026#34;lat\u0026#34;: 15.3, \u0026#34;lng\u0026#34;: 121.8} ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; }\u0026#39; Check Logs After Deploy aws logs tail /aws/lambda/storm-prediction --region ap-southeast-1 --follow Summary Before: The .pth model file was in the root → Python mistook it for a config .pth file → UnicodeDecodeError After: The .pth model file is inside models/ → Python ignores it → Lambda works ✅ "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/5-workshop/5.4-frontback-end/5.4.1-frontend-architecture/",
	"title": "Frontend Architecture",
	"tags": [],
	"description": "",
	"content": "Frontend Architecture - Storm Prediction Web Application Overview Below is the detailed documentation of our front-end development: a React + TypeScript web application for tracking and predicting typhoon trajectories.\nAWS Services Architecture ┌─────────────────────────────────────────────────────────────┐\r│ USER BROWSER │\r└────────────────────────┬────────────────────────────────────┘\r│\r▼\r┌─────────────────────────────────────────────────────────────┐\r│ CloudFront CDN │\r│ - Distribution: d3lj47ilp0fgxy.cloudfront.net │\r│ - SSL/TLS: HTTPS │\r│ - Cache: Static assets + JSON data │\r│ - Origin Access: OAI/OAC (Secure) │\r└────────────────────────┬────────────────────────────────────┘\r│\r▼\r┌─────────────────────────────────────────────────────────────┐\r│ S3 Bucket (Private) │\r│ - Bucket: storm-frontend-hosting-duc-2025 │\r│ - Static Website Hosting: DISABLED │\r│ - Access: CloudFront only via REST API │\r│ - Content: HTML, CSS, JS, Images, recent_storms.json │\r└─────────────────────────────────────────────────────────────┘\r┌─────────────────────────────────────────────────────────────┐\r│ Lambda Functions │\r│ ┌─────────────────────────────────────────────────────┐ │\r│ │ Lambda #1: Storm Prediction │ │\r│ │ - URL: vill3povlzqxdyxm7ubldizobu0kdgbi... │ │\r│ │ - Method: POST /predict │ │\r│ │ - Auth: NONE (public) │ │\r│ │ - Container: ECR (Docker) │ │\r│ │ - Models: LSTM + TCN │ │\r│ └─────────────────────────────────────────────────────┘ │\r│ ┌─────────────────────────────────────────────────────┐ │\r│ │ Lambda #2: Storm Data Crawler (New) │ │\r│ │ - Trigger: EventBridge (Weekly) │ │\r│ │ - Function: Crawl IBTrACS data │ │\r│ │ - Output: recent_storms.json → S3 │ │\r│ └─────────────────────────────────────────────────────┘ │\r└─────────────────────────────────────────────────────────────┘\r┌─────────────────────────────────────────────────────────────┐\r│ EventBridge │\r│ - Rule: storm-data-crawler-weekly-trigger │\r│ - Schedule: Every Sunday 00:00 UTC (7AM Vietnam) │\r│ - Target: Lambda #2 (Storm Data Crawler) │\r└─────────────────────────────────────────────────────────────┘ Frontend Directory Structure frontend/\r├── src/\r│ ├── components/ # React components\r│ │ ├── ui/ # shadcn/ui components (button, card, input, etc.)\r│ │ ├── storm/ # Storm-specific components\r│ │ ├── timeline/ # Timeline controls\r│ │ ├── wind/ # Wind visualization\r│ │ ├── StormPredictionForm.tsx # Storm coordinate input form\r│ │ ├── WeatherMap.tsx # Main Leaflet map\r│ │ ├── StormTracker.tsx # Storm list\r│ │ ├── StormInfo.tsx # Storm details\r│ │ ├── StormAnimation.tsx # Animated markers\r│ │ ├── WeatherOverlay.tsx # Temperature/Wind overlay\r│ │ ├── WeatherLayerControl.tsx # Satellite/Radar layers\r│ │ ├── WeatherLayerControlPanel.tsx # Control panel UI\r│ │ ├── WeatherValueTooltip.tsx # Hover tooltip\r│ │ ├── WindyLayer.tsx # Windy.com integration\r│ │ ├── ProvinceLayer.tsx # Vietnam provinces\r│ │ ├── OptimizedTemperatureLayer.tsx\r│ │ ├── TemperatureHeatMapLayer.tsx\r│ │ ├── ThemeToggle.tsx # Dark/Light mode\r│ │ ├── PreferencesModal.tsx # User preferences\r│ │ ├── RightSidebar.tsx # Right panel\r│ │ └── WeeklyForecast.tsx # 7-day forecast\r│ │\r│ ├── pages/\r│ │ ├── Index.tsx # Main page\r│ │ └── NotFound.tsx # 404 page\r│ │\r│ ├── lib/ # Business logic \u0026amp; utilities\r│ │ ├── api/ # API clients\r│ │ ├── __tests__/ # Unit tests\r│ │ ├── stormData.ts # Types \u0026amp; interfaces\r│ │ ├── stormAnimations.ts # Animation logic\r│ │ ├── stormIntensityChanges.ts\r│ │ ├── stormPerformance.ts\r│ │ ├── stormValidation.ts\r│ │ ├── windData.ts\r│ │ ├── windStrengthCalculations.ts\r│ │ ├── windyStatePersistence.ts\r│ │ ├── windyUrlState.ts\r│ │ ├── mapUtils.ts # Map helpers\r│ │ ├── openWeatherMapClient.ts\r│ │ ├── dataWorker.ts # Web Worker\r│ │ ├── utils.ts\r│ │ └── colorInterpolation.ts\r│ │\r│ ├── hooks/ # Custom React hooks\r│ │ ├── use-toast.ts\r│ │ ├── use-theme.tsx\r│ │ ├── use-mobile.tsx\r│ │ ├── useTimelineState.ts\r│ │ ├── useWindyStateSync.ts\r│ │ └── useSimplifiedTooltip.ts\r│ │\r│ ├── contexts/ # React Context\r│ │ └── WindyStateContext.tsx\r│ │\r│ ├── api/\r│ │ └── weatherApi.ts # API calls\r│ │\r│ ├── utils/\r│ │ └── colorInterpolation.ts\r│ │\r│ ├── styles/\r│ │ └── accessibility.css # WCAG compliance styles\r│ │\r│ ├── test/ # Test suite\r│ │ ├── accessibility.test.ts\r│ │ ├── accessibility-audit.test.ts\r│ │ ├── wcag-compliance.test.ts\r│ │ ├── performance.test.ts\r│ │ ├── cross-browser.test.ts\r│ │ └── setup.ts\r│ │\r│ ├── assets/ # Images, icons\r│ ├── App.tsx\r│ ├── main.tsx\r│ └── index.css\r│\r├── public/ # Static assets\r├── dist/ # Build output (after npm run build)\r├── .env.production # Production config\r├── .env.example\r├── package.json\r├── vite.config.ts\r├── vitest.config.ts # Test config\r├── tailwind.config.ts\r├── tsconfig.json\r└── components.json # shadcn/ui config Environment Variables .env.production # OpenWeather API VITE_OPENWEATHER_API_KEY=8ff7f009d2bd420c86845c6bcf6de4a9 # CloudFront URL - Fetch storm data VITE_CLOUDFRONT_URL=https://d3lj47ilp0fgxy.cloudfront.net # Lambda Function URL - Storm prediction API VITE_PREDICTION_API_URL=https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws ** Screenshots needed:**\nAWS CloudFront → Distributions → Distribution domain name AWS Lambda → storm-prediction → Function URL Build \u0026amp; Deploy Process 1. Build Production cd frontend npm run build Output: dist/ folder contains:\nindex.html assets/index-[hash].js assets/index-[hash].css 2. Upload to S3 aws s3 sync dist/ s3://storm-frontend-hosting-duc-2025/ --delete Important Notes:\nS3 bucket is PRIVATE (no public access) CloudFront uses REST API endpoint, not website endpoint Origin: storm-frontend-hosting-duc-2025.s3.ap-southeast-1.amazonaws.com\n3. Invalidate CloudFront Cache aws cloudfront create-invalidation \\ --distribution-id E1234567890ABC \\ --paths \u0026#34;/*\u0026#34; Data Flow A. Load Storm Data (Startup) Browser → CloudFront → S3 ↓\rGET /recent_storms.json\r↓\rParse JSON → Display on map File: src/pages/Index.tsx (line ~40)\nconst CLOUDFRONT_URL = import.meta.env.VITE_CLOUDFRONT_URL; const FETCH_URL = `${CLOUDFRONT_URL}/recent_storms.json?t=${Date.now()}`; B. Storm Prediction (User Action) User fills form → Click \u0026#34;Run Prediction\u0026#34;\r↓\rPOST /predict to Lambda Function URL\r↓\r{\r\u0026#34;history\u0026#34;: [{lat, lng}, ...],\r\u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34;\r}\r↓\rLambda processes → Returns forecast\r↓\rDisplay predicted path on map File: src/components/StormPredictionForm.tsx (line ~80)\nconst API_URL = `${import.meta.env.VITE_PREDICTION_API_URL}/predict`; const response = await fetch(API_URL, { method: \u0026#34;POST\u0026#34;, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }, body: JSON.stringify({ history, storm_name }) }); Key Components 1. Core Components StormPredictionForm File: src/components/StormPredictionForm.tsx\nFeatures:\nForm for inputting storm coordinates (min 9 points) Input validation (valid lat/lng) Calls Lambda API for prediction Displays results on map Scrollable list with Add/Remove positions Props:\ninterface StormPredictionFormProps { onPredictionResult: (result: PredictionResult) =\u0026gt; void; setIsLoading: (isLoading: boolean) =\u0026gt; void; } WeatherMap File: src/components/WeatherMap.tsx\nFeatures:\nDisplays Leaflet map Renders storm tracks (historical + forecast) Renders prediction path (purple, dashed) Weather overlays (temperature, wind, radar) Multiple storm rendering Auto-zoom to selected storm Custom panes for z-index layering Props:\ninterface WeatherMapProps { storms: Storm[]; selectedStorm?: Storm; customPrediction?: PredictionResult | null; mapFocusBounds?: LatLngBounds | null; onMapFocusComplete?: () =\u0026gt; void; } Index (Main Page) File: src/pages/Index.tsx\nFeatures:\nMain layout with header/footer State management (storms, selectedStorm, customPrediction) Sidebar with tabs (Current Storms / Predict Storm) Timeline state synchronization Loading \u0026amp; error handling Skip links for accessibility 2. Storm Components StormTracker File: src/components/StormTracker.tsx\nList of current storms Filter by status (active/developing/dissipated) Click to select storm StormInfo File: src/components/StormInfo.tsx\nDetailed storm information Wind speed, pressure, category Historical data Forecast timeline StormAnimation File: src/components/StormAnimation.tsx\nAnimated markers for storm positions Pulsing effect Category-based colors 3. Weather Layer Components WeatherOverlay File: src/components/WeatherOverlay.tsx\nTemperature heatmap overlay Wind speed visualization Real-time data from OpenWeather API Hover to view values WeatherLayerControl File: src/components/WeatherLayerControl.tsx\nSatellite imagery layer Radar layer Temperature layer Tile layer management WeatherLayerControlPanel File: src/components/WeatherLayerControlPanel.tsx\nUI controls for weather layers Opacity slider Layer toggle buttons Temperature animation toggle OptimizedTemperatureLayer \u0026amp; TemperatureHeatMapLayer Files: src/components/OptimizedTemperatureLayer.tsx, TemperatureHeatMapLayer.tsx\nPerformance-optimized temperature rendering Color interpolation Grid-based heatmap 4. Wind Components WindyLayer File: src/components/WindyLayer.tsx\nWindy.com iframe integration Wind animation overlay Synchronized state with main map Context: src/contexts/WindyStateContext.tsx\nGlobal state for Windy layer URL state persistence Sync across components 5. Map Enhancement Components ProvinceLayer File: src/components/ProvinceLayer.tsx\nVietnam provinces boundaries GeoJSON rendering Province labels WeatherValueTooltip File: src/components/WeatherValueTooltip.tsx\nTooltip displaying weather values on hover Temperature, wind speed, pressure Positioned tooltip 6. UI Components ThemeToggle File: src/components/ThemeToggle.tsx\nDark/Light mode switch Persisted preference System theme detection PreferencesModal File: src/components/PreferencesModal.tsx\nUser preferences settings Map options Display preferences RightSidebar File: src/components/RightSidebar.tsx\nAdditional info panel Collapsible sidebar WeeklyForecast File: src/components/WeeklyForecast.tsx\n7-day weather forecast Temperature trends Weather icons 7. Timeline Components Folder: src/components/timeline/\nTimeline controls for storm animation Play/Pause functionality Time scrubbing Speed controls Data Types PredictionResult File: src/lib/stormData.ts\nexport interface PredictionResult { storm_id: string; storm_name: string; prediction_time: string; totalDistance: number; // km actualDistance: number; // km lifespan: number; // hours forecastHours: number; // hours forecast: StormPoint[]; // Predicted positions path?: StormPoint[]; // Legacy support } StormPoint export interface StormPoint { timestamp: number; // Unix timestamp (ms) lat: number; lng: number; windSpeed: number; // km/h pressure: number; // hPa category: string; // \u0026#34;Typhoon\u0026#34;, \u0026#34;Super Typhoon\u0026#34;, etc. } AWS Permissions Required S3 Bucket Policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::storm-frontend-hosting-duc-2025/*\u0026#34; } ] } CloudFront Origin Access Origin: S3 bucket Origin Access: Public (or OAI if used) Testing Local Development npm run dev # Open http://localhost:5173 Production Build Test npm run build npm run preview # Open http://localhost:4173 📸 Screenshots needed:\nBrowser DevTools → Network tab → API calls Browser DevTools → Console → No errors Common Issues 1. CORS Error when calling Lambda Symptom: Access-Control-Allow-Origin error\nSolution: Lambda must return CORS headers:\nreturn { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } 2. CloudFront stale cache Symptom: New code not showing\nSolution: Invalidate cache\naws cloudfront create-invalidation --distribution-id E... --paths \u0026#34;/*\u0026#34; 3. Environment variables not loading Symptom: undefined when accessing import.meta.env.VITE_*\nSolution:\nEnsure .env.production file exists Rebuild: npm run build Variables must start with VITE_ Deployment Checklist Update .env.production with correct URLs npm run build succeeds Upload dist/ to S3 Invalidate CloudFront cache Test on production URL Verify Lambda API works Verify storm data loads Test prediction form with 9+ positions API Endpoints 1. Get Storm Data GET https://d3lj47ilp0fgxy.cloudfront.net/recent_storms.json Response: Array of Storm objects\n2. Predict Storm Path POST https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict\rBody:\r{\r\u0026#34;history\u0026#34;: [\r{\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0},\r{\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1},\r...\r],\r\u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34;\r}\rResponse:\r{\r\u0026#34;storm_id\u0026#34;: \u0026#34;unknown\u0026#34;,\r\u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34;,\r\u0026#34;totalDistance\u0026#34;: 500.5,\r\u0026#34;lifespan\u0026#34;: 72,\r\u0026#34;forecast\u0026#34;: [...]\r} Performance Optimization 1. Code Optimization Code Splitting: Vite automatically splits chunks by routes Tree Shaking: Remove unused code Minification: Production build auto-minifies JS/CSS Lazy Loading: Components load on demand 2. Data Optimization Web Workers: Heavy computations run in worker (dataWorker.ts) Memoization: React.memo for expensive components Debouncing: Input handlers are debounced Caching: LocalStorage cache for preferences 3. Rendering Optimization Virtual Scrolling: Large lists use virtual scrolling Optimized Layers: OptimizedTemperatureLayer for performance Canvas Rendering: Heatmap uses canvas instead of DOM Pane Management: Custom Leaflet panes for z-index optimization 4. Network Optimization CDN Caching: CloudFront caches static assets Image Optimization: WebP format, lazy loading API Caching: Cache storm data with timestamp Compression: Gzip/Brotli compression 5. Accessibility Performance Skip Links: Keyboard navigation shortcuts ARIA Labels: Proper semantic HTML Focus Management: Logical tab order Screen Reader: Optimized for screen readers Libraries \u0026amp; Utilities Business Logic (lib/) Storm Management stormData.ts: Types, interfaces, Storm/StormPoint definitions stormAnimations.ts: Animation logic for storm markers stormIntensityChanges.ts: Storm intensity change calculations stormPerformance.ts: Performance optimization for rendering stormValidation.ts: Storm data validation Wind System windData.ts: Wind data structures windStrengthCalculations.ts: Wind strength calculations windyStatePersistence.ts: Windy layer state persistence windyUrlState.ts: URL state management for Windy Map \u0026amp; Weather mapUtils.ts: Map helpers (center, zoom, bounds calculations) openWeatherMapClient.ts: OpenWeather API client colorInterpolation.ts: Color gradient calculations Performance dataWorker.ts: Web Worker for heavy computations utils.ts: General utilities Custom Hooks (hooks/) use-toast.ts: Toast notification system use-theme.tsx: Dark/Light theme management use-mobile.tsx: Mobile device detection useTimelineState.ts: Timeline state synchronization useWindyStateSync.ts: Windy layer state sync useSimplifiedTooltip.ts: Simplified tooltip logic Context (contexts/) WindyStateContext.tsx: Global state for Windy layer integration Testing (test/) accessibility.test.ts: Accessibility testing accessibility-audit.test.ts: WCAG audit wcag-compliance.test.ts: WCAG 2.1 compliance performance.test.ts: Performance benchmarks cross-browser.test.ts: Cross-browser compatibility setup.ts: Test environment setup Dependencies Core React 18 TypeScript Vite (build tool) Vitest (testing) UI Framework Tailwind CSS shadcn/ui (component library) Lucide Icons Radix UI (primitives) Map \u0026amp; Visualization Leaflet React-Leaflet GeoJSON support API \u0026amp; Data Fetch API (native) OpenWeather API AWS Lambda Function URL State Management React Context API URL state (query params) LocalStorage persistence Performance Web Workers Code splitting (Vite) Lazy loading Screenshots Cloudfront Distribution Figure 1\rOrigin Settings Figure 1\rInvalidations Figure 2\rstorm-frontend-hosting-duc-2025 Figure 3\rPermissions Figure 4\rstorm-ai-models-2025 Figure 5\rstorm-data-store-2025 Figure 6\rMain Page Figure 7\rStorm Tracking Features Figure 8\rStorm Details Figure 9\rPredict Feature Figure 10\rFigure 11\rFigure 12\r"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/",
	"title": "Lambda Architecture",
	"tags": [],
	"description": "",
	"content": "Lambda Architecture - Storm Prediction AI Service Overview Lambda functions are an important component of a serverless architecture. They are especially useful due to their cost-effectiveness and ease of deployment—both of which are valuable for our hurricane prediction platform.\nThis section presents the details of how we designed and built our Lambda architecture.\nOur Lambda functions run PyTorch models for typhoon trajectory prediction and are deployed using a Docker container image.\nAWS Services Architecture ┌─────────────────────────────────────────────────────────────┐\r│ Frontend (Browser) │\r└────────────────────────┬────────────────────────────────────┘\r│ POST /predict\r▼\r┌─────────────────────────────────────────────────────────────┐\r│ Lambda Function URL (Public) │\r│ URL: https://vill3povlzqxdyxm7ubldizobu0kdgbi... │\r│ Auth: NONE │\r│ Method: POST │\r└────────────────────────┬────────────────────────────────────┘\r│\r▼\r┌─────────────────────────────────────────────────────────────┐\r│ Lambda Function │\r│ Name: storm-prediction │\r│ Runtime: Python 3.10 (Container) │\r│ Memory: 3008 MB │\r│ Timeout: 120 seconds │\r│ Architecture: x86_64 │\r└────────────────────────┬────────────────────────────────────┘\r│\r▼\r┌─────────────────────────────────────────────────────────────┐\r│ ECR Repository │\r│ Account: 339570693867 │\r│ Region: ap-southeast-1 │\r│ Repo: storm-prediction │\r│ Image: latest │\r│ Size: ~2 GB │\r└─────────────────────────────────────────────────────────────┘\r┌─────────────────────────────────────────────────────────────┐\r│ S3 Buckets │\r│ 1. storm-frontend-hosting-duc-2025 │\r│ - models/lstm_totald_256_4.pt (optional) │\r│ - predictions/[storm_id]_[timestamp].json │\r│ │\r│ 2. storm-ai-models (recommended) │\r│ - models/lstm_totald_256_4.pt │\r│ - models/tcn_model.pth (backup) │\r└─────────────────────────────────────────────────────────────┘ storm_prediction/ Directory Structure storm_prediction/\r├── app.py # Lambda handler (main code)\r├── Dockerfile # Container definition\r├── requirements.txt # Python dependencies\r├── cropping_storm_7304_2l.pth # TCN model (included in image)\r│\r├── DEPLOY_NOW.md # Quick deploy guide\r├── DEPLOY_CONSOLE_STEP_BY_STEP.md # AWS Console guide\r├── LAMBDA_DEPLOYMENT_GUIDE.md # Detailed deployment\r├── AWS_CONSOLE_DEPLOYMENT_GUIDE.md\r├── FIX_ECR_PUSH_ERROR.md # Troubleshooting\r├── FIX_UNICODE_ERROR.md # UnicodeDecodeError fix\r├── FIX_UNICODE_ERROR_SOLUTION.md # Solution details\r└── REBUILD_AND_DEPLOY.sh # Automated script Docker Image Structure Dockerfile FROM public.ecr.aws/lambda/python:3.10 # Install dependencies COPY requirements.txt . RUN pip3 install -r requirements.txt \\ --target \u0026#34;${LAMBDA_TASK_ROOT}\u0026#34; \\ --extra-index-url https://download.pytorch.org/whl/cpu # Copy Lambda handler COPY app.py ${LAMBDA_TASK_ROOT} # Copy TCN model to subdirectory (avoid .pth confusion) RUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ # Set handler CMD [ \u0026#34;app.handler\u0026#34; ] Image Layers Layer 1: AWS Lambda Python 3.10 base (~500 MB)\rLayer 2: PyTorch CPU + dependencies (~1.2 GB)\rLayer 3: app.py + TCN model (~300 MB)\r─────────────────────────────────────────────\rTotal: ~2 GB AI Models 1. TCN Model (Trajectory Prediction) File: cropping_storm_7304_2l.pth Location: Inside Docker image at /var/task/models/ Size: ~300 MB Purpose: Predict next step (lat, lng) of typhoon trajectory\nArchitecture:\nclass StormTCN(nn.Module): def __init__(self, input_dim=4, hidden_units=1024, num_layers=2): self.tcn = TCN(...) self.head_latlon = nn.Linear(hidden_units, 2) # Predict lat, lng self.head_aux = nn.Linear(hidden_units, 2) # Predict aux features Input: [batch, sequence, 4] - (lat, lng, distance, bearing) Output:\npred_latlon: Next (lat, lng) pred_aux: Auxiliary features 2. LSTM Model (Total Distance Prediction) File: lstm_totald_256_4.pt Location: S3 bucket (downloaded on first use) Size: ~50 MB Purpose: Predict total distance typhoon will travel\nArchitecture:\nclass StormLSTM(nn.Module): def __init__(self, input_size=4, hidden_size=256, num_layers=2): self.lstm = nn.LSTM(...) self.fc = nn.Sequential( nn.Linear(hidden_size, hidden_size // 2), nn.ReLU(), nn.Linear(hidden_size // 2, 1) # Predict total distance ) Input: Daily summary [batch, days, 4] - (day, daily_dist, avg_speed, motion_type) Output: Total distance (km)\nRequest Flow 1. Receive Request POST /predict\rContent-Type: application/json\r{\r\u0026#34;history\u0026#34;: [\r{\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0},\r{\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1},\r... // Min 9 points\r],\r\u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34;,\r\u0026#34;storm_id\u0026#34;: \u0026#34;TEST001\u0026#34; // Optional\r} 2. Load Models (First Invocation Only) def load_models(): global LSTM_MODEL, TCN_MODEL # Load LSTM from S3 (if available) if not os.path.exists(\u0026#39;/tmp/lstm_model.pt\u0026#39;): s3_client.download_file( MODEL_BUCKET, \u0026#39;models/lstm_totald_256_4.pt\u0026#39;, \u0026#39;/tmp/lstm_model.pt\u0026#39; ) LSTM_MODEL = StormLSTM(...) LSTM_MODEL.load_state_dict(torch.load(\u0026#39;/tmp/lstm_model.pt\u0026#39;)) # Load TCN from local (already in image) TCN_MODEL = StormTCN(...) TCN_MODEL.load_state_dict( torch.load(\u0026#39;/var/task/models/cropping_storm_7304_2l.pth\u0026#39;) ) 3. Preprocess Input def preprocess_history(history): # Convert to tensor [1, sequence_length, 4] # Features: [lat, lng, distance, bearing] processed = [] for i in range(len(history)): if i == 0: processed.append([lat, lng, 0.0, 0.0]) else: dist = haversine(prev_lat, prev_lng, lat, lng) brng = bearing(prev_lat, prev_lng, lat, lng) processed.append([lat, lng, dist, brng]) return torch.tensor(processed).unsqueeze(0) 4. Predict Total Distance (LSTM) def predict_total_distance(record_tensor): if LSTM_MODEL is None: # Fallback: avg_distance * 24 steps return fallback_distance # Group by day (9 points/day) # Run LSTM prediction with torch.no_grad(): pred = LSTM_MODEL(summary_tensor, lengths) return pred.item() # km 5. Predict Path (TCN) def predict_storm_path(record_tensor, total_distance, history): seq = record_tensor.clone() gone_distance = 0 predicted_points = [] while gone_distance \u0026lt; total_distance: # Predict next position pred_latlon, pred_aux = TCN_MODEL(seq) new_lat = pred_latlon[0, -1, 0].item() new_lng = pred_latlon[0, -1, 1].item() # Calculate distance \u0026amp; bearing step_distance = haversine(last_lat, last_lng, new_lat, new_lng) # Estimate windspeed (decay over time) estimated_wind = max(avg_wind * (0.98 ** step), 30) predicted_points.append({ \u0026#39;lat\u0026#39;: new_lat, \u0026#39;lng\u0026#39;: new_lng, \u0026#39;timestamp\u0026#39;: base_timestamp + (step * 3 * 3600 * 1000), \u0026#39;windSpeed\u0026#39;: estimated_wind, \u0026#39;pressure\u0026#39;: 980.0, \u0026#39;category\u0026#39;: calculate_category(estimated_wind) }) # Update sequence (sliding window) seq = torch.cat([seq[:, 1:, :], next_point.unsqueeze(1)], dim=1) gone_distance += step_distance step += 1 return predicted_points 6. Return Response result = { \u0026#39;storm_id\u0026#39;: storm_id, \u0026#39;storm_name\u0026#39;: storm_name, \u0026#39;prediction_time\u0026#39;: datetime.now().isoformat(), \u0026#39;totalDistance\u0026#39;: 500.5, \u0026#39;actualDistance\u0026#39;: 520.3, \u0026#39;lifespan\u0026#39;: 72, \u0026#39;forecastHours\u0026#39;: 72, \u0026#39;forecast\u0026#39;: [ { \u0026#39;lat\u0026#39;: 15.1, \u0026#39;lng\u0026#39;: 106.99, \u0026#39;timestamp\u0026#39;: 1765015351626, \u0026#39;windSpeed\u0026#39;: 65, \u0026#39;pressure\u0026#39;: 980, \u0026#39;category\u0026#39;: \u0026#39;Typhoon\u0026#39; }, ... ] } return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } Build \u0026amp; Deploy Process Step 1: Build Docker Image cd storm_prediction docker build \\ --provenance=false \\ --platform linux/amd64 \\ -t storm-prediction-model . Flags:\n--provenance=false: Reduce image size (no build metadata) --platform linux/amd64: Lambda only supports x86_64 -t storm-prediction-model: Tag name Step 2: Tag for ECR docker tag storm-prediction-model:latest \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Step 3: Login to ECR aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com Step 4: Push to ECR docker push \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Time: ~5-10 minutes (2GB upload)\nStep 5: Update Lambda Function AWS Console:\nLambda → storm-prediction Tab Image → Deploy new image Select latest image Click Save Lambda Configuration Function Settings Name: storm-prediction\rRuntime: Container image\rArchitecture: x86_64\rMemory: 3008 MB\rTimeout: 120 seconds\rEphemeral storage: 512 MB Environment Variables MODEL_BUCKET=storm-frontend-hosting-duc-2025\rDATA_BUCKET=storm-frontend-hosting-duc-2025 Function URL URL: https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws\rAuth type: NONE\rCORS: Enabled\r- Allow origins: *\r- Allow methods: POST, OPTIONS\r- Allow headers: Content-Type IAM Role Permissions { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::storm-frontend-hosting-duc-2025/*\u0026#34;, \u0026#34;arn:aws:s3:::storm-ai-models/*\u0026#34; ] } ] } Monitoring \u0026amp; Logs CloudWatch Logs Log Group: /aws/lambda/storm-prediction\nKey Log Messages:\nLoading LSTM model...\rDownloaded LSTM from S3\rLSTM loaded successfully\rLoading TCN model...\rChecking: /var/task/models/cropping_storm_7304_2l.pth\rFound TCN at /var/task/models/cropping_storm_7304_2l.pth\rTCN loaded successfully\rProcessing: Test Storm (TEST001)\rInput points: 9\rPredicted total distance: 500.50 km\rGenerated 24 predictions (72 hours)\rSaved to S3: predictions/TEST001_1733486400.json Screenshots needed:\nCloudWatch → Log groups → /aws/lambda/storm-prediction Log stream → Recent logs with emojis Logs → Duration, Memory used Metrics CloudWatch Metrics:\nInvocations Duration (avg ~5-10 seconds) Errors Throttles Memory used (~500-800 MB) Screenshots needed:\nLambda → Monitor → Metrics CloudWatch → Metrics → Lambda → Function metrics Common Issues \u0026amp; Solutions 1. UnicodeDecodeError: \u0026lsquo;utf-8\u0026rsquo; codec can\u0026rsquo;t decode byte 0x80 Symptom:\nUnicodeDecodeError: \u0026#39;utf-8\u0026#39; codec can\u0026#39;t decode byte 0x80 in position 64 Cause: Model .pth file at root mistaken as Python config file\nSolution: Move model to subdirectory\nRUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ 2. 502 Bad Gateway Symptom: Frontend receives 502 error\nCauses:\nLambda timeout (exceeds 120s) Lambda crash (out of memory) Model failed to load Solutions:\nCheck CloudWatch Logs Increase memory if needed Increase timeout if needed 3. LSTM Fallback Symptom: Log shows \u0026ldquo;⚠️ Using fallback distance\u0026rdquo;\nCause: LSTM model not on S3\nSolution: Upload lstm_totald_256_4.pt to S3:\naws s3 cp lstm_totald_256_4.pt \\ s3://storm-frontend-hosting-duc-2025/models/ 4. ECR Push 403 Forbidden Symptom: 403 Forbidden when pushing image\nCauses:\nECR login expired Wrong account ID Repository doesn\u0026rsquo;t exist Solutions:\n# Re-login aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com # Create repository if needed aws ecr create-repository \\ --repository-name storm-prediction \\ --region ap-southeast-1 Testing Local Test (if possible) # Run locally python app.py # Test event test_event = { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, ... ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } result = handler(test_event, None) print(result) Lambda Test AWS Console:\nLambda → Test tab Create test event: { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, {\u0026#34;lat\u0026#34;: 15.2, \u0026#34;lng\u0026#34;: 120.2}, {\u0026#34;lat\u0026#34;: 15.3, \u0026#34;lng\u0026#34;: 120.3}, {\u0026#34;lat\u0026#34;: 15.4, \u0026#34;lng\u0026#34;: 120.4}, {\u0026#34;lat\u0026#34;: 15.5, \u0026#34;lng\u0026#34;: 120.5}, {\u0026#34;lat\u0026#34;: 15.6, \u0026#34;lng\u0026#34;: 120.6}, {\u0026#34;lat\u0026#34;: 15.7, \u0026#34;lng\u0026#34;: 120.7}, {\u0026#34;lat\u0026#34;: 15.8, \u0026#34;lng\u0026#34;: 120.8} ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } Click Test Check response cURL Test curl -X POST \\ \u0026#34;https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, {\u0026#34;lat\u0026#34;: 15.2, \u0026#34;lng\u0026#34;: 120.2}, {\u0026#34;lat\u0026#34;: 15.3, \u0026#34;lng\u0026#34;: 120.3}, {\u0026#34;lat\u0026#34;: 15.4, \u0026#34;lng\u0026#34;: 120.4}, {\u0026#34;lat\u0026#34;: 15.5, \u0026#34;lng\u0026#34;: 120.5}, {\u0026#34;lat\u0026#34;: 15.6, \u0026#34;lng\u0026#34;: 120.6}, {\u0026#34;lat\u0026#34;: 15.7, \u0026#34;lng\u0026#34;: 120.7}, {\u0026#34;lat\u0026#34;: 15.8, \u0026#34;lng\u0026#34;: 120.8} ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; }\u0026#39; Deployment Checklist Model file cropping_storm_7304_2l.pth exists (Optional) Upload LSTM model to S3 Build Docker image successfully Tag image with correct account ID (339570693867) Login to ECR successfully Push image to ECR Update Lambda function with new image Check Lambda configuration (memory, timeout) Test Lambda with test event Test via Function URL with cURL Test from frontend Check CloudWatch Logs Verify prediction results on map Screenshots Function Figure 1\rConfiguration Figure 2\rEnvironment Variables Figure 3\rECR Repository Figure 4\rFigure 5\r"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.3-lambda-deployment/",
	"title": "Lamda-Deployment",
	"tags": [],
	"description": "",
	"content": "Step to deploy our PyTorch Model on storm prediction on AWS Lambda AWS Lambda deployment plays a critical role in our website development pipeline. In this section, we document the process we followed to successfully complete the deployment.\nStep 1: Prepare the Code 1.1. Update app.py import json import torch import numpy as np from typing import List, Dict MODEL_PATH = \u0026#34;model.pth\u0026#34; device = torch.device(\u0026#34;cpu\u0026#34;) model = None def load_model(): global model if model is None: print(f\u0026#34;Loading model from {MODEL_PATH}...\u0026#34;) model = torch.load(MODEL_PATH, map_location=device) model.eval() print(\u0026#34;Model loaded successfully!\u0026#34;) return model def prepare_features(history: List[Dict]) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34; Convert history into a tensor for the model history: [{\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 107.0}, ...] \u0026#34;\u0026#34;\u0026#34; # TODO: Implement feature engineering based on your model lats = [p[\u0026#34;lat\u0026#34;] for p in history] lngs = [p[\u0026#34;lng\u0026#34;] for p in history] features = np.array([lats + lngs]) # Shape: (1, 18) return torch.tensor(features, dtype=torch.float32) def format_predictions(predictions: torch.Tensor, storm_name: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Format output to match what the frontend expects \u0026#34;\u0026#34;\u0026#34; pred_array = predictions.detach().cpu().numpy()[0] forecast = [] base_timestamp = int(time.time() * 1000) for i in range(0, len(pred_array), 2): if i + 1 \u0026lt; len(pred_array): forecast.append({ \u0026#34;lat\u0026#34;: float(pred_array[i]), \u0026#34;lng\u0026#34;: float(pred_array[i + 1]), \u0026#34;timestamp\u0026#34;: base_timestamp + (i // 2) * 3600000, # +1 hour each \u0026#34;windSpeed\u0026#34;: 120.0, # TODO: Predict from model \u0026#34;pressure\u0026#34;: 980.0, # TODO: Predict from model \u0026#34;category\u0026#34;: \u0026#34;Category 3\u0026#34;, # TODO: Classify from windSpeed \u0026#34;confidence\u0026#34;: 0.85 }) return { \u0026#34;storm_name\u0026#34;: storm_name, \u0026#34;forecast\u0026#34;: forecast } def handler(event, context): \u0026#34;\u0026#34;\u0026#34; Lambda handler function \u0026#34;\u0026#34;\u0026#34; try: # Parse input body = json.loads(event.get(\u0026#39;body\u0026#39;, \u0026#39;{}\u0026#39;)) history = body.get(\u0026#39;history\u0026#39;, []) storm_name = body.get(\u0026#39;storm_name\u0026#39;, \u0026#39;Unknown Storm\u0026#39;) # Validate input if len(history) \u0026lt; 9: return { \u0026#39;statusCode\u0026#39;: 400, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: f\u0026#39;Need at least 9 positions, got {len(history)}\u0026#39; }) } # Load model model = load_model() # Prepare features X = prepare_features(history) # Predict with torch.no_grad(): predictions = model(X) # Format output result = format_predictions(predictions, storm_name) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } except Exception as e: print(f\u0026#34;Error: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: str(e) }) } 1.2. Update Dockerfile FROM public.ecr.aws/lambda/python:3.11 # Copy requirements and install COPY requirements.txt ${LAMBDA_TASK_ROOT} RUN pip install --no-cache-dir -r requirements.txt # Copy model (rename to model.pth) COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/model.pth # Copy code COPY app.py ${LAMBDA_TASK_ROOT} # Set handler CMD [\u0026#34;app.handler\u0026#34;] 1.3. Verify requirements.txt torch==2.1.0 numpy==1.24.3 Step 2: Build the Docker Image cd storm_prediction # Build image docker build -t storm-prediction-model . # Test locally (optional) docker run -p 9000:8080 storm-prediction-model # Test with curl curl -X POST \u0026#34;http://localhost:9000/2015-03-31/functions/function/invocations\u0026#34; \\ -d \u0026#39;{ \u0026#34;body\u0026#34;: \u0026#34;{\\\u0026#34;history\\\u0026#34;: [{\\\u0026#34;lat\\\u0026#34;: 15.0, \\\u0026#34;lng\\\u0026#34;: 107.0}, {\\\u0026#34;lat\\\u0026#34;: 15.1, \\\u0026#34;lng\\\u0026#34;: 107.1}, {\\\u0026#34;lat\\\u0026#34;: 15.2, \\\u0026#34;lng\\\u0026#34;: 107.2}, {\\\u0026#34;lat\\\u0026#34;: 15.3, \\\u0026#34;lng\\\u0026#34;: 107.3}, {\\\u0026#34;lat\\\u0026#34;: 15.4, \\\u0026#34;lng\\\u0026#34;: 107.4}, {\\\u0026#34;lat\\\u0026#34;: 15.5, \\\u0026#34;lng\\\u0026#34;: 107.5}, {\\\u0026#34;lat\\\u0026#34;: 15.6, \\\u0026#34;lng\\\u0026#34;: 107.6}, {\\\u0026#34;lat\\\u0026#34;: 15.7, \\\u0026#34;lng\\\u0026#34;: 107.7}, {\\\u0026#34;lat\\\u0026#34;: 15.8, \\\u0026#34;lng\\\u0026#34;: 107.8}], \\\u0026#34;storm_name\\\u0026#34;: \\\u0026#34;Test Storm\\\u0026#34;}\u0026#34; }\u0026#39; Step 3: Upload to AWS ECR # 1. Create ECR repository aws ecr create-repository \\ --repository-name storm-prediction-model \\ --region ap-southeast-1 # 2. Login Docker to ECR aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com # 3. Tag image docker tag storm-prediction-model:latest \\ \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest # 4. Push image docker push \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest Step 4: Create the Lambda Function 4.1. Create Lambda from Console Open AWS Lambda Console Click “Create function” Choose “Container image” Function name: storm-prediction Container image URI: select the image you pushed to ECR Architecture: x86_64 Click “Create function” 4.2. Configure Lambda # Or use AWS CLI aws lambda create-function \\ --function-name storm-prediction \\ --package-type Image \\ --code ImageUri=\u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest \\ --role arn:aws:iam::\u0026lt;account-id\u0026gt;:role/lambda-execution-role \\ --timeout 60 \\ --memory-size 3008 \\ --region ap-southeast-1 Important configuration:\nMemory: 3008 MB (PyTorch models need RAM) Timeout: 60 seconds (inference can take 10–30s) Ephemeral storage: 512 MB (default; increase if needed) 4.3. Create IAM Role Lambda needs a role with permissions:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:GetDownloadUrlForLayer\u0026#34;, \u0026#34;ecr:BatchGetImage\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Step 5: Create API Gateway # 1. Create REST API aws apigateway create-rest-api \\ --name storm-prediction-api \\ --region ap-southeast-1 # 2. Get API ID and Root Resource ID API_ID=\u0026lt;your-api-id\u0026gt; ROOT_ID=\u0026lt;your-root-resource-id\u0026gt; # 3. Create resource /predict aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part predict # 4. Create POST method RESOURCE_ID=\u0026lt;predict-resource-id\u0026gt; aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $RESOURCE_ID \\ --http-method POST \\ --authorization-type NONE # 5. Integrate with Lambda aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $RESOURCE_ID \\ --http-method POST \\ --type AWS_PROXY \\ --integration-http-method POST \\ --uri arn:aws:apigateway:ap-southeast-1:lambda:path/2015-03-31/functions/arn:aws:lambda:ap-southeast-1:\u0026lt;account-id\u0026gt;:function:storm-prediction/invocations # 6. Deploy API aws apigateway create-deployment \\ --rest-api-id $API_ID \\ --stage-name prod API URL: https://\u0026lt;api-id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/prod/predict\nStep 6: Update the Frontend 6.1. Update .env.production VITE_PREDICTION_API_URL=https://\u0026lt;api-id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/prod 6.2. Build \u0026amp; deploy frontend cd frontend npm run build # Deploy dist/ to S3/CloudFront Optimizations 1. Reduce Cold Start Provisioned Concurrency:\naws lambda put-provisioned-concurrency-config \\ --function-name storm-prediction \\ --provisioned-concurrent-executions 1 \\ --qualifier $LATEST 2. Reduce Image Size Use PyTorch CPU-only:\n# requirements.txt torch==2.1.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu numpy==1.24.3 Multi-stage build:\n# Stage 1: Build FROM python:3.11-slim as builder COPY requirements.txt . RUN pip install --target /packages -r requirements.txt # Stage 2: Runtime FROM public.ecr.aws/lambda/python:3.11 COPY --from=builder /packages ${LAMBDA_RUNTIME_DIR} COPY model.pth ${LAMBDA_TASK_ROOT}/ COPY app.py ${LAMBDA_TASK_ROOT}/ CMD [\u0026#34;app.handler\u0026#34;] 3. Cache the Model in /tmp import os MODEL_PATH = \u0026#34;/tmp/model.pth\u0026#34; if os.path.exists(\u0026#34;/tmp/model.pth\u0026#34;) else \u0026#34;model.pth\u0026#34; def load_model(): global model if model is None: # Copy to /tmp for faster access if not os.path.exists(\u0026#34;/tmp/model.pth\u0026#34;): import shutil shutil.copy(\u0026#34;model.pth\u0026#34;, \u0026#34;/tmp/model.pth\u0026#34;) model = torch.load(\u0026#34;/tmp/model.pth\u0026#34;, map_location=device) model.eval() return model Monitoring CloudWatch Logs `bash\nView logs aws logs tail /aws/lambda/storm-prediction \u0026ndash;follow `\nCloudWatch Metrics Invocations: number of calls Duration: runtime Errors: error count Throttles: throttled invocations Alerts # Create an alarm for errors aws cloudwatch put-metric-alarm \\ --alarm-name storm-prediction-errors \\ --alarm-description \u0026#34;Alert when Lambda has errors\u0026#34; \\ --metric-name Errors \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 300 \\ --threshold 5 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=storm-prediction Troubleshooting Error: \u0026ldquo;Task timed out after 3.00 seconds\u0026rdquo;\nFix: Increase timeout to 60s\nError: \u0026ldquo;Runtime exited with error: signal: killed\u0026rdquo;\nFix: Increase memory to 3008 MB\nError: \u0026ldquo;No module named \u0026rsquo;torch\u0026rsquo;\u0026rdquo;\nFix: Check requirements.txt and rebuild the image\nError: Model cannot be loaded\nFix: Verify the model filename in Dockerfile and app.py match\nEstimated Cost Lambda: Free tier: 1M requests/month, 400,000 GB-seconds After that: $0.20 per 1M requests + $0.0000166667 per GB-second Example: 10,000 requests/month, each request 10s, 3GB RAM Compute: 10,000 × 10s × 3GB × $0.0000166667 = ~$5/month Requests: 10,000 × $0.20/1M = ~$0.002/month Total: ~$5/month API Gateway: $3.50 per million requests 10,000 requests = ~$0.035/month ECR: $0.10 per GB/month storage Image ~2GB = ~$0.20/month Estimated total: ~$5.25/month for 10,000 predictions\nFinal Checklist Fix the model filename mismatch in app.py or Dockerfile Test the Docker image locally Push the image to ECR Create Lambda with 3008MB memory, 60s timeout Create API Gateway and integrate with Lambda Test the API with Postman/curl Update VITE_PREDICTION_API_URL in the frontend Build and deploy the frontend Test the prediction form on the web UI Set up CloudWatch alerts Monitor logs and performance "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "ONLINE PLATFORM FOR TRACKING AND FORECASTING HURRICANE TRAJECTORY Overview Hurricanes are powerful natural disasters that cause severe damage to infrastructure and pose significant risks to human life. Early detection and timely warnings are essential so that people in affected areas have enough time to prepare and evacuate safely.\nTo address this need, our project aims to build an online platform that allows users to freely access information about the most recent storms in the Western Pacific, using data sourced from the trusted NOAA (National Oceanic and Atmospheric Administration). In addition, students, meteorologists, or anyone interested in hurricane dynamics can interact with our system by providing their own input trajectories and receiving predictions generated by our machine learning model.\nThis workshop presents the complete process of building such a model for hurricane forecasting, including several novel time-series techniques—Stepwise Temporal Fading and Plausible Geodesic-Aware Augmentation—as well as a step-by-step explanation of how we built and deployed the platform from scratch.\nWith the support of AWS services such as Amazon S3, AWS Lambda, API Gateway, and CloudFront, we construct a fully serverless architecture. This offers simplicity, scalability, and long-term cost efficiency while ensuring reliable and responsive performance.\nPlatform Architecture\rThe final platform delivers two core functionalities:\nStorm Viewing Users can explore up-to-date information on recent Western Pacific storms, including their historical path, wind speed, temperature, and other relevant parameters.\nHurricane Trajectory Prediction Users can input their own partial storm trajectory and receive a predicted future path generated by our trained model.\nContent Workshop overview Data Preparation ML Model Training Front\u0026amp;Back-End Architect API "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/5-workshop/5.5-platform-api/",
	"title": "Platform API",
	"tags": [],
	"description": "",
	"content": "BACK-END API DETAIL DESCRIPTION Table of Contents Introduction System Architecture Core Features Technology Stack Project Structure API Endpoints 1. Introduction Weather Backend API is a RESTful service that provides weather information by integrating with OpenWeatherMap API. The backend serves as a middleware layer between frontend applications and external weather data sources.\nFigure 1\r2. System Architecture High-Level Architecture ┌─────────────────────────────────────────────────────────────┐\r│ Frontend Applications │\r│ (React, Mobile, Web Clients) │\r└─────────────────────────────────────────────────────────────┘\r│\r│ HTTPS / REST API\r▼\r┌─────────────────────────────────────────────────────────────┐\r│ Weather Backend API │\r│ (.NET 9.0 - ASP.NET Core) │\r├─────────────────────────────────────────────────────────────┤\r│ ┌────────────────┐ ┌────────────────┐ ┌────────────────┐\r│ │ Controllers │ │ Services │ │ Program.cs │\r│ │ │ │ │ │ - App Startup │\r│ │ - WeatherCtrl │ │ - WeatherSvc │ │ - Logging │\r│ │ - ForecastCtrl │ │ - Cache Layer │ │ - DI Setup │\r│ └────────────────┘ └────────────────┘ └────────────────┘\r└─────────────────────────────────────────────────────────────┘\r│\r│ HTTPS / REST API (External)\r▼\r┌─────────────────────────────────────────────────────────────┐\r│ External Weather Services │\r├─────────────────────────────────────────────────────────────┤\r│ • OpenWeatherMap API │\r│ • Redis Caching Layer │\r│ • Rate Limiting \u0026amp; Monitoring │\r└─────────────────────────────────────────────────────────────┘ 3. Core Features Description: Retrieve current weather conditions for any city worldwide.\nFeatures:\nSearch by city name (e.g., \u0026ldquo;Hanoi\u0026rdquo;, \u0026ldquo;Ho Chi Minh City\u0026rdquo;) Optional country code for precise location Multiple unit systems support (metric, imperial, standard) Multi-language weather descriptions Cached responses for performance API Parameters:\ncityName (required): Name of the city countryCode (optional): ISO 3166 country code units (optional): metric, imperial, or standard language (optional): en, vi, fr, etc. Response Example:\nResponse body Download { \u0026#34;localDate\u0026#34;: \u0026#34;2025-12-06 19:57:02\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Hà Nội\u0026#34;, \u0026#34;coord\u0026#34;: { \u0026#34;lon\u0026#34;: 105.8412, \u0026#34;lat\u0026#34;: 21.0245 }, \u0026#34;weather\u0026#34;: [ { \u0026#34;id\u0026#34;: 804, \u0026#34;main\u0026#34;: \u0026#34;Clouds\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;mây đen u ám\u0026#34;, \u0026#34;icon\u0026#34;: \u0026#34;04n\u0026#34; } ], \u0026#34;main\u0026#34;: { \u0026#34;temp\u0026#34;: 22, \u0026#34;feels_like\u0026#34;: 22.11, \u0026#34;temp_min\u0026#34;: 22, \u0026#34;temp_max\u0026#34;: 22, \u0026#34;pressure\u0026#34;: 1018, \u0026#34;humidity\u0026#34;: 71, \u0026#34;sea_level\u0026#34;: 1018, \u0026#34;grnd_level\u0026#34;: 1017 }, \u0026#34;wind\u0026#34;: { \u0026#34;speed\u0026#34;: 4.14, \u0026#34;deg\u0026#34;: 136, \u0026#34;gust\u0026#34;: 6.84 }, \u0026#34;sys\u0026#34;: { \u0026#34;type\u0026#34;: 1, \u0026#34;id\u0026#34;: 9308, \u0026#34;country\u0026#34;: \u0026#34;VN\u0026#34;, \u0026#34;sunrise\u0026#34;: 1764976827, \u0026#34;sunset\u0026#34;: 1765016103 } } Figure 2\r4. Technology Stack Backend Framework .NET 9.0 – Latest .NET runtime ASP.NET Core – Web API framework C# 12 – Primary programming language API Integration HttpClientFactory – Managed HTTP client usage Polly – Retry policies \u0026amp; transient fault handling Newtonsoft.Json / System.Text.Json – JSON serialization Caching \u0026amp; Performance MemoryCache – In-memory caching Redis (optional) – Distributed caching ResponseCompression – Gzip / Brotli compression Development Tools Visual Studio 2022 / VS Code – IDE / Code editor Swagger / OpenAPI – API documentation Git – Version control Docker – Containerization 5. Project Structure WeatherBackend/ │ ├── WeatherBackend.csproj # Project file ├── Program.cs # Application entry point ├── WeatherBackend.http # HTTP request testing file │ ├── appsettings.json # Configuration settings │ ├── Controllers/ # API Controllers │ └── WeatherController.cs # Main weather endpoints │ ├── Services/ # Business logic services │ └── WeatherService/ # Service contracts \u0026amp; implementation 6. API Endpoints Base URL https://localhost:7042/swagger/index.html 6.1 GET /api/weather/current Description: Retrieve the current weather by city name.\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather?city=hanoi\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather?city=hanoi 6.2 GET /api/weather/forecast Description: Get the 5-day weather forecast for a selected city.\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/forecast?city=hochiminh\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather/forecast?city=hochiminh 6.3 GET /api/weather/coordinates Description: Retrieve weather data using latitude and longitude.\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/by-coord?lat=21.0245\u0026amp;lon=105.8412\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather/by-coord?lat=21.0245\u0026amp;lon=105.8412 6.4 GET /api/weather/location Description: Retrieve weather data for the user\u0026rsquo;s current location (requires coordinates from client device).\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/global\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather/global Figure 3\rLast Updated: 2025-12-09\nVersion: 1.0.0\nMaintained by: SKYNET\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services (AWS) from 08/09/2025 to 29/11/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in a storm trajectory prediction project using deep learning (Transformer-based time-series modeling) and serverless web deployment, through which I improved my skills in data preprocessing, feature engineering, time-series modeling, training \u0026amp; fine-tuning, model evaluation, API integration, AWS serverless basics, technical documentation, and teamwork/communication.\nOver 12 weeks, I gradually progressed from understanding AWS fundamentals and studying related research, to building storm datasets and baseline models, selecting model directions (LSTM/RNN/Transformer), developing a prototype, preparing for training, running full training and fine-tuning, evaluating the model, and finally integrating the trained model into an API to serve predictions on a web interface.\nI also strengthened my ability to write technical reports, build presentation slides, and create a runbook for project handover.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ☐ ✅ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Improve time management and daily planning to maintain steady productivity and avoid last-minute rush when multiple tasks overlap. Be more confident in initiating discussions and asking for clarification early, especially when requirements are not fully clear. Strengthen professional communication habits such as concise updates, clearer status reporting, and more proactive follow-ups with teammates. "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe workplace feels professional yet welcoming. The team maintains a calm and organized space, which makes it easier to concentrate and stay productive. I also appreciate the flexible and open atmosphere where interns can interact comfortably with full-time members. If possible, having occasional informal activities (e.g., short team coffee chats or internal sharing sessions) could help strengthen connections across the team.\n2. Support from Mentor / Team Admin\nI received strong support from both the mentor and the administrative team throughout the internship. The mentor gave clear directions, provided timely feedback, and helped me stay on track while still encouraging independent problem-solving. The admin team was responsive and well-prepared, ensuring that logistics, documents, and internal processes were handled smoothly so I could focus on work.\n3. Relevance of Work to Academic Major\nThe work assignments were closely related to my academic background and allowed me to apply foundational knowledge in a practical setting. At the same time, the tasks also exposed me to new tools and real workflows that are not fully covered in university courses. This combination helped me connect theory with real implementation and better understand industry expectations.\n4. Learning \u0026amp; Skill Development Opportunities\nThe internship offered many opportunities to grow beyond technical skills. I learned how to structure tasks more systematically, document progress, and communicate updates in a professional manner. I also gained experience in collaborating with different roles in a project team, which improved my ability to adapt and work effectively in a corporate environment.\n5. Company Culture \u0026amp; Team Spirit\nThe team culture is collaborative and respectful. People work with a strong sense of responsibility while still being approachable and supportive. When challenges arise, team members are willing to help and share knowledge, which creates a good learning environment. This made it easier for me to feel included and motivated to contribute.\n6. Internship Policies / Benefits\nThe internship policies are clear and supportive. The allowance and flexible scheduling helped me balance learning and productivity effectively. I also found it valuable that interns have access to internal learning materials and training opportunities, which adds meaningful value beyond day-to-day work.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/en/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]