[
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/",
	"title": "Báo cáo thực tập",
	"tags": [],
	"description": "",
	"content": "Báo cáo thực tập Thông tin sinh viên: Họ và tên: Nguyễn Minh Đức\nSố điện thoại: 0768803969\nEmail: Anguyenvan@gmail.com\nTrường: Đại học Sư phạm Kỹ thuật TP.HCM\nNgành: Công nghệ thông tin\nLớp: AWS082025\nCông ty thực tập: Công ty TNHH Amazon Web Services Vietnam\nVị trí thực tập: FCJ Cloud Intern\nThời gian thực tập: Từ ngày 12/08/2025 đến ngày 12/11/2025\nNội dung báo cáo Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Trexlix đạt 35% mức độ tiết kiệm chi phí và tăng cường bảo mật nhờ vào dịch vụ OpenSearch của Amazon Bởi Leeneksh Dubey, Harsh Bansal và Prashant Agrawal, ngày 19 tháng 9 năm 2025 | Amazon OpenSearch Service, Customer Solutions, Intermediate (200).\nĐây là bài viết khách mời của Leeneksh Dubey, Kỹ sư Cloud tại Trellix, hợp tác cùng AWS.\nTrellix,nhà lãnh đạo hàng đầu về các giải pháp an toàn thông tin,nổi lên vào năm 2022 từ sự hợp nhất của McAfee Enterprise và FireEye, phục vụ cho hơn 40,000 doanh nghiệp khách hàng trên khắp thế giới,Trelix cung cấp nền tảng bảo mật AI toàn diện, cởi mở nhất và quen thuộc cho các doanh nghiệp.Các biện pháp của họ giúp các tổ chức xây dựng khả năng phục hồi để chống lại các mối đe dọa nâng cao thông qua tự động phát hiện,điều tra và phản ứng\nNgày nay các đội ngũ an toàn thông tin đang đối mặt với 1 bối cảnh phức tạp từ các mối đe dọa an toàn thông thông tin, trong khi lượng lớn các ghi chép nhật kí về bảo mật và ứng dụng đang tăng trưởng rõ rệt. Do sự giới hạn tài nguyên và nhân lực,các đội ngũ đang chiến đấu để nghiên cứu các dữ kiện bảo mật và nguy cơ các mối đe đọa mới sẽ xuất hiện.Trelix định hình lại các thử thách này bằng cách thống nhất lại các công cụ bảo mật trên endpoint, mạng, đám mây và email thành 1 nền tảng duy nhất và hỗ trợ AI. Nhờ có tự động phát hiện mối đe dọa,điều tra và phản hồi đã giúp cho các đội ngũ an ninh mạng tìm ra và xử lý các tiềm ẩn nhanh hơn cũng như là làm giảm sự phức tạp trong hoạt động.\nĐể giải quyết sự tăng trưởng của nhật kí dữ liệu theo cấp số nhân trên hạ tầng đa thuê bao (multi-tenant), đa vùng (multi-Region), Trellix đã sử dụng Amazon OpenSearch Service, Amazon OpenSearch Ingestion, và Amazon Simple Storage Service (Amazon S3) để hiện đại hóa hệ thống log của mình. Trước đây, Trellix gặp nhiều thách thức với các cụm Elasticsearch tự quản lý trên Amazon Elastic Compute Cloud (Amazon EC2). Việc chuyển đổi sang OpenSearch Service được quản lý (managed) đã giúp họ tối ưu hóa đáng kể hoạt động vận hành. Chiến lược này cho phép Trellix xử lý hàng terabyte dữ liệu bảo mật mỗi ngày trên nhiều vùng AWS, đồng thời đạt được mức giảm 35% chi phí lưu trữ tính đến Quý 3 năm 2024. Việc chuyển sang dịch vụ được quản lý còn giúp tiết kiệm tới 10 giờ bảo trì hạ tầng mỗi tuần, tạo điều kiện cho các nhà phát triển tập trung nhiều hơn vào các nhiệm vụ mang lại giá trị gia tăng.\nTrong bài viết này, chúng tôi sẽ chia sẻ cách Trellix, thông qua việc áp dụng các giải pháp AWS, để nâng cao hiệu năng, tính sẵn sàng và khả năng mở rộng của hệ thống, đồng thời giảm thiểu chi phí vận hành.\nTổng quan giải pháp Giải pháp quản lý nhật kí dữ liệu đầy sáng tạo của Trellix, được xây dựng trên các dịch vụ AWS, đã giải quyết thách thức trong việc xử lý khối lượng lớn dữ liệu bảo mật trên nhiều vùng (Regions). Kiến trúc cấp doanh nghiệp này cho thấy cách các tổ chức có thể quản lý hiệu quả log bảo mật ở quy mô lớn đồng thời tối ưu hóa chi phí. Giải pháp tập trung giải quyết ba thách thức kinh doanh quan trọng: quản lý hiệu quả việc lưu trữ log dài hạn, phân phối có khả năng mở rộng cho các chức năng phân tích và cảnh báo, cũng như tối ưu chi phí lưu trữ trên hạ tầng đa vùng. Kiến trúc này được minh họa trong sơ đồ dưới đây, cho thấy cách Trellix quản lý log bảo mật ở quy mô lớn đồng thời tối ưu chi phí.\nFigure 1\rGiải pháp quản lý log bảo mật của Trellix trên AWS triển khai một quy trình dữ liệu toàn diện, xử lý liền mạch các bước từ thu thập log, xử lý, lưu trữ đến phân tích. Trong các phần tiếp theo, chúng tôi sẽ đi sâu hơn vào sáu bước của quy trình làm việc này.\nBước 1: Tải dữ liệu lên Amazon S3\nGiải pháp bắt đầu với quá trình thu thập dữ liệu, tận dụng hạ tầng phân tán toàn cầu và khả năng mở rộng cao của Amazon S3. Các log thô về bảo mật và ứng dụng được thu thập từ nhiều triển khai ở các vùng (Region) khác nhau, giúp Trellix duy trì cả tính chủ quyền dữ liệu lẫn khả năng truy cập độ trễ thấp tại nhiều khu vực pháp lý. Các log này sau đó được xử lý bởi công cụ nội bộ của Trellix, nơi chúng được làm giàu bằng logic bảo mật độc quyền. Bộ dữ liệu đã được làm giàu này tiếp tục được lưu trữ trở lại Amazon S3, tạo nên một nền tảng an toàn, có khả năng mở rộng cho các phân tích bảo mật và xử lý tiếp theo.\nBước 2: Kích hoạt thông báo Amazon SNS bởi sự kiện S3\nSau khi dữ liệu được làm phong phú và lưu trữ thành công trong Amazon S3, hệ thống sẽ khởi động một chuỗi tự động hóa dựa trên sự kiện. Amazon S3 được cấu hình để phát thông báo sự kiện tới một chủ đề (topic) của Amazon Simple Notification Service (Amazon SNS) bất cứ khi nào có dữ liệu mới được tải lên. Amazon SNS đóng vai trò như một trung tâm thông báo, phát tán hiệu quả các sự kiện này tới các dịch vụ hoặc điểm cuối đã đăng ký. Cách tiếp cận này giúp kiến trúc duy trì tính linh hoạt và tách biệt, vì nó cho phép nhiều bên nhận được cảnh báo theo thời gian thực ngay khi dữ liệu mới xuất hiện trong hệ thống.\nBước 3: Xếp hàng tin nhắn trong Amazon SQS\nỞ bước tiếp theo, các thông báo từ SNS được định tuyến đến Amazon Simple Queue Service (Amazon SQS), nơi đóng vai trò là lớp hàng đợi bền bỉ và có khả năng mở rộng giữa các nhà sản xuất và người tiêu thụ dữ liệu. Hàng đợi này hoạt động như một bộ đệm, hỗ trợ việc truyền tải siêu dữ liệu sự kiện một cách tin cậy và không đồng bộ đến các thành phần xử lý phía sau. Việc sử dụng Amazon SQS mang lại khả năng lưu giữ thông điệp và khả năng chống lỗi, đặc biệt hữu ích trong các kịch bản tải cao hoặc sự cố, cho phép OpenSearch Ingestion xử lý dữ liệu đầu vào một cách có kiểm soát và bền vững.\nBước 4: Xử lý dữ liệu tự động với OpenSearch Ingestion OpenSearch Ingestion liên tục truy vấn hàng đợi SQS để tìm các thông điệp mới báo hiệu dữ liệu đã sẵn sàng trong Amazon S3. Khi nhận được các thông điệp này, nó sử dụng khả năng tích hợp sẵn để truy xuất trực tiếp dữ liệu từ Amazon S3. Sau khi dữ liệu được lấy về, pipeline ingestion thực hiện các bước chuyển đổi cần thiết trước khi chuyển tiếp đến miền OpenSearch Service. Để đạt hiệu quả tối ưu về chi phí và hiệu năng, Trellix đã chọn loại phiên bản OR1 instances cho triển khai OpenSearch của mình. Các phiên bản này có tỷ lệ bộ nhớ trên vCPU cao và được tối ưu hóa đặc biệt cho các khối lượng công việc lập chỉ mục và tìm kiếm chuyên sâu, khiến chúng trở nên lý tưởng để xử lý các hoạt động phân tích log quy mô lớn.\nBước 5: Thiết lập vòng đời log với Index State Management Để tối ưu hóa việc sử dụng lưu trữ và quản lý vòng đời dữ liệu, Trellix đã triển khai các chính sách Index State Management (ISM) trong OpenSearch Service. Các chính sách này tự động hóa vòng đời của dữ liệu log đã được nhập vào bằng cách đưa chúng qua các giai đoạn được định nghĩa dựa trên độ tuổi và mô hình truy cập. Ban đầu, log nằm trong hot tier trong tối đa 24 giờ, cho phép truy cập ngay lập tức để phục vụ phân tích bảo mật theo thời gian thực. Khi log quá thời hạn này, chúng sẽ tự động được chuyển sang UltraWarm storage, một tùy chọn lưu trữ tiết kiệm chi phí hơn nhưng vẫn giữ khả năng truy vấn dữ liệu. Cuối cùng, sau khi thời gian lưu trữ được định sẵn kết thúc, chính sách ISM sẽ xóa dữ liệu khỏi hệ thống. Cách tiếp cận quản lý vòng đời hoàn toàn tự động này cân bằng giữa hiệu năng, tuân thủ quy định và tối ưu chi phí.\nBước 6: Giám sát và trực quan hóa toàn diện Bằng cách sử dụng khả năng giám sát toàn diện của Amazon CloudWatch, kết hợp với các tự động hóa nội bộ của Trellix thông qua OpenSearch public APIs cho giám sát tùy chỉnh, giải pháp này mang lại khả năng quan sát từ đầu đến cuối thông qua các công cụ trực quan hóa tích hợp. OpenSearch Dashboards cung cấp cho các đội ngũ bảo mật khả năng phân tích log và tìm kiếm mạnh mẽ, cho phép họ đi sâu vào các sự kiện bảo mật và xác định các mối đe dọa tiềm ẩn. Ngoài ra, giải pháp còn sử dụng Amazon Managed Grafana để tạo các bảng điều khiển tùy chỉnh nhằm theo dõi cả tình trạng của pipeline dữ liệu và hiệu năng của cụm OpenSearch.\nCách tiếp cận trực quan kép này mang lại nhiều lợi ích: giám sát và phân tích sự kiện bảo mật theo thời gian thực, cung cấp các chỉ số hiệu năng toàn diện trên toàn bộ hạ tầng, cảnh báo tự động để phản ứng nhanh với mối đe dọa, bảng điều khiển tùy biến cho các nhu cầu vận hành bảo mật khác nhau, và tầm nhìn hợp nhất trên nhiều triển khai đa vùng. Sức mạnh kết hợp của các công cụ này tạo ra một khung giám sát vững chắc, giúp Trellix duy trì tư thế bảo mật mạnh mẽ đồng thời tạo điều kiện cho hiệu năng tối ưu trên hạ tầng toàn cầu của họ.\nLợi ích chính Việc Trellix triển khai OpenSearch Service như một giải pháp logging đã mang lại ba lợi ích quan trọng, giúp chuyển đổi toàn diện hoạt động an ninh mạng của họ.\n1. Đơn giản hóa kiến trúc quản lý log\nTrellix đã hợp lý hóa hoạt động bảo mật của mình bằng cách triển khai một kiến trúc quản lý log thống nhất, loại bỏ sự phức tạp trong việc phải quản lý nhiều công cụ rời rạc. Bằng cách sử dụng OpenSearch Ingestion, một pipeline dữ liệu không máy chủ (serverless) được quản lý hoàn toàn, Trellix đã đơn giản hóa pipeline xử lý dữ liệu bảo mật theo thời gian thực. Việc tích hợp với Managed Grafana mang lại một lớp trực quan hóa thống nhất, cho phép các nhóm bảo mật tập trung vào việc phát hiện mối đe dọa thay vì quản lý hạ tầng.\n2. Khả năng mở rộng và tính linh hoạt\nViệc triển khai OpenSearch Service giúp Trellix đạt được khả năng mở rộng và tính linh hoạt chưa từng có trong hoạt động an ninh mạng của mình. Kiến trúc của Trellix sử dụng pipeline OpenSearch Ingestion để xử lý dễ dàng các đột biến khối lượng log trên nhiều vùng triển khai (Regional deployments). OpenSearch Ingestion cho phép mở rộng động với tối ưu hóa tài nguyên tự động, giúp quản lý dung lượng liền mạch khi dữ liệu tăng trưởng. Khả năng này giúp Trellix duy trì hiệu năng ổn định ngay cả trong những giai đoạn gia tăng log sự kiện bảo mật. Giải pháp cũng triển khai chiến lược Multi-AZ deployment mạnh mẽ để duy trì độ linh hoạt tối đa và đảm bảo dịch vụ liên tục. Trong các bài kiểm tra tự động phục hồi (self-healing testing), kiến trúc đã chứng minh khả năng khôi phục ấn tượng dưới 9 phút khi một nút bị khởi động lại, cho thấy khả năng duy trì liên tục hoạt động kinh doanh ngay cả khi có sự cố nút. Các tính năng failover tự động giúp giảm thiểu gián đoạn cho hoạt động bảo mật, đảm bảo Trellix có thể duy trì giám sát liên tục đối với tư thế an ninh của khách hàng. Ngoài ra, giải pháp còn sử dụng sao lưu tự động Amazon S3 kết hợp với snapshot hàng giờ để đảm bảo khả năng khôi phục dữ liệu tại từng điểm thời gian. Mỗi Region duy trì thêm các bản sao dữ liệu khách hàng, tạo nên chiến lược bảo vệ dữ liệu nhiều lớp, đảm bảo tính toàn vẹn và khả năng sẵn sàng của thông tin bảo mật quan trọng.\n3. Mở rộng dễ dàng với chi phí tối ưu\nSự tăng trưởng theo cấp số nhân trong xử lý dữ liệu bảo mật của Trellix đòi hỏi một giải pháp có khả năng mở rộng linh hoạt đồng thời duy trì hiệu quả về chi phí. Việc triển khai chiến lược Amazon S3 và OpenSearch Service với UltraWarm storage đã cung cấp nền tảng cho kiến trúc có khả năng mở rộng này. UltraWarm, một tầng lưu trữ trung gian được quản lý hoàn toàn cho OpenSearch Service, đã cách mạng hóa cách Trellix quản lý khối lượng lớn dữ liệu bảo mật trên nhiều vùng. Giải pháp tận dụng kiến trúc sáng tạo của UltraWarm, sử dụng Amazon S3 để lưu trữ bền vững trong khi vẫn duy trì hiệu năng truy vấn nhanh cho phân tích bảo mật. Lợi ích chính của kiến trúc dựa trên Amazon S3 của UltraWarm là loại bỏ nhu cầu bản sao chỉ mục (index replicas), giúp giảm đáng kể kích thước cụm và chi phí liên quan trong khi vẫn đảm bảo độ bền dữ liệu. Khung phân loại log thông minh đóng vai trò xương sống trong chiến lược quản lý dữ liệu của Trellix, phân loại dữ liệu đầu vào dựa trên mức độ quan trọng về bảo mật. Cách tiếp cận hệ thống này cho phép: định tuyến hiệu quả các nguồn log P2 và P3, tối ưu hóa luồng xử lý cho các ưu tiên bảo mật khác nhau, giảm tải cho hạ tầng SIEM chính và xử lý tùy chỉnh dựa trên yêu cầu khách hàng. Việc triển khai này đặc biệt có giá trị trong phân tích log bảo mật, nơi phân tích dữ liệu lịch sử đóng vai trò then chốt trong phát hiện mối đe dọa và yêu cầu tuân thủ. Kết quả là, Trellix đã đạt được những lợi ích đáng kể về vận hành và tài chính: kết hợp định tuyến dựa trên ưu tiên và quản lý lưu trữ phân tầng giúp giảm 35% chi phí lưu trữ và tính toán, vẫn duy trì hiệu năng cao trong hoạt động bảo mật, đảm bảo lưu trữ và phân tích hiệu quả khối lượng lớn dữ liệu lịch sử, hỗ trợ cam kết của Trellix trong việc giám sát an ninh toàn diện đồng thời tối ưu chi phí vận hành. Triển khai này cho thấy cách các dịch vụ AWS có thể giúp các tổ chức tối ưu chi phí mà không ảnh hưởng đến khả năng bảo mật hoặc hiệu quả vận hành.\nCác bước tiếp theo Việc triển khai thành công giải pháp này đã giúp Trellix sẵn sàng khám phá thêm các năng lực của AWS và những công nghệ mới nổi để nâng cao hoạt động an ninh mạng của mình:\nTích hợp các dịch vụ AWS ML/AI để phân tích petabyte dữ liệu log bảo mật\nTriển khai phát hiện bất thường dựa trên ML trong OpenSearch Service\nSử dụng các plugin phân tích bảo mật cho khả năng phát hiện mối đe dọa nâng cao\nTriển khai các cấu hình tùy chỉnh và quy tắc bảo mật dựng sẵn\nKết luận Trellix đã hiện đại hóa thành công hạ tầng quản lý log của mình thông qua hợp tác với AWS, triển khai một kiến trúc tinh vi để giải quyết thách thức xử lý hàng terabyte dữ liệu bảo mật mỗi ngày trên nhiều vùng (Regions). Bằng cách sử dụng OpenSearch Service với các nút UltraWarm và tích hợp Amazon S3, giải pháp đã mang lại những cải thiện đáng kể về hiệu năng, bao gồm tốc độ ingest log nhanh hơn và quản lý vận hành tinh gọn hơn. Cách tiếp cận lưu trữ phân tầng sáng tạo của kiến trúc, kết hợp với các chính sách lưu giữ dữ liệu tối ưu, đã giúp giảm 35% chi phí lưu trữ trong khi vẫn đáp ứng đầy đủ các yêu cầu tuân thủ.\nSự chuyển đổi này đã giúp Trellix sẵn sàng xử lý hiệu quả khối lượng dữ liệu ngày càng tăng và những thách thức bảo mật đang phát triển, đồng thời chứng minh cách việc sử dụng chiến lược các dịch vụ đám mây có thể đồng thời nâng cao hiệu năng, giảm chi phí và cải thiện hiệu quả vận hành.\nGiới thiệu về các tác giả Leeneksh Dubey Leeneksh là Kỹ sư Điện toán đám mây tại Trellix, chuyên về thiết kế các hệ thống hạ tầng đám mây có khả năng mở rộng và chịu lỗi cao trên AWS. Anh có nhiều kinh nghiệm trong các lĩnh vực dữ liệu, phân tích và trí tuệ nhân tạo (AI), bao gồm thiết kế giải pháp đầu-cuối, tự động hóa triển khai và tối ưu hóa chi phí. Mục tiêu của anh là xây dựng các môi trường bảo mật, hiệu suất cao nhằm hỗ trợ danh mục sản phẩm an ninh mạng của công ty. Harsh Bansal Harsh là Kiến trúc sư Giải pháp Phân tích và AI tại Amazon Web Services (AWS). Ông làm việc chặt chẽ với khách hàng, hỗ trợ họ di chuyển hệ thống lên nền tảng đám mây và tối ưu hóa cụm xử lý để nâng cao hiệu suất cũng như giảm chi phí. Trước khi gia nhập AWS, Harsh đã giúp khách hàng tận dụng OpenSearch và Elasticsearch cho nhiều nhu cầu tìm kiếm và phân tích log khác nhau. Prashant Agrawal Prashant là Kiến trúc sư Giải pháp Chuyên trách Tìm kiếm cao cấp (Sr. Search Specialist Solutions Architect) tại Amazon OpenSearch Service. Ông hợp tác với khách hàng để hỗ trợ họ di chuyển khối lượng công việc lên đám mây, đồng thời giúp tinh chỉnh các cụm hiện có nhằm đạt hiệu suất cao hơn và tiết kiệm chi phí. Trước khi gia nhập AWS, Prashant đã giúp nhiều khách hàng triển khai OpenSearch và Elasticsearch cho các trường hợp sử dụng tìm kiếm và phân tích nhật ký. Khi không làm việc, anh thường đi du lịch và khám phá những vùng đất mới — nói ngắn gọn, anh yêu thích phong cách sống “Ăn uống → Du lịch → Lặp lại”. "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Tính năng cấp quyền truy cập dữ liệu trong các hệ thống RAG Bởi Riggs Goodman III, ngày 18 tháng 9 năm 2025.|Trong Advanced (300), Amazon Bedrock, Best Practices, Generative AI, Security, Identity, \u0026amp; Compliance\nCác tổ chức hiện nay ngày càng sử dụng các mô hình ngôn ngữ lớn (LLM) để mang đến những hình thức tương tác mới với khách hàng thông qua chatbot được hỗ trợ bởi trí tuệ nhân tạo sinh (generative AI), trợ lý ảo và khả năng tìm kiếm thông minh. Để nâng cao các tương tác này, các tổ chức đang áp dụng phương pháp Retrieval-Augmented Generation (RAG) nhằm kết hợp dữ liệu độc quyền, kiến thức chuyên ngành và tài liệu nội bộ để cung cấp các phản hồi chính xác và có ngữ cảnh hơn. Với RAG, các LLM sử dụng một kho tri thức bên ngoài dựa trên vector store để kết hợp các dữ liệu kiến thức cụ thể trước khi tạo phản hồi.\nKhách hàng của chúng tôi cho biết họ lo ngại rằng việc thêm ngữ cảnh bổ sung vào prompt có thể dẫn đến rò rỉ thông tin nhạy cảm cho các principal (người dùng hoặc ứng dụng) có thể tồn tại trong một số công cụ này hoặc cho dữ liệu phi cấu trúc trong kho tri thức. Như đã đề cập trong các bài viết trước (Part 1, Part 2), LLM nên được xem là các thực thể không đáng tin cậy vì chúng không thực hiện kiểm soát quyền truy cập (authorization) như một phần của phản hồi. Một mô hình tư duy hợp lý cho các tổ chức là giả định rằng bất kỳ dữ liệu nào được gửi vào LLM trong prompt đều có thể được trả lại cho principal. Với các công cụ (API mà LLM có thể gọi để tương tác với các tài nguyên bên ngoài), bạn có thể truyền mã định danh (identity token) của principal vào công cụ để xác định những gì principal được phép truy cập và những hành động nào được cho phép. Các khả năng trên nhiều cơ sở dữ liệu vector — bao gồm các bộ lọc metadata và việc đồng bộ hóa thông tin danh tính giữa nguồn dữ liệu và kho tri thức — giúp cải thiện kết quả từ kho tri thức và cung cấp khả năng lọc cơ bản. Tuy nhiên, điều này không cung cấp khả năng cấp quyền mạnh mẽ dựa trên nguồn dữ liệu như nguồn xác thực duy nhất (source of truth) mà một số khách hàng đang tìm kiếm.\nTrong bài viết này, tôi sẽ trình bày một mô hình kiến trúc để cung cấp cơ chế cấp quyền mạnh mẽ cho các kết quả trả về từ kho tri thức, kèm theo ví dụ minh họa sử dụng Amazon S3 Access Grants với Amazon Bedrock Knowledge Bases. Tôi cũng sẽ đưa ra một số điểm cần xem xét khi triển khai các mô hình kiến trúc tương tự với các nguồn dữ liệu khác.\nTổng quan về việc sử dụng RAG Kiến trúc RAG có nhiều điểm tương đồng với các công cụ tìm kiếm, nhưng cũng có một số khác biệt quan trọng. Mặc dù cả hai đều sử dụng các nguồn dữ liệu được lập chỉ mục để tìm thông tin liên quan, nhưng cách tiếp cận trong việc truy cập dữ liệu của chúng lại khác nhau. Các công cụ tìm kiếm cung cấp liên kết đến các nguồn thông tin, yêu cầu người dùng phải truy cập trực tiếp vào nguồn dữ liệu gốc dựa trên quyền được cấp cho họ. Quy trình này được minh họa trong Hình 1.\nHình 1: Cấu trúc principal (trong ví dụ này là Người dùng) truy cập vào nguồn dữ liệu sau khi công cụ tìm kiếm trả về kết quả.\rKhông giống như các công cụ tìm kiếm, các hệ thống RAG trả về kết quả từ cơ sở dữ liệu vector trực tiếp thông qua LLM, bỏ qua bước kiểm tra quyền truy cập tại nguồn dữ liệu gốc. Mặc dù việc lọc theo metadata có thể giúp kiểm soát quyền truy cập, nhưng cách tiếp cận này tồn tại hai thách thức chính. Thứ nhất, các cơ sở dữ liệu vector chỉ đồng bộ định kỳ, nên các thay đổi về quyền truy cập trong nguồn dữ liệu không được cập nhật ngay lập tức. Thứ hai, với các quyền danh tính phức tạp — nơi một principal có thể thuộc hàng trăm nhóm — việc lọc kết quả một cách chính xác trở nên rất khó khăn. Điều này khiến lọc theo metadata không đủ cho những tổ chức yêu cầu cơ chế cấp quyền chặt chẽ hơn. Quy trình này được minh họa trong Hình 2.\nHình 2: Một ứng dụng truy cập dữ liệu trong cơ sở dữ liệu vector.\rĐể triển khai cơ chế cấp quyền mạnh mẽ cho việc truy cập dữ liệu trong kho tri thức, hãy xác minh quyền truy cập trực tiếp tại nguồn dữ liệu thay vì dựa vào các hệ thống trung gian. Khi sử dụng ví dụ về công cụ tìm kiếm, việc xác minh quyền truy cập diễn ra khi truy xuất kết quả thực tế từ nguồn dữ liệu, không phải trong quá trình tìm kiếm ban đầu. Đối với các cơ sở dữ liệu vector, ứng dụng AI sinh sẽ xác thực quyền truy cập bằng cách gửi yêu cầu cấp quyền đến nguồn dữ liệu trước khi truy xuất dữ liệu. Cách này giúp đảm bảo rằng nguồn dữ liệu, nơi duy trì các quy tắc kiểm soát truy cập chính thống, sẽ xác định xem principal có quyền truy cập vào các đối tượng cụ thể hay không. Việc kiểm tra cấp quyền theo thời gian thực này đồng nghĩa với việc các thay đổi về quyền truy cập được phản ánh ngay lập tức khi truy cập vào nguồn dữ liệu. Mẫu cấp quyền này tương tự như cách AWS Lake Formation quản lý quyền truy cập đối với dữ liệu có cấu trúc. Lake Formation đánh giá quyền khi một principal yêu cầu truy cập vào cơ sở dữ liệu hoặc bảng, cấp hoặc từ chối quyền truy cập dựa trên quyền đã được định nghĩa cho principal. Bạn có thể triển khai các cơ chế kiểm soát quyền tương tự cho kết quả của cơ sở dữ liệu vector trước khi cung cấp ngữ cảnh đó cho các mô hình ngôn ngữ lớn.\nHãy cùng xem xét một giải pháp ví dụ sử dụng S3 Access Grants kết hợp với Amazon Bedrock Knowledge Bases trong trường hợp thực tế.\nTổng quan giải pháp: S3 Access Grants với Bedrock Knowledge Bases Trong ví dụ sau, bạn có một tổ chức ACME muốn tạo một chatbot AI sinh cho nhân viên của họ. Có nhiều đội nhóm trong tổ chức (Marketing, Sales, HR và IT) làm việc trên các dự án trong toàn bộ tổ chức. Bạn có năm người dùng (các principal truy cập ứng dụng) với quyền nhóm như sau:\nAlice: Marketing Team Bob: Sales Team, Project A Team Carol: HR Team, Project B Team Dave: IT Support, Project C Team Eve: Marketing Team Mỗi principal sẽ được truy cập vào thư mục dự án (ví dụ /projects/projectA) hoặc thư mục phòng ban tương ứng (ví dụ departments/marketing/). Marketing cũng sẽ có quyền truy cập mọi thứ trong thư mục projects (/projects/*) trừ khi đó là các tệp được coi là rất mật (highly confidential). Để đánh dấu các tệp của Project B là rất mật, bạn sẽ thêm thẻ metadata cho các đối tượng trong tiền tố Project C với classification = ‘highly confidential’. Hình 3 cho thấy mối quan hệ giữa các principal và quyền truy cập vào các thư mục khác nhau trong nguồn dữ liệu. Ví dụ, chỉ có Carol được truy cập dữ liệu rất mật trong thư mục Project B.\nHình 3: Quyền nhóm của tổ chức\r*Hình 3: Quyền nhóm của tổ chức*\rĐể cấp quyền truy cập cho từng principal đến các đối tượng trong kho tri thức, bạn sẽ sử dụng Amazon S3 Access Grants. Bạn có thể tìm hiểu cách thiết lập S3 Access Grants trong Part 1 hoặc Part 2 của loạt bài viết này.\nTrong AWS IAM Identity Center, bạn sẽ thêm từng người dùng vào nhóm tương ứng của họ. Ví dụ, Bob sẽ được thêm vào cả nhóm Sales Team và nhóm Project A Team, giống như minh họa trong Hình 3.\nMỗi tiền tố (ví dụ projectA/, marketing/) sẽ chứa một tệp duy nhất cung cấp trạng thái của nhóm. Ngoài ra, với Project B, bạn cũng sẽ thêm tệp status.txt.metadata.json để gắn thẻ (tag) cho đối tượng là rất mật (highly confidential) vì đây là dự án của bộ phận HR.\nProject B status is as follows: Project B = Compensation Update STATUS = YELLOW Project completion = 50% Notes: we are tracking behind schedule. Need to pull more resources to get it completed by next month. Và tệp metadata.json sẽ có nội dung như sau:\n{ \u0026ldquo;metadataAttributes\u0026rdquo; : { \u0026ldquo;classification\u0026rdquo; : \u0026ldquo;highly confidential\u0026rdquo; } } Sau khi kho tri thức và S3 Access Grants được cấu hình xong, bạn có thể bắt đầu kiểm tra cơ chế cấp quyền cho các phần dữ liệu (chunk) trong kho tri thức. Quy trình hoạt động của ứng dụng được mô tả như sau (như minh họa trong Hình 4):\nNgười dùng sử dụng nhà cung cấp danh tính (IdP) để đăng nhập vào ứng dụng AI sinh (các bước 1a, 1b và 1c).\nỨng dụng AI sinh trao đổi token với IAM Identity Center và giả định vai trò (assume role) thay mặt cho người dùng (bước 2).\nỨng dụng AI sinh gọi S3 Access Grants để lấy danh sách các quyền truy cập (grants) mà người dùng được phép sử dụng (bước 3).\nNgười dùng gửi truy vấn đến ứng dụng AI sinh (bước 4).\nỨng dụng AI sinh gửi truy vấn đến kho tri thức (bước 5).\nỨng dụng AI sinh so sánh các phần dữ liệu (chunks) từ kho tri thức với phạm vi truy cập (scopes) mà người dùng được cấp quyền (bước 6).\nChỉ những phạm vi mà người dùng được phép truy cập mới được chuyển tiếp đến LLM để tạo phản hồi (bước 7).\nỨng dụng AI sinh sẽ lặp lại các bước 5–7 cho đến khi bạn muốn làm mới danh sách quyền truy cập (lặp lại bước 4) hoặc khi token hết hạn (lặp lại bước 3 và 4).\nFigure 4: Luồng hoạt động của ứng dụng để cấp quyền truy cập dữ liệu từ từ điển\rHình 4: Luồng hoạt động của ứng dụng trong việc cấp quyền truy cập dữ liệu từ kho tri thức\nCác phạm vi cấp quyền (grant scopes) được thể hiện trong bảng dưới đây:\nPhạm vi cấp quyền (Grant scope) Mã định danh cấp quyền (Grant ID) s3:// amzn-s3-demo-bucket/departments/sales/* edbd7575-0ba8-4837-8df1-07fe5d89f973 (sales group) s3:// amzn-s3-demo-bucket/departments/it/* a8f1d390-10d1-7037-7b27-c9fcf0b04441 (it group) s3:// amzn-s3-demo-bucket/departments/marketing/* 28f1e3c0-8081-70fe-6b4f-531ae370e7fd (marketing group s3:// amzn-s3-demo-bucket/departments/hr/* 38f11380-d011-70fb-261b-aa50d7edc1d5 (hr group) s3:// amzn-s3-demo-bucket/projects/projectA/* c84173b0-b071-70c5-3207-dadc1e6f76a9 (project A group) s3:// amzn-s3-demo-bucket/projects/projectB/* 2871d3c0-6001-7073-baaf-62717f56b8d0 (project B group) s3:// amzn-s3-demo-bucket/projects/projectC/* f8a183b0-f001-707b-aa8e-1826ca04595e (project C group) s3:// amzn-s3-demo-bucket/projects/* 28f1e3c0-8081-70fe-6b4f-531ae370e7fd (marketing group) Trong ví dụ này, bạn có thể sử dụng vai trò của Bob để minh họa cách hoạt động của việc cấp quyền cho từng phần dữ liệu (chunk authorization). Khi bạn gọi đến kho tri thức mà không thực hiện bất kỳ bước cấp quyền dữ liệu nào, bạn sẽ nhận được phản hồi như sau khi đặt câu hỏi “Tình trạng dự án của tôi là gì.” Với mỗi đối tượng trong nguồn dữ liệu, bạn cũng bao gồm metadata dưới dạng tệp *.metadata.json, được kho tri thức sử dụng để gán các cặp khóa/giá trị cụ thể cho từng đối tượng. Đây là nơi bạn thêm phân loại cho Project A và Project C là confidential và Project B là highly confidential, như đã đề cập trước đó. Bạn truyền bộ lọc này như một phần của yêu cầu đến Bedrock knowledge base bằng cách sử dụng RetrievalFilter trong retrievalConfiguration. Đoạn mã sau đây hiển thị phản hồi từ Bedrock knowledge base:\n{ \u0026#34;ResponseMetadata\u0026#34;: { ... }, \u0026#34;retrievalResults\u0026#34;: [ { \u0026#34;content\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Project A status is as follows: Project A = Sales Strategy STATUS = GREEN Project completion = 80% Notes: we are on track to complete the project by end of month\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;TEXT\u0026#34; }, \u0026#34;location\u0026#34;: { \u0026#34;s3Location\u0026#34;: { \u0026#34;uri\u0026#34;: \u0026#34;s3://amzn-s3-demo-bucket/projects/projectA/status.txt\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;S3\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;x-amz-bedrock-kb-source-uri\u0026#34;: \u0026#34;s3://amzn-s3-demo-bucket/projects/projectA/status.txt\u0026#34;, \u0026#34;classification\u0026#34;: \u0026#34;confidential\u0026#34;, \u0026#34;x-amz-bedrock-kb-chunk-id\u0026#34;: \u0026#34;1%3A0%3AnTT-15UBTG7d8qG4nL6p\u0026#34;, \u0026#34;x-amz-bedrock-kb-data-source-id\u0026#34;: \u0026#34;CIUUDCONV2\u0026#34; }, \u0026#34;score\u0026#34;: 0.558023 }, { \u0026#34;content\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Project C status is as follows: Project C = Infrastucture Update STATUS = RED Project completion = 30% Notes: ROI is not meeting expectations, rethinking strategy with project\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;TEXT\u0026#34; }, \u0026#34;location\u0026#34;: { \u0026#34;s3Location\u0026#34;: { \u0026#34;uri\u0026#34;: \u0026#34;s3://amzn-s3-demo-bucket/projects/projectC/status.txt\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;S3\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;x-amz-bedrock-kb-source-uri\u0026#34;: \u0026#34;s3://amzn-s3-demo-bucket/projects/projectC/status.txt\u0026#34;, \u0026#34;classification\u0026#34;: \u0026#34;confidential\u0026#34;, \u0026#34;x-amz-bedrock-kb-chunk-id\u0026#34;: \u0026#34;1%3A0%3AnDT-15UBTG7d8qG4mb78\u0026#34;, \u0026#34;x-amz-bedrock-kb-data-source-id\u0026#34;: \u0026#34;CIUUDCONV2\u0026#34; }, \u0026#34;score\u0026#34;: 0.52052265 } ] } Dữ liệu từ Project B không được bao gồm trong kết quả đầu ra vì nó được gắn nhãn là highly confidential. Dữ liệu từ Project C lại được bao gồm, trong khi Bob không nên có quyền truy cập vào dữ liệu này, vì vậy hãy cùng xem qua cách cấp quyền truy cập chính xác cho Bob.Trong các bước sau và với mẫu mã Python được cung cấp, tôi sẽ hướng dẫn cách gọi từng hàm được hiển thị trong khối mã bên dưới. Bạn có thể sử dụng đoạn mã này như một phần của ứng dụng để xác thực quyền truy cập đối với dữ liệu được trả về từ Bedrock knowledge base.\n# Thực thi quy trình làm việc # 1. Giả định vai trò để truy cập S3 client_s3_oidc = assume_role( args.client_id, args.grant_type, args.assertion, args.role_arn, args.role_session_name, args.provider_arn ) # 2. Lấy phạm vi S3 mà người gọi được cấp quyền scopes = get_caller_grant_scopes(client_s3_oidc, args.account) # 3. Lọc các phần dữ liệu (chunks) dựa trên quyền của người gọi authorized, not_authorized = check_grant_scopes(chunks, scopes) Bước 1: Người dùng sử dụng IdP để đăng nhập vào ứng dụng AI sinh Khi Bob lần đầu truy cập vào ứng dụng AI sinh, ứng dụng sẽ chuyển hướng anh ấy qua luồng đăng nhập một lần (single sign-on) để xác thực với IdP của họ. Bob sẽ nhận được một mã định danh đã được IdP ký xác nhận danh tính của anh ấy. Ví dụ về identity token của Bob được hiển thị như sau:\n{ \u0026ldquo;sub\u0026rdquo;: \u0026ldquo;sub\u0026rdquo;, \u0026ldquo;email\u0026rdquo;: \u0026ldquo;bob@example.com\u0026rdquo;, \u0026ldquo;aud\u0026rdquo;: \u0026ldquo;bob\u0026rdquo;, \u0026ldquo;iss\u0026rdquo;: \u0026ldquo;https://tokens.identity-solutions.example.com\u0026rdquo;, \u0026ldquo;exp\u0026rdquo;: 1744219319, \u0026ldquo;iat\u0026rdquo;: 1744218719, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;bob\u0026rdquo; } Bước 2: Trao đổi token với IAM Identity Center Sau khi Bob được xác thực và chuyển token của mình cho ứng dụng AI sinh, ứng dụng sẽ trao đổi identity token từ IdP với identity token của IAM Identity Center và lấy thông tin xác thực tạm thời (temporary credentials) thay mặt cho Bob. Bạn sẽ tạo một hàm Python có tên assume_role để truyền nhiều biến khác nhau, cho phép Bob giả định vai trò (assume role) trong AWS:\nclient_id: Chuỗi định danh duy nhất cho client hoặc ứng dụng. Giá trị này là ARN (Amazon Resource Name) của ứng dụng có cấu hình OAuth grants.\ngrant_type: Kiểu OAuth grant, trong ví dụ này sẽ là JWT Bearer.\nrole_arn: ARN của vai trò được giả định.\nrole_session_name: Định danh cho phiên làm việc của vai trò được giả định.\nprovider_arn: ARN của nhà cung cấp ngữ cảnh (context provider) mà từ đó trusted context assertion được tạo ra.\nclient_assertion: Chuỗi JSON Web Token (JWT) được cấp bởi một trusted token issuer.\nTrong hàm Python mẫu được hiển thị bên dưới, bạn sẽ thực hiện các bước sau:\nMở hai client boto3: sso-oidc (để tạo token với IAM) và sts (để giả định vai trò tạm thời cho Bob).\nSử dụng client_id, grant_type, và client_assertion để gọi create_token_with_iam, tạo ra token của IAM Identity Center và gán vào biến token_response.\nTrong token_response có chứa sts:identity_context, đây là thông tin cần thiết để giả định vai trò cho Bob.\nVới identity_context, bạn truyền ngữ cảnh này vào assume_role cùng với role_arn, role_session_name, và provider_arn để lấy thông tin xác thực tạm thời (temporary credentials) cho Bob.\nCuối cùng, bạn trả về cho ứng dụng một client boto3 cho s3-control, sử dụng thông tin xác thực tạm thời của Bob để xác minh quyền truy cập của anh ấy với S3 Access Grants.\ndef assume_role(client_id, grant_type, client_assertion, role_arn, role_session_name, provider_arn): \u0026#34;\u0026#34;\u0026#34; Assume an IAM role using SSO/OIDC authentication and return an S3 control client. Args: client_id: The ID of the OIDC client grant_type: The type of grant being requested client_assertion: The client assertion token role_arn: ARN of the role to assume role_session_name: Name for the temporary session provider_arn: ARN of the identity provider Returns: boto3.client: An S3 control client with temporary credentials \u0026#34;\u0026#34;\u0026#34; client_oidc = boto3.client(\u0026#39;sso-oidc\u0026#39;) client_sts = boto3.client(\u0026#39;sts\u0026#39;) try: # Get ID token from IAM using SSO OIDC token_response = client_oidc.create_token_with_iam( clientId=client_id, grantType=grant_type, assertion=client_assertion ) # Extract identity context from token id_token = jwt.decode(token_response[\u0026#39;idToken\u0026#39;], options={\u0026#39;verify_signature\u0026#39;: False}) identity_context = id_token[\u0026#39;sts:identity_context\u0026#39;] # Assume role using identity context temp_credentials = client_sts.assume_role( RoleArn=role_arn, RoleSessionName=role_session_name, ProvidedContexts=[{ \u0026#39;ProviderArn\u0026#39;: provider_arn, \u0026#39;ContextAssertion\u0026#39;: identity_context }] ) # Create and return S3 control client with temporary credentials creds = temp_credentials[\u0026#39;Credentials\u0026#39;] return boto3.client( \u0026#39;s3control\u0026#39;, region_name=\u0026#39;us-west-2\u0026#39;, aws_access_key_id=creds[\u0026#39;AccessKeyId\u0026#39;], aws_secret_access_key=creds[\u0026#39;SecretAccessKey\u0026#39;], aws_session_token=creds[\u0026#39;SessionToken\u0026#39;] ) except ClientError as e: print(f\u0026#39;Error: {e}\u0026#39;) sys.exit(1) Bước 3: Truy xuất phạm vi cấp quyền của người gọi (caller grant scopes) Tiếp theo, bạn cần truy xuất những gì Bob được phép truy cập trong nguồn dữ liệu bằng cách sử dụng S3 Access Grants. Trong ví dụ này, bạn cần xác thực dữ liệu mà Bob được cấp quyền truy cập với nguồn dữ liệu, chứ không phải với từng đối tượng S3 cụ thể.\nĐể lấy danh sách các tiền tố (prefix) mà Bob được phép truy cập, bạn sẽ thực hiện các bước sau trong hàm get_caller_grant_scopes:\nTruyền vào client s3control được trả về từ hàm assume_role, cùng với tài khoản (account) liên quan đến S3 Access Grants.\nVới vai trò tạm thời của Bob, bạn gọi hàm list_caller_access_grants. Lệnh này sẽ trả về danh sách các quyền truy cập (access grants) mà Bob có.\nVí dụ, khi bạn gọi hàm này cho Bob, bạn sẽ nhận được phản hồi từ list_caller_access_grants như bên dưới, cho thấy rằng Bob có quyền truy cập vào tiền tố sales và projectA. Đoạn mã ví dụ được thể hiện như sau:\n{ \u0026ldquo;ResponseMetadata\u0026rdquo;: { \u0026hellip; }, \u0026ldquo;CallerAccessGrantsList\u0026rdquo;: [ { \u0026ldquo;Permission\u0026rdquo;: \u0026ldquo;READ\u0026rdquo;, \u0026ldquo;GrantScope\u0026rdquo;: \u0026ldquo;s3:// amzn-s3-demo-bucket/departments/sales/*\u0026rdquo;, \u0026ldquo;ApplicationArn\u0026rdquo;: \u0026ldquo;ALL\u0026rdquo; }, { \u0026ldquo;Permission\u0026rdquo;: \u0026ldquo;READ\u0026rdquo;, \u0026ldquo;GrantScope\u0026rdquo;: \u0026ldquo;s3:// amzn-s3-demo-bucket/projects/projectA/*\u0026rdquo;, \u0026ldquo;ApplicationArn\u0026rdquo;: \u0026ldquo;ALL\u0026rdquo; } ] } Bạn thêm các phạm vi cấp quyền (scopes) vào một mảng (array) và trả mảng này về cho ứng dụng. Ví dụ về đoạn mã minh họa được hiển thị bên dưới.Lưu ý*:* bạn loại bỏ ký tự * khỏi access grant vì URI của từng chunk là đường dẫn đầy đủ, không chỉ là tiền tố (prefix).\ndef get_caller_grant_scopes(client, account): \u0026#34;\u0026#34;\u0026#34; Retrieve the S3 access scopes granted to a caller. Args: client: S3 control client with assumed role credentials account: AWS account ID Returns: List of S3 path prefixes the caller is authorized to access \u0026#34;\u0026#34;\u0026#34; try: # Get list of access grants for the caller response = client.list_caller_access_grants(AccountId=account) # Extract S3 path prefixes and remove trailing wildcards scopes = [grant[\u0026#39;GrantScope\u0026#39;].replace(\u0026#39;*\u0026#39;,\u0026#39;\u0026#39;) for grant in response[\u0026#39;CallerAccessGrantsList\u0026#39;]] return scopes except ClientError as e: print(f\u0026#39;Error: {e}\u0026#39;) sys.exit(1) Tại thời điểm này, bạn đã có danh sách phạm vi cấp quyền (grant scopes) mà Bob được phép truy cập trong nguồn dữ liệu. Thông tin này giờ đây có thể được sử dụng để đối chiếu với các phần dữ liệu (chunks) được trả về từ kho tri thức (knowledge base), nhằm xác thực quyền truy cập trước khi truyền prompt cuối cùng cùng với ngữ cảnh bổ sung cho LLM.\nBước 4: Kiểm tra phạm vi cấp quyền của người gọi (Check caller grant scopes) Bước cuối cùng là đối chiếu các chunk được trả về từ kho tri thức với danh sách các quyền (grants) mà Bob được phép truy cập.\nĐể làm điều này, bạn định nghĩa hàm check_grant_scopes và truyền vào cả hai đối số: chunks và scopes mà Bob được ủy quyền truy cập.\nBiến chunks là một mảng chứa các dictionary, bạn sẽ duyệt qua và xác thực từng phần dữ liệu so với danh sách scopes, như được minh họa trong đoạn mã ví dụ bên dưới.\nDuyệt qua từng chunk được truyền vào hàm.\nVới mỗi chunk, kiểm tra xem vị trí của chunk có bắt đầu bằng prefix nào nằm trong S3 access grant hay không.\nNếu tìm thấy khớp, bạn thêm chunk đó (cùng với scope tương ứng trong S3 access grant) vào danh sách authorized chunks.\nNếu không tìm thấy khớp trong scopes, bạn thêm chunk đó vào danh sách not_authorized chunks.\nHàm sẽ trả về hai danh sách — một danh sách các chunk được cấp quyền (authorized) và một danh sách các chunk bị từ chối quyền truy cập (not_authorized) — giúp bạn dễ dàng xem những phần dữ liệu nào Bob không được phép truy cập.\ndef check_grant_scopes(chunks, scopes): \u0026#34;\u0026#34;\u0026#34; Check which chunks a user is authorized to access based on their granted scopes. Args: chunks: List of dictionaries containing content chunks with \u0026#39;location\u0026#39; keys scopes: List of authorized S3 path prefixes the user has access to Returns: tuple: (authorized_chunks, unauthorized_chunks) \u0026#34;\u0026#34;\u0026#34; authorized = [] not_authorized = [] # If user has no scopes, they are not authorized for any chunks if not scopes: return [], chunks # Check each chunk against available scopes for chunk in chunks: location = chunk[\u0026#39;location\u0026#39;] authorized_scope = next((scope for scope in scopes if location.startswith(scope)), None) if authorized_scope: chunk[\u0026#39;scope\u0026#39;] = authorized_scope authorized.append(chunk) else: not_authorized.append(chunk) return authorized, not_authorized Khi chạy hàm trên cho Bob cùng với các chunk được trả về từ kho tri thức, bạn sẽ nhận được danh sách các chunk được cấp quyền (authorized chunks) và không được cấp quyền (not authorized chunks) như minh họa trong ví dụ dưới đây. # Authorized: [ { \u0026#34;content\u0026#34;: \u0026#34;Project A status is as follows: Project A = Sales Strategy STATUS = GREEN Project completion = 80% Notes: we are on track to complete the project by end of month\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;s3://amzn-s3-demo-bucket/projects/projectA/status.txt\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;s3://amzn-s3-demo-bucket/projects/projectA/\u0026#34; } ] # Not Authorized: [ { \u0026#34;content\u0026#34;: \u0026#34;Project C status is as follows: Project C = Infrastucture Update STATUS = RED Project completion = 30% Notes: ROI is not meeting expectations, rethinking strategy with project\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;s3://amzn-s3-demo-bucket/projects/projectC/status.txt\u0026#34; } ] Các yếu tố cần xem xét trong giải pháp\nKhi triển khai kiến trúc cấp quyền truy cập (authorization) cho các hệ thống RAG, có một số yếu tố quan trọng cần hiểu rõ vì chúng ảnh hưởng đến bảo mật, hiệu năng và khả năng mở rộng. Việc cân nhắc đúng những yếu tố này giúp đảm bảo rằng hệ thống của bạn duy trì được mức độ kiểm soát bảo mật mạnh mẽ, đồng thời tối ưu hiệu suất và linh hoạt với nhiều nguồn dữ liệu khác nhau. Dưới đây là các điểm quan trọng cần đánh giá khi thiết kế và triển khai mô hình cấp quyền này:\nTrong ví dụ này, bạn đã sử dụng S3 Access Grants như một cách để kiểm tra quyền truy cập. Tuy nhiên, kiến trúc này hoàn toàn có thể áp dụng cho các nguồn dữ liệu khác, miễn là URI của nguồn dữ liệu được trả về từ kho tri thức, và API có sẵn để kiểm tra quyền truy cập của principal — tương tự như hàm get_caller_grant_scopes được mô tả ở phần trước.\nViệc sử dụng S3 Access Grants cung cấp cơ chế xác thực để principal truy cập vào nguồn dữ liệu. Tuy nhiên, bạn vẫn có thể áp dụng chính sách kiểm soát truy cập bổ sung cho từng bucket bằng cách thêm thẻ khóa/giá trị (key/value tag) hoặc chính sách ở cấp nguồn dữ liệu, nếu cần. Bằng cách này, principal có thể bị từ chối truy cập bucket ngay cả khi S3 Access Grants cho phép.\nĐể hỗ trợ chức năng này, bạn có thể thêm metadata cho cơ sở dữ liệu vector để hệ thống lọc truy vấn vào kho tri thức, như trong ví dụ trước.\nGiống như dữ liệu trong kho tri thức có thể trở nên lỗi thời giữa các lần đồng bộ (resync), danh sách phạm vi được cấp quyền (authorized scopes) cũng có thể không còn chính xác theo thời gian. Bạn cần xác định tần suất làm mới danh sách quyền (bước 3 trong Hình 4) và thời lượng hiệu lực của vai trò được giả định (bước 2 trong Hình 4).\nTùy thuộc vào các chunk mà principal được phép truy cập và kết quả trả về từ kho tri thức, một số chunk có thể bị loại bỏ trước khi gửi đến LLM.\nVề mặt bảo mật, điều này là tốt, vì nó đảm bảo người dùng không thể truy cập dữ liệu mà họ không có quyền.\nKết luận\nTrong bài viết này, tôi đã giới thiệu cho bạn một mô hình kiến trúc giúp thực hiện cơ chế cấp quyền mạnh mẽ đối với các kết quả được trả về từ kho tri thức (knowledge base). Bạn đã cùng tìm hiểu tầm quan trọng của việc cấp quyền chặt chẽ trong kho tri thức, cũng như cách triển khai cơ chế này bằng Amazon S3 Access Grants.\nCuối cùng, chúng ta đã cùng xem qua các ví dụ mã thực tế, minh họa cách cơ chế này hoạt động khi kết hợp Amazon Bedrock Knowledge Bases với S3 Access Grants — cho phép bạn kiểm soát truy cập dữ liệu một cách an toàn, linh hoạt và hiệu quả hơn trong các hệ thống RAG.\nĐể tìm hiểu thêm về bảo mật trong các hệ thống AI sinh (generative AI), bạn có thể tham khảo thêm các bài viết khác trên AWS Security Blog và các AWS blog posts covering generative AI trên AWS Blog.\nNếu bạn có phản hồi về bài viết này, vui lòng để lại bình luận trong phần Comments bên dưới.\nNếu bạn có câu hỏi liên quan đến nội dung bài viết, hãy liên hệ AWS Support. để được hỗ trợ.\nGiới thiệu về các tác giả Riggs Goodman III Riggs là Kiến trúc sư Giải pháp Đối tác Cấp cao (Principal Partner Solution Architect) tại AWS. Trọng tâm hiện tại của ông là bảo mật và mạng cho AI, cung cấp hướng dẫn kỹ thuật, mô hình kiến trúc và định hướng cho khách hàng và đối tác trong việc xây dựng các khối lượng công việc AI trên AWS. Nội bộ tại AWS, Riggs tập trung vào việc thúc đẩy chiến lược kỹ thuật tổng thể và đổi mới trên các nhóm dịch vụ AWS để giải quyết những thách thức của khách hàng và đối tác. .\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Công bố tính năng ingestion đa tài khoản cho Amazon OpenSearch Service Bởi David Venable, ngày 19 tháng 9 năm 2025 | Amazon OpenSearch Service, Announcements, Intermediate (200)\nAmazon OpenSearch Ingestion là một pipeline ingestion dữ liệu mạnh mẽ mà khách hàng AWS sử dụng cho nhiều mục đích khác nhau, chẳng hạn như khả năng quan sát (observability), phân tích (analytics), và tìm kiếm không cần ETL (zero-ETL search). Nhiều khách hàng ngày nay đẩy log, trace và metric từ các ứng dụng của họ vào OpenSearch Ingestion để lưu trữ và phân tích dữ liệu này.\nHôm nay, chúng tôi vui mừng thông báo rằng các pipeline của OpenSearch Ingestion hiện đã hỗ trợ ingestion đa tài khoản (cross-account ingestion) cho các nguồn dữ liệu dạng push như HTTP và OpenTelemetry (OTel). Các tổ chức giờ đây có thể sử dụng tính năng này để dễ dàng chia sẻ dữ liệu giữa các nhóm. Ví dụ, nhiều tổ chức có các nhóm chuyên trách observability trung tâm — giờ đây các nhóm này có thể tạo các pipeline OpenSearch Ingestion và chia sẻ chúng với các nhóm khác trong tổ chức của họ. Bạn cũng có thể sử dụng tính năng này để ingestion dữ liệu vào các domain của Amazon OpenSearch Service hoặc các collection Amazon OpenSearch Serverless trong các tài khoản khác.\nTrước đây, việc chia sẻ các pipeline OpenSearch Ingestion giữa các tài khoản yêu cầu các nhóm phải sử dụng các tính năng của virtual private cloud (VPC) để chia sẻ quyền truy cập. Ví dụ, các nhóm có thể sử dụng VPC peering, vốn không phải lúc nào cũng khả thi, hoặc AWS Transit Gateway. Các tính năng ingestion đa tài khoản mới trong OpenSearch Ingestion có thể đơn giản hóa việc triển khai của bạn và giảm chi phí cho việc chia sẻ pipeline.\nTổng quan về giải pháp\nHãy cùng nhìn cách chia sẻ một pipeline từ một tài khoản logging trung tâm với hai tài khoản phát triển khác (A và B). Tài khoản logging trung tâm có thể tạo một pipeline OpenSearch Ingestion bằng cách sử dụng một nguồn dạng push, ví dụ như HTTP. Sau khi tạo pipeline, một thành viên của nhóm logging trung tâm có thể cấp quyền truy cập cho các nhóm khác. Họ có thể sử dụng một resource policy cung cấp quyền cho hai tài khoản nhóm còn lại để tạo các endpoint của pipeline. Sau khi thực hiện thay đổi này, pipeline OpenSearch Ingestion sẽ có sẵn để các nhóm khác sử dụng.\nSơ đồ dưới đây minh họa cấu trúc này.\nFigure 1\rTrong các phần tiếp theo, chúng sẽ minh họa cách triển khai giải pháp này.\nCác điều kiện tiên quyết (Prerequisites) Trước hết, tài khoản logging trung tâm phải có một VPC với hai tùy chọn được bật:\nenableDnsSupport phải được đặt thành true\nenableDnsHostnames phải được đặt thành true\nTài khoản logging trung tâm cũng phải tạo một pipeline OpenSearch Ingestion dạng push trong VPC. Đây có thể là một pipeline nhận log từ receiving logs from FluentBit hoặc OpenTelemetry telemetry.\nCác tài khoản phát triển (development accounts) sẽ kết nối với pipeline cũng phải có VPC trong cùng một vùng (region) với các tùy chọn DNS giống nhau được bật:\nenableDnsSupport phải được đặt thành true\nenableDnsHostnames phải được đặt thành true\nTạo resource policy Là chủ sở hữu của pipeline, bạn có thể tạo một resource policy cho phép hai tài khoản phát triển (development accounts) tạo các endpoint của pipeline dựa trên pipeline của bạn.\nDưới đây là một ví dụ về resource policy cho kịch bản này:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: [ \u0026#34;000000000000\u0026#34;, \u0026#34;999999999999\u0026#34; ] }, \u0026#34;Action\u0026#34;: \u0026#34;osis:CreatePipelineEndpoint\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:osis:us-west-2:123456789012:pipeline/central-logging\u0026#34; } ] } Bảng điều khiển OpenSearch Ingestion giúp việc tạo các policy này trở nên đơn giản, như minh họa trong ảnh chụp màn hình sau.\nFigure 2\rTạo pipeline endpoint Bây giờ khi tài khoản logging trung tâm đã chia sẻ quyền trên pipeline của họ, các tài khoản phát triển có thể tạo pipeline endpoint. Một pipeline endpoint là một kết nối từ một VPC đến một pipeline OpenSearch Ingestion.\nCác tài khoản phát triển chịu trách nhiệm tạo các pipeline endpoint trong các VPC mà họ muốn kết nối từ đó. Họ tạo endpoint này trong các subnet cần thiết và cung cấp một security group. Security group này phải có một inbound rule cho phép truy cập cổng HTTPS qua cổng 443 từ bất kỳ nguồn nào mà các tài khoản phát triển cần để ingestion log.\nNhóm phát triển A có thể tạo một pipeline endpoint bằng cách sử dụng một lệnh tương tự như sau:\naws --region us-west-2 osis create-pipeline-endpoint \\ --pipeline-arn arn:aws:osis:us-west-2:123456789012:pipeline/central-logging \\ --vpc-options \u0026#39;{\u0026#34;SubnetIds\u0026#34;:[\u0026#34;subnet-123456789012345678\u0026#34;,\u0026#34;subnet-012345678912345678\u0026#34;],\u0026#34;SecurityGroupIds\u0026#34;:[\u0026#34;sg-123456789012345678\u0026#34;]}\u0026#39; Nhóm phát triển A cũng có thể sử dụng bảng điều khiển (OpenSearch Ingestion console) để tạo pipeline endpoint.\nFigure 3\rSau khi thực hiện thay đổi này, VPC của nhóm phát triển A sẽ có một pipeline endpoint. Pipeline endpoint này hiện cho phép ingestion dữ liệu vào central logging pipeline. Giờ đây, các Amazon Elastic Compute Cloud (Amazon EC2) instances,Amazon Elastic Container Service (Amazon ECS) tasks, Kubernetes pods, và các workload tính toán khác chạy trong VPC có thể ingestion dữ liệu log của chúng vào pipeline bằng các công cụ như FluentBit.\nĐồng thời hoặc sau đó, nhóm phát triển B cũng có thể tạo một pipeline endpoint. Nhóm này sẽ tạo endpoint cho VPC riêng của họ.\nSau bước này, pipeline sẽ có hai pipeline endpoints, vì vậy cả hai nhóm đều có thể ingestion dữ liệu log của họ vào central logging VPC.\nDọn dẹp (Clean up) Sau khi một pipeline endpoint được tạo, bất kỳ tài khoản nào cũng có thể xóa nó. Các nhóm phát triển trong kịch bản của chúng ta có thể sử dụng DeletePipelineEndpoint API để xóa endpoint khỏi tài khoản của họ. Ngoài ra, nếu tài khoản logging trung tâm cần loại bỏ một pipeline endpoint khỏi pipeline, họ có thể sử dụng RevokePipelineEndpointConnections API. Cả hai tùy chọn đều có sẵn trên OpenSearch Ingestion console.\nSau khi các pipeline endpoints bị xóa, nhóm logging trung tâm cũng có thể xóa luôn pipeline nếu họ không còn cần đến nó\nKết luận Tính năng pipeline endpoint mới cho OpenSearch Ingestion giúp đơn giản hóa cách bạn có thể chia sẻ pipeline cho cross-account ingestion. Điều này có thể giúp các nhóm tận dụng những tính năng mạnh mẽ của OpenSearch Ingestion và mở ra những khả năng mới cho các nhóm hoặc tổ chức sử dụng nhiều tài khoản và VPC. Tính năng pipeline endpoint mới hiện đã có sẵn trong các AWS Regions nơi OpenSearch Ingestion được hỗ trợ.\nĐể bắt đầu với cross-account ingestion trong OpenSearch Ingestion, hãy tham khảo tài liệu OpenSearch Ingestion documentation hoặc thử tạo pipeline cross-account đầu tiên của bạn trên OpenSearch Ingestion console.\n.\nGiới thiệu về tác giả David Venable David là Kỹ sư Phần mềm cao cấp tại Amazon Web Services (AWS), hiện đang làm việc trong mảng quan sát hệ thống (observability) của OpenSearch. Anh cũng là người duy trì (maintainer) của dự án Data Prepper. "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Báo cáo tóm tắt: “AI-Powered Cloud Solutions \u0026amp; Amazon Bedrock Workshop” Mục tiêu sự kiện Giới thiệu Foundation Models và sự khác biệt giữa ML truyền thống và Generative AI Cung cấp kiến thức thực hành về Prompt Engineering và RAG Khám phá AWS AI Services và các ứng dụng thực tế Giới thiệu Amazon Bedrock AgentCore để xây dựng AI agent có khả năng mở rộng Chia sẻ lời khuyên nghề nghiệp và tầm quan trọng của việc xây dựng sản phẩm AI thực tế cho portfolio Diễn giả Lâm Tuấn Kiệt – Sr. DevOps Engineer, FPT Software Đặng Hoàng Hiếu Nghị – AI Engineer, Reonova Cloud Đinh Lê Hoàng Anh – Cloud Engineer Trainee, FCJ Đại diện FPT – Chia sẻ về việc doanh nghiệp ứng dụng AI và phát triển sản phẩm cloud Điểm nổi bật chính Hiểu về Foundation Models so với ML truyền thống ML truyền thống: thường tối ưu cho một nhiệm vụ, cần dữ liệu gán nhãn, khả năng khái quát hóa hạn chế Foundation Models: huấn luyện trên dữ liệu lớn (thường không gán nhãn), theo hướng self-supervised, hỗ trợ đa nhiệm; có thể khai thác qua các dịch vụ như Amazon Bedrock Bedrock hỗ trợ nhiều mô hình khác nhau Các kỹ thuật Prompt Engineering Zero-shot prompting: đưa ít hướng dẫn → đầu ra đơn giản, ngắn gọn Few-shot prompting: cung cấp ví dụ để “định hướng” mô hình Chain-of-Thought prompting: khuyến khích suy luận theo từng bước để tăng độ chính xác Ví dụ thảo luận: câu hỏi “What is 10 + 10?” có thể bị hiểu theo nhiều cách nếu thiếu ngữ cảnh Retrieval Augmented Generation (RAG) Truy xuất thông tin liên quan từ nguồn dữ liệu trước khi tạo câu trả lời Tăng độ chính xác, giảm hallucination, hỗ trợ dữ liệu doanh nghiệp tùy biến Embeddings chuyển văn bản thành vector; AWS Titan Text Embeddings hỗ trợ hơn 100 ngôn ngữ Minh họa quy trình RAG trong thực tế Tổng quan AWS AI Services Rekognition – Phân tích ảnh/video Translate – Tự phát hiện ngôn ngữ \u0026amp; dịch Textract – Trích xuất text \u0026amp; cấu trúc tài liệu Transcribe – Speech-to-text Polly – Text-to-speech Comprehend – NLP insights, phân tích cảm xúc Kendra – Tìm kiếm thông minh trên tài liệu Lookout Family – Phát hiện bất thường trong chỉ số, thiết bị và thị giác máy tính Personalize – Cá nhân hóa gợi ý Pipecat – Framework pipeline cho AI agents Các dịch vụ đều có thể truy cập qua API tương đối đơn giản Amazon Bedrock AgentCore Nền tảng phát triển AI agent mà không cần gánh nặng DevOps quá lớn Giải quyết các thách thức về mở rộng, quản lý bộ nhớ, danh tính/quyền truy cập và tích hợp công cụ Các cơ chế chính gồm: Runtime, Memory, Identity, Gateway, Code Interpreter, Browser Tool, Observability Hướng tới xây dựng trợ lý AI sẵn sàng cho môi trường production và tự động hóa workflow Các điểm rút ra (Key Takeaways) Tư duy thiết kế Xây dựng dự án thực tế là rất quan trọng — không chỉ dừng ở bài tập học thuật Doanh nghiệp ngày càng tập trung vào sản phẩm cloud ứng dụng AI Hiểu nhu cầu kinh doanh là chìa khóa để tạo ra giải pháp AI có ý nghĩa Kiến trúc kỹ thuật Foundation Models linh hoạt hơn ML truyền thống Prompt engineering ảnh hưởng trực tiếp đến độ chính xác và độ tin cậy RAG cải thiện tính đúng ngữ cảnh bằng cách kết hợp prompt với dữ liệu doanh nghiệp AWS AI Services giúp tăng tốc phát triển và giảm vận hành Chiến lược hiện đại hóa Dùng Bedrock để triển khai các tính năng AI có thể mở rộng và dùng đa mục đích Kết hợp embeddings và RAG cho ứng dụng cấp doanh nghiệp Dùng AgentCore để đơn giản hóa việc triển khai các AI agent phức tạp Ứng dụng vào công việc Xây dựng prototype nhỏ với Amazon Bedrock và AWS AI Services Thử nghiệm các kỹ thuật prompt engineering để cải thiện kết quả Áp dụng RAG để nâng chất lượng chatbot nội bộ hoặc hệ thống tự động hóa Dùng AgentCore để tạo AI agent có khả năng ra quyết định đa bước Đưa các dự án AI đã hoàn thiện vào portfolio/CV theo gợi ý từ diễn giả Trải nghiệm sự kiện Tham gia “AI-Powered Cloud Solutions \u0026amp; Amazon Bedrock Workshop” mang lại nhiều góc nhìn sâu về phát triển AI hiện đại và kiến trúc cloud.\nHọc hỏi từ diễn giả trong ngành Diễn giả chia sẻ kiến thức thực tiễn và làm rõ khác biệt giữa ML truyền thống và Foundation Models Ví dụ thực tế giúp hình dung cách doanh nghiệp triển khai AI ở quy mô lớn Tiếp cận kỹ thuật thực hành Demo embeddings, RAG và prompt engineering Hiểu khi nào nên dùng Zero-shot, Few-shot hoặc Chain-of-Thought Nắm cách AWS AI services tích hợp vào workflow sản phẩm Tận dụng công cụ hiện đại Tiếp cận hệ sinh thái Bedrock và khả năng dùng nhiều mô hình Hiểu AgentCore để xây dựng và mở rộng AI agent Học cách tích hợp nhanh qua API cho dự án thực tế Kết nối và trao đổi Diễn giả khuyến khích xây dự án AI thực tế để tăng giá trị CV Thảo luận cho thấy nhu cầu AI trên nền cloud đang tăng mạnh trong doanh nghiệp Bài học rút ra Foundation Models tăng tính linh hoạt so với ML truyền thống RAG rất quan trọng để xây hệ thống AI có nền tảng dữ kiện (factual grounding) Kinh nghiệm xây sản phẩm thực tế có giá trị lớn cho phát triển nghề nghiệp Hình ảnh sự kiện Figure 1\rFigure 2\rFigure 3\r\u003e Nhìn chung, workshop cung cấp nền tảng kỹ thuật vững, góc nhìn thực tiễn và động lực để xây dựng các sản phẩm AI “thật” dựa trên công nghệ AWS.\r"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Báo cáo tóm tắt: “AWS Cloud, AI \u0026amp; Innovation Summit” Mục tiêu sự kiện Trình bày chiến lược quốc gia về mở rộng hạ tầng cloud và thúc đẩy chuyển đổi số Củng cố quan hệ đối tác Hoa Kỳ – Việt Nam trong công nghệ và đổi mới sáng tạo Chia sẻ góc nhìn về AI, blockchain và phát triển hệ sinh thái định hình tương lai Việt Nam Nhấn mạnh các sáng kiến của AWS về phát triển nhân tài, tăng khả năng tiếp cận cloud và AI có trách nhiệm Cung cấp kiến thức kỹ thuật thực hành về phát triển ứng dụng theo hướng AI và bảo mật AI Diễn giả Đại diện Chính phủ Việt Nam Đại sứ Hoa Kỳ tại Việt Nam Eric Elock – CEO phụ trách Việt Nam, Lào, Campuchia \u0026amp; Myanmar Chloe Phùng – CEO, U2U Erik – Lãnh đạo AWS Jaime Valless – Lãnh đạo AWS Các chuyên gia kỹ thuật AWS – Các phiên kỹ thuật buổi chiều Điểm nổi bật chính Chiến lược quốc gia về hạ tầng cloud và chuyển đổi số Chính phủ nhấn mạnh mở rộng điện toán đám mây và hệ thống số là nền tảng cho Công nghiệp 4.0 Đảm bảo an ninh, an toàn và bảo vệ thông tin trên toàn bộ dịch vụ số Tạo điều kiện hợp tác mở giữa khu vực công, doanh nghiệp tư nhân và nhà đầu tư nước ngoài Cloud được định vị là yếu tố chiến lược thúc đẩy tăng trưởng kinh tế và hiện đại hóa quốc gia Quan hệ Hoa Kỳ – Việt Nam và phát triển công nghệ Đại sứ Hoa Kỳ nhấn mạnh quan hệ đối tác 30 năm giữa hai quốc gia Các công ty công nghệ như AWS đóng vai trò cầu nối thúc đẩy đồng phát triển Tập trung vào lợi ích kinh tế song phương và hợp tác dài hạn Đổi mới thông qua hỗ trợ ngân hàng và hệ sinh thái blockchain – Eric Elock Ngành ngân hàng giữ vai trò quan trọng trong việc hỗ trợ hiện đại hóa CNTT U2U xây dựng hệ sinh thái giúp doanh nghiệp và người dùng tương tác thông qua blockchain Thể hiện cách cloud + blockchain tạo nên các mô hình kinh tế số mới AI định hình tương lai Việt Nam – Chloe Phùng Hai năm trước, nhiều đối tác cho rằng ý tưởng của U2U là “không thể”, nhưng nay đã trở thành hiện thực Việt Nam không chỉ bắt kịp xu hướng AI toàn cầu mà còn góp phần định hình cuộc cách mạng Tác động thực tế của AI tại Việt Nam Giáo dục:\n60% học sinh/sinh viên Việt Nam sử dụng ứng dụng EdTech AI phá vỡ rào cản ngôn ngữ và tăng mức độ tương tác trong học tập Kinh tế:\nHơn 765 startup AI, xếp thứ 2 trong ASEAN Dự kiến đóng góp GDP: 120–130 tỷ USD Tác động xã hội:\nBệnh viện ứng dụng AI giúp rút ngắn thời gian khám xuống 5 phút/ca AI hỗ trợ quản lý giao thông, theo dõi năng lượng và bảo vệ bờ biển Ví dụ công nghệ Nubila – Dự báo thời tiết bằng AI Staex – Triển khai thành công 1.000+ thiết bị IoT tại châu Á và châu Âu Sự kết hợp AI \u0026amp; blockchain GenAI rút ngắn thời gian phát triển từ vài tuần xuống còn vài giờ hoặc vài ngày Blockchain trở nên dễ tiếp cận hơn với người mới nhờ AI hỗ trợ AI giúp doanh nghiệp và nhà hoạch định chính sách ra quyết định tốt hơn mỗi ngày Gửi lời cảm ơn đến AWS vì góp phần thúc đẩy hệ sinh thái Các sáng kiến của AWS tại Việt Nam – Erik AWS đã đào tạo hơn 100.000 người học cloud tại Việt Nam Nỗ lực mở rộng khả năng tiếp cận dịch vụ cloud trên toàn quốc Giới thiệu chương trình FJC 6 tháng, tạo lộ trình nghề nghiệp an toàn Nhấn mạnh văn hóa AWS là một lợi thế khác biệt Khi văn hóa gặp đổi mới – Jaime Valless Nhân loại đang ở thời điểm đặc biệt khi AI sẽ biến đổi mọi ngành nghề Chuyển đổi AI không chỉ là công nghệ mà còn là kỹ năng, con người, văn hóa và trách nhiệm Khuyến khích học tập liên tục và sử dụng AI một cách có trách nhiệm AWS hỗ trợ triển khai an toàn với khả năng truy cập đa mô hình và cơ chế bảo vệ Ví dụ use case Nearmap: Dùng AI để giúp khách hàng ra quyết định nhanh hơn bằng cách tự động hóa tác vụ lặp lại, để con người tập trung vào sáng tạo và tư duy phản biện Các điểm rút ra (Key Takeaways) Tư duy thiết kế Cloud, AI và blockchain kết hợp tạo ra cơ hội đổi mới rất lớn Hợp tác giữa chính phủ, doanh nghiệp và đối tác toàn cầu giúp tăng tốc chuyển đổi số AI cần được triển khai có trách nhiệm, đi kèm bảo mật mạnh và giám sát của con người Kiến trúc kỹ thuật AI tăng tốc vòng đời phát triển nhờ tự động hóa coding và testing Trường hợp IoT và mô hình thời tiết cho thấy AI có thể mở rộng đa ngành GenAI giảm rào cản tiếp cận các hệ thống phức tạp như blockchain Chiến lược hiện đại hóa Ứng dụng cloud + AI + blockchain cho các chuyển đổi có tác động cao Đầu tư học tập liên tục và phát triển kỹ năng cloud–AI Duy trì thực hành AI có trách nhiệm: kiểm soát truy cập, chống hallucination, bảo vệ prompt, bảo vệ dữ liệu Ứng dụng vào công việc Khám phá workflow phát triển theo hướng AI bằng các công cụ AWS Áp dụng Responsible AI: theo dõi người dùng, human-in-the-loop, bảo vệ prompt, kiểm tra/validate dữ liệu Cân nhắc RAG để tạo đầu ra AI cấp doanh nghiệp: an toàn và chính xác hơn Thử nghiệm các dịch vụ như Amazon Q và QuickSight để prototype nhanh và dựng dashboard Trải nghiệm sự kiện Tham gia “AWS Cloud, AI \u0026amp; Innovation Summit” mang lại nhiều góc nhìn về cách cloud, AI và blockchain đang định hình chuyển đổi số tại Việt Nam.\nHọc hỏi từ các lãnh đạo trong ngành Đại diện Chính phủ, đại diện quốc tế và lãnh đạo AWS chia sẻ tầm nhìn chiến lược cho Việt Nam Ví dụ thực tế giúp thấy rõ AI có thể chuyển đổi giáo dục, kinh tế và xã hội Tiếp cận kỹ thuật thực hành Phiên buổi chiều có nội dung về AWS SageMaker, AI-driven SDLC và bảo mật ứng dụng AI Minh họa một vòng đời phát triển được tăng cường bằng AI: Inception: lên ý tưởng, xác định yêu cầu Construction: mô hình hóa domain, sinh code, testing, triển khai IaC Operation: triển khai production, xử lý sự cố Thực hành bảo mật cho ứng dụng AI Đề cập các rủi ro: hallucination, data poisoning, prompt security, access control Nêu các kiến trúc triển khai an toàn: xác thực chuỗi cung ứng (supply-chain validation), theo dõi người dùng, và dùng RAG Tận dụng công cụ AWS Amazon Q và QuickSight giúp đơn giản hóa tạo dashboard và tự động hóa workflow AI hỗ trợ coding, viết tài liệu và refactor, giúp dev tăng tốc nhưng vẫn kiểm soát được chất lượng Bài học rút ra AI và cloud tạo hiệu quả và đổi mới trên nhiều lĩnh vực Bảo mật mạnh, thực hành có trách nhiệm và giám sát của con người là yếu tố bắt buộc Việt Nam có đà phát triển và tiềm năng lớn trong AI và chuyển đổi số Hình ảnh sự kiện Figure 1\rFigure 2\rFigure 3\r\u003e Nhìn chung, summit mang lại góc nhìn chiến lược, kiến thức kỹ thuật và ví dụ thực tế cho thấy AI, cloud và blockchain đang định hình tương lai Việt Nam.\r"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/5-workshop/5.1-workshop-overview/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "WEBSITE TRỰC TUYẾN THEO DÕI VÀ DỰ BÁO QUỸ ĐẠO BÃO Trong workshop này, nhóm chúng em trình bày cách xây dựng một nền tảng trực tuyến cho phép người dùng truy cập Internet có thể tự do kiểm tra, theo dõi và thậm chí dự đoán đường đi của các cơn bão đang hoạt động tại khu vực Tây Thái Bình Dương. Nền tảng này giúp người dùng chủ động chuẩn bị cho các thảm họa tự nhiên sắp xảy ra và giảm thiểu thiệt hại tiềm tàng.\nNền tảng cung cấp hai chức năng chính:\nHiển thị các cơn bão gần nhất – Cho phép người dùng xem đường đi, cường độ, tốc độ gió và các đặc điểm khác của bão gần đây trong khu vực Tây Thái Bình Dương.\nDự đoán quỹ đạo bão – Cho phép người dùng nhập dữ liệu vị trí bão trong quá khứ (vĩ độ và kinh độ; tối thiểu 9 điểm dữ liệu) để hệ thống dự đoán hướng di chuyển trong tương lai.\nPhân chia theo tiến độ, chúng em sẽ lần lượt trình bày về bộ dữ liệu, quá trình tiền xử lý, pipeline huấn luyện mô hình, và quá trình xây dựng nền tảng trực tuyến bằng các dịch vụ AWS. Chúng em cũng sẽ perform kỹ thuật tăng cường dữ liệu được đề xuất — Stepwise Temporal Fading Augmentation (STFA) cùng với việc ứng dụng machine learning dựa trên các quy tắc vật lý (physics-informed ML). Những phương pháp này giúp dữ liệu huấn luyện trở nên chân thực hơn và cải thiện đáng kể độ chính xác trong dự đoán đường đi bão, tuổi thọ bão và tổng quãng đường di chuyển.\nHình 1 : Pipeline mô hình\rSau khi hoàn thành quá trình huấn luyện mô hình, chúng em triển khai xây dựng nền tảng trực tuyến bằng kiến trúc serverless. Đây là kiến trúc tiết kiệm chi phí, có khả năng mở rộng tốt, và dễ dàng bảo trì/triển khai — rất phù hợp cho mục tiêu dự án. Dưới đây là các dịch vụ AWS chính được sử dụng:\nAWS Lambda – Chạy các mô hình ML và xử lý logic phía backend Amazon S3 – Lưu trữ file tĩnh, mô hình ML và dữ liệu bão Amazon API Gateway – Định tuyến và phần luồng các yêu cầu của người dùng đến Lambda phù hợp, tùy theo việc họ xem dữ liệu bão gần đây hay chạy dự đoán Amazon CloudFront – Tăng tốc phân phối nội dung thông qua các edge location AWS Secrets Manager – Lưu trữ khóa API và các thông tin nhạy cảm … – Các dịch vụ hỗ trợ bổ sung khác khi cần Hình 2 : Kiến trúc nền tảng\r"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/1-worklog/",
	"title": "Nhật ký công việc",
	"tags": [],
	"description": "",
	"content": "Tuần 1: Làm quen với AWS và các nhóm dịch vụ AWS cơ bản\nTuần 2: Tìm hiểu phạm vi dự án, đọc proposal và tham khảo các hướng tiếp cận dự đoán quỹ đạo bão\nTuần 3: Tiền xử lý dữ liệu bão, xây dựng đặc trưng, tạo mô hình baseline và tìm hiểu nền tảng AWS Lambda\nTuần 4: Tinh chỉnh đặc trưng, chạy thử nghiệm dự đoán nhỏ và học kiến thức cơ bản về Lambda – API Gateway – CloudWatch\nTuần 5: Cải thiện trích xuất đặc trưng, chạy thêm baseline test và tìm hiểu Lambda concurrency cùng các mẫu truy xuất S3\nTuần 6: So sánh các mô hình phù hợp (LSTM/RNN/Transformer), phác thảo kế hoạch mô hình và thực hành AWS cơ bản (S3/CloudWatch/Lambda)\nTuần 7: Xây dựng prototype mô hình, kiểm tra hành vi ban đầu và học Lambda Layers \u0026amp; đóng gói dependencies\nTuần 8: Chuẩn bị cho training: ổn định preprocessing, kiểm tra cấu trúc mô hình và tìm hiểu thiết lập môi trường ML trên AWS\nTuần 9: Chạy training đầy đủ lần đầu, phân tích log và đánh giá dự đoán sau training so với baseline\nTuần 10: Fine-tune mô hình, đánh giá sau fine-tune và hoàn tất phần học AWS phục vụ sẵn sàng triển khai\nTuần 11: Phân tích lỗi, tích hợp mô hình vào API và kiểm thử triển khai trên giao diện web\nTuần 12: Hoàn thiện tài liệu, báo cáo kỹ thuật, slide trình bày, kiểm thử end-to-end và bàn giao dự án\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/1-worklog/1.1-week1/",
	"title": "Worklog Tuần 1",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 1: Khám phá chủ đề dự án và ý tưởng chính cho dự án. Tìm hiểu về các dịch vụ AWS, cách sử dụng AWS Console và CLI. Nhiệm vụ cần thực hiện trong tuần này: Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 2 - Làm quen với các thành viên FCJ\n- Đọc và ghi chú các quy định, nội quy của đơn vị thực tập 08/09/2025 11/09/2025 3 - Nghiên cứu dự án dự đoán đường đi bão bằng cách đọc các bài báo học thuật 10/09/2025 13/09/2025 - Bài học học thuật 1: Tropical Cyclone Track Forecasting Using Fused Deep Learning (S. Giffard-Roisin et al., 2020) (link)\n- Bài học học thuật 2: Forecasting Tropical Cyclone Tracks in the Northwestern Pacific Based on Deep-Learning Model (Wang, L. et al., 2023) (link) 4 - Tìm nguồn dữ liệu cho dự án 14/09/2025 16/09/2025 4 - Tìm hiểu các dịch vụ AWS bằng cách xem YouTube từ đường link trong email đã nhận 15/09/2025 15/09/2025 Kết quả đạt được Tuần 1: Hiểu và làm quen giữa các thành viên trong nhóm.\nThu thập kiến thức nền tảng cho dự án:\nBão nhiệt đới (hurricane/typhoon) là gì và cấu trúc của nó (mắt bão, thành mắt bão, các dải mưa). Điều kiện hình thành trong tự nhiên: nhiệt độ bề mặt biển ấm (~26–27 °C trở lên), độ ẩm, nhiễu động ban đầu, lực Coriolis và độ đứt gió theo phương thẳng đứng thấp. Nguồn năng lượng \u0026amp; vòng đời: giải phóng nhiệt ẩn, tăng cường độ, đạt đỉnh, suy yếu/chuyển pha ngoại nhiệt đới. Hiểu các nhóm dịch vụ AWS:\nCompute — EC2, Lambda, ECS/EKS Storage — S3, EBS, EFS Networking \u0026amp; CDN — VPC, Route 53, CloudFront Các khái niệm chính đã học về dịch vụ AWS:\nRegions, Availability Zones và các dịch vụ toàn cầu. IAM users, roles và nguyên tắc phân quyền tối thiểu (least privilege). Thiết kế VPC, định tuyến và security groups. Phân bổ chi phí bằng tags và budgets. "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/1-worklog/1.2-week2/",
	"title": "Worklog Tuần 2",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 2: Thúc đẩy dự án mô hình hóa bão bằng cách tinh chỉnh cấu trúc dữ liệu và hướng tiếp cận phân tích. Củng cố kiến thức AWS Cloud thông qua thực hành trực tiếp với các dịch vụ cốt lõi. Nhiệm vụ cần thực hiện trong tuần này: Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 1 - Tiếp tục tiến độ dự án Phân tích bão và Mô hình hóa quỹ đạo\n+ Phân tích dữ liệu bão\n+ Mô hình hóa \u0026amp; phân loại quỹ đạo\n+ Phát triển mô hình dự đoán 15/09/2025 21/09/2025 2 - Phối hợp với nhóm Kỹ thuật Phần mềm để tích hợp mô hình vào phần mềm 16/09/2025 18/09/2025 3 - Tiếp tục khám phá các dịch vụ AWS Cloud:\n+ Amazon S3 – học cách tạo bucket và quản lý lưu trữ dữ liệu.\n+ Amazon EC2 – học cách khởi tạo instance, cấu hình security group và kết nối qua SSH. 17/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được Tuần 2: Nắm vững hơn các kiến thức nền tảng về AWS.\nThực hành với dịch vụ Amazon S3 và EC2:\nTạo và quản lý các S3 bucket để lưu trữ dữ liệu bão Khởi tạo EC2 instance và cấu hình security group Tạo và sử dụng key pair để truy cập an toàn vào EC2 Kết nối qua SSH và thiết lập môi trường làm việc Liên kết S3 với EC2 để tích hợp luồng dữ liệu (data workflow) Tiến triển trong Dự án Mô hình hóa bão:\nKhám phá các hướng tiếp cận mô hình khác nhau và cách áp dụng vào dữ liệu dự án Đọc và tổng hợp nghiên cứu học thuật để xác định các công thức/phương pháp liên quan Xác định các chỉ số chính (tốc độ, hướng di chuyển, khoảng cách, vòng đời) và làm rõ cách suy ra từ dữ liệu thô Phối hợp với nhóm Kỹ thuật Phần mềm:\nXác định các điểm tích hợp giữa mô hình và phần mềm Làm rõ trách nhiệm của mỗi bên "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/1-worklog/1.3-week3/",
	"title": "Worklog Tuần 3",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 3: Làm sạch và chuẩn bị dữ liệu quỹ đạo bão để phục vụ mô hình hóa. Tạo các đặc trưng (features) thiết yếu cho dự đoán quỹ đạo. Xây dựng và đánh giá các mô hình dự đoán baseline. Học kiến thức cơ bản về AWS Lambda để phục vụ tích hợp sau này. Thống nhất thiết kế API và luồng dữ liệu với nhóm Kỹ thuật Phần mềm. Nhiệm vụ cần thực hiện trong tuần này: Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 1 - Tiếp tục tiền xử lý bộ dữ liệu bão\n+ Làm sạch các track bão lịch sử\n+ Chuẩn hóa timestamp và loại bỏ các bản ghi không nhất quán 22/09/2025 24/09/2025 - 2 - Feature engineering cho mô hình quỹ đạo bão: tính Δlat, Δlon, tốc độ, hướng di chuyển (heading) 23/09/2025 25/09/2025 - 3 - Phát triển các mô hình dự đoán quỹ đạo baseline\n+ Mô hình Persistence\n+ Benchmark hồi quy đơn giản\n+ Đánh giá MAE \u0026amp; lỗi khoảng cách 24/09/2025 26/09/2025 - 4 - Tìm hiểu AWS Lambda chi tiết\n- Nghiên cứu handler, cơ chế runtime và cold start\n- Tìm hiểu mô hình tích hợp Lambda → API Gateway 25/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Phối hợp với nhóm Kỹ thuật Phần mềm về thiết kế API và luồng dữ liệu 26/09/2025 26/09/2025 - Kết quả đạt được Tuần 3: Hoàn thành tiền xử lý cho bộ dữ liệu bão.\nXây dựng các đặc trưng chính cho quỹ đạo:\nΔlat, Δlon Tốc độ tiến (forward speed) Vector hướng di chuyển (heading vector) Phát triển các mô hình dự đoán baseline.\nĐánh giá hiệu năng bước đầu:\nBenchmark mô hình Persistence MAE và lỗi khoảng cách của mô hình hồi quy đơn giản Hiểu rõ hơn về AWS Lambda và quy trình hoạt động:\nCấu trúc handler Hành vi runtime Tích hợp cơ bản với API Gateway Phối hợp với nhóm Kỹ thuật Phần mềm về cấu trúc API.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/1-worklog/1.4-week4/",
	"title": "Worklog Tuần 4",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 4: Đảm bảo bộ dữ liệu bão nhất quán và sẵn sàng cho việc mô hình hóa. Tinh chỉnh và cải thiện cách tính các đặc trưng (features) quỹ đạo quan trọng. Chạy các thử nghiệm thăm dò nhỏ để quan sát hành vi mô hình ban đầu. Học các thành phần AWS thiết yếu cho tích hợp về sau (Lambda, API Gateway, CloudWatch). Thống nhất với nhóm Kỹ thuật Phần mềm (SE) về định dạng đầu ra dự đoán và các bước tích hợp tiếp theo. Nhiệm vụ cần thực hiện trong tuần này: Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 1 - Rà soát bộ dữ liệu bão để đảm bảo tính nhất quán 29/09/2025 30/09/2025 - 2 - Tiếp tục tinh chỉnh trích xuất đặc trưng\n+ Điều chỉnh cách tính Δlat/Δlon\n+ Kiểm tra lại định dạng tốc độ di chuyển 30/09/2025 01/10/2025 - 3 - Thực hiện các thử nghiệm dự đoán thăm dò quy mô nhỏ 01/10/2025 02/10/2025 - 4 - Nghiên cứu tài liệu AWS liên quan đến dự án\n+ Luồng thực thi Lambda\n+ Kiến thức cơ bản về trigger từ API Gateway\n+ Cách dùng CloudWatch log 02/10/2025 02/10/2025 AWS Docs 5 - Trao đổi với nhóm SE về định dạng đầu ra dự đoán và các bước tiếp theo 03/10/2025 03/10/2025 - Kết quả đạt được Tuần 4: Rà soát và xác thực bộ dữ liệu bão để đảm bảo đầu vào ổn định cho giai đoạn mô hình hóa sau. Tinh chỉnh một số đặc trưng quỹ đạo quan trọng sau khi kiểm tra lại thay đổi tọa độ. Thực hiện các thử nghiệm dự đoán thăm dò nhỏ để hiểu hành vi của dữ liệu. Nghiên cứu tài liệu AWS liên quan đến dự án: Luồng thực thi Lambda Cách API Gateway kết nối với Lambda Cách sử dụng CloudWatch log cơ bản Thảo luận cấu trúc API và các bước tích hợp trong tương lai với nhóm Kỹ thuật Phần mềm. "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/1-worklog/1.5-week5/",
	"title": "Worklog Tuần 5",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 5: Cải thiện và tinh chỉnh logic trích xuất đặc trưng (feature extraction) hiện có cho dữ liệu bão. Thực hiện thêm các bài test baseline để quan sát cách thay đổi đầu vào ảnh hưởng đến độ chính xác dự đoán. Củng cố kiến thức nền tảng AWS, tập trung vào tương tác giữa Lambda, API Gateway và S3. Xây dựng cấu trúc JSON ban đầu cho đầu ra dự đoán của mô hình. Đồng bộ với nhóm để cập nhật tiến độ và làm rõ các khó khăn hiện tại. Nhiệm vụ cần thực hiện trong tuần này: Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 1 - Rà soát và điều chỉnh một số phần trong logic trích xuất đặc trưng 06/10/2025 07/10/2025 - 2 - Thực hiện thêm các bài test dự đoán baseline quy mô nhỏ\n+ Thử thay đổi nhẹ đầu vào\n+ Quan sát sự thay đổi của các chỉ số lỗi cơ bản 07/10/2025 08/10/2025 - 3 - Tiếp tục đọc tài liệu AWS\n+ Kiến thức cơ bản về Lambda concurrency\n+ Luồng xử lý request của API Gateway\n+ Các pattern truy xuất object từ S3 08/10/2025 08/10/2025 AWS Docs 4 - Phác thảo cấu trúc JSON đơn giản cho output dự đoán 09/10/2025 09/10/2025 - 5 - Tham gia buổi team sync để chia sẻ cập nhật và làm rõ các vấn đề/khó khăn hiện tại 10/10/2025 10/10/2025 - Kết quả đạt được Tuần 5: Cải thiện chất lượng tổng thể của các đặc trưng bão đã chuẩn bị.\nThực hiện một vài thử nghiệm nhẹ để quan sát tác động của việc thay đổi nhỏ ở đầu vào lên hành vi dự đoán:\nSo sánh các lần chạy thử ngắn Ghi nhận các xu hướng từ giá trị lỗi đã cập nhật Mở rộng hiểu biết về AWS thông qua việc đọc thêm tài liệu:\nCách Lambda xử lý các lần thực thi đồng thời (concurrent executions) Luồng request điển hình trong API Gateway Các cách tiếp cận cơ bản để truy xuất object từ S3 Phác thảo ý tưởng ban đầu về cách tổ chức cấu trúc đầu ra (output) của mô hình.\nTham gia thảo luận trong nhóm để thống nhất kỳ vọng và chia sẻ cập nhật tiến độ.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/1-worklog/1.6-week6/",
	"title": "Worklog Tuần 6",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 6: Xác định và so sánh các lựa chọn mô hình phù hợp cho bài toán dự đoán quỹ đạo bão. Khám phá và phân tích điểm mạnh/điểm yếu của các hướng tiếp cận dựa trên LSTM, RNN và Transformer. Phác thảo kế hoạch mô hình ban đầu, bao gồm đầu vào, cấu trúc và luồng huấn luyện/kiểm thử đơn giản trên môi trường local. Tiếp tục làm quen AWS thông qua các thử nghiệm nhỏ với S3, CloudWatch và Lambda. Chạy các thử nghiệm sớm để hiểu hành vi mô hình ban đầu và kiểm chứng logic cơ bản. Nhiệm vụ cần thực hiện trong tuần này: Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 1 - Rà soát các ứng viên mô hình phù hợp cho dự đoán quỹ đạo 13/10/2025 13/10/2025 - 2 - Tiếp tục khám phá các hướng tiếp cận mô hình\n+ So sánh LSTM, RNN đơn giản và một ý tưởng Transformer nhẹ\n+ Ghi chú ưu/nhược điểm của từng phương pháp 14/10/2025 16/10/2025 - 3 - Phác thảo kế hoạch ban đầu cho cấu trúc mô hình\n+ Xác định yêu cầu đầu vào\n+ Phác thảo luồng huấn luyện/kiểm thử nhỏ (chạy local) 16/10/2025 17/10/2025 - 4 - Nghiên cứu tài liệu AWS liên quan đến workflow dự án\n+ Rà soát cách tổ chức thư mục (folder) cơ bản trên S3\n+ Quan sát ví dụ log trên CloudWatch\n+ Thử một bài test Lambda nhỏ 17/10/2025 18/10/2025 AWS Docs 5 - Thực hiện các lần chạy thử nhỏ để kiểm tra logic mô hình ở giai đoạn đầu 18/10/2025 18/10/2025 - Kết quả đạt được Tuần 6: Hiểu rõ hơn hướng mô hình nào là khả thi và thực tế nhất cho dự án.\nDành thời gian so sánh nhiều ý tưởng mô hình:\nCác biến thể của LSTM và RNN Một khái niệm Transformer đơn giản Ghi chú về điểm mạnh và điểm yếu Phác thảo kế hoạch mô hình ban đầu để định hướng các bước làm tiếp theo.\nTiếp tục học các kiến thức AWS cơ bản phục vụ dự án:\nCác pattern tổ chức dữ liệu trên S3 Dùng CloudWatch để kiểm tra log đơn giản Thử nghiệm Lambda quy mô nhỏ Chạy một vài thử nghiệm mô hình nhẹ để quan sát hành vi của logic ở giai đoạn sơ khởi.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/1-worklog/1.7-week7/",
	"title": "Worklog Tuần 7",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 7: Xác thực bộ dữ liệu bão và đảm bảo sẵn sàng cho việc xây dựng mô hình prototype. Bắt đầu xây dựng một mô hình prototype đơn giản để kiểm tra logic cốt lõi và luồng dữ liệu. Củng cố hiểu biết về các thành phần AWS phục vụ triển khai trong tương lai, đặc biệt là Lambda Layers và đóng gói dependency. Chạy các bài test prototype sớm để quan sát hành vi mô hình ban đầu. Ghi lại các phát hiện và xác định các điểm cần tinh chỉnh cho các vòng lặp tiếp theo. Nhiệm vụ cần thực hiện trong tuần này: Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 1 - Rà soát và kiểm tra bộ dữ liệu đầu vào (storm input dataset) để phục vụ mô hình hóa 20/10/2025 21/10/2025 - 2 - Bắt đầu xây dựng mô hình prototype đơn giản\n+ Thiết lập cấu trúc mô hình cơ bản\n+ Chuẩn bị input tensors\n+ Viết forward logic ban đầu 21/10/2025 23/10/2025 - 3 - Tiếp tục nghiên cứu tài liệu AWS\n+ Kiến thức cơ bản về Lambda Layers\n+ Đóng gói Python dependencies\n+ Giới thiệu về CloudWatch logs 23/10/2025 23/10/2025 AWS Docs 4 - Chạy các test case nhỏ trên prototype để quan sát hành vi ban đầu 24/10/2025 24/10/2025 - 5 - Tổng kết tiến độ và ghi lại các điểm cần tinh chỉnh cho mô hình 25/10/2025 25/10/2025 - Kết quả đạt được Tuần 7: Hiểu rõ hơn về bộ dữ liệu sau khi rà soát các đầu vào cho mô hình.\nCó tiến triển trong mô hình prototype:\nThiết lập cấu trúc ban đầu Kiểm tra forward logic cơ bản Xác định các điểm cần tinh chỉnh Nâng cao kiến thức về các thành phần AWS liên quan đến triển khai mô hình:\nHiểu về Lambda Layers Xử lý cơ bản các Python dependencies Làm quen với luồng logging của CloudWatch Quan sát hành vi của prototype thông qua các test case đơn giản.\nGhi lại các phát hiện hiện tại và phác thảo những nội dung cần cải thiện ở các vòng lặp tiếp theo.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/1-worklog/1.8-week8/",
	"title": "Worklog Tuần 8",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 8: Hoàn thiện các bước tiền xử lý để đảm bảo bộ dữ liệu sẵn sàng hoàn toàn cho huấn luyện. Rà soát và tinh chỉnh cấu trúc mô hình prototype nhằm chuẩn bị cho giai đoạn training sắp tới. Củng cố hiểu biết về thiết lập môi trường AWS, bao gồm đóng gói dependency và các luồng logging. Xác thực độ ổn định của mô hình thông qua các bài test prototype có kiểm soát trước khi huấn luyện. Ghi lại các vấn đề còn tồn đọng và phác thảo các bước cuối cùng cần hoàn tất để bắt đầu training ban đầu. Nhiệm vụ cần thực hiện trong tuần này: Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 1 - Xem lại các bước tiền xử lý để đảm bảo dữ liệu đã làm sạch sẵn sàng đưa vào pipeline huấn luyện\n+ Xác minh chuẩn hóa tọa độ nhất quán giữa các mẫu\n+ Kiểm tra các giá trị bị thiếu hoặc không nhất quán 27/10/2025 28/10/2025 - 2 - Rà soát cấu trúc mô hình prototype để đảm bảo có thể tiến tới phiên bản trainable\n+ Kiểm tra tensor shapes ở từng layer\n+ Ghi chú các phần cần logic rõ ràng hơn trước khi training\n+ Đảm bảo forward pass ổn định 28/10/2025 30/10/2025 - 3 - Nghiên cứu tài liệu AWS về chuẩn bị môi trường cho workload ML\n+ Tìm hiểu về đóng gói dependency\n+ Xem cách theo dõi log training trong tương lai qua CloudWatch 30/10/2025 30/10/2025 AWS Docs 4 - Chạy nhiều bài test prototype có kiểm soát để xác nhận mô hình hoạt động nhất quán với tiền xử lý đã cập nhật\n- So sánh output trước và sau các điều chỉnh để kiểm tra mức độ sẵn sàng cho lần training đầu tiên 31/10/2025 31/10/2025 - 5 - Tổng hợp toàn bộ phát hiện, liệt kê các vấn đề còn lại và phác thảo những việc bắt buộc phải hoàn tất trước khi bắt đầu các bước training ban đầu 01/11/2025 01/11/2025 - Kết quả đạt được Tuần 8: Cải thiện pipeline tiền xử lý để đảm bảo bộ dữ liệu ổn định và phù hợp cho các bước huấn luyện sắp tới.\nHiểu rõ hơn hành vi của mô hình sau nhiều lần tinh chỉnh và xác nhận forward pass hiện đã ổn định hơn.\nChuẩn bị tốt hơn cho giai đoạn training:\nXác định những phần của mô hình cần cấu trúc/logic rõ ràng hơn Kiểm tra toàn bộ kích thước tensor bên trong mô hình Đảm bảo không còn mismatch shape ngoài ý muốn Hiểu thêm về các yêu cầu AWS cho môi trường ML, đặc biệt là chuẩn bị dependency và logging.\nTổng hợp danh sách các hạng mục cần hoàn thành trước khi bắt đầu giai đoạn training đầu tiên của mô hình.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/1-worklog/1.9-week9/",
	"title": "Worklog Tuần 9",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 9: Hoàn thiện và xác minh bộ dữ liệu huấn luyện để đảm bảo chất lượng đầu vào nhất quán. Chạy chu kỳ huấn luyện đầy đủ đầu tiên và quan sát hành vi học ban đầu của mô hình. Phân tích training logs để phát hiện sớm các vấn đề và các dấu hiệu không ổn định. Đánh giá dự đoán của mô hình sau huấn luyện so với hiệu năng baseline. Xác định các mảng chính cần fine-tuning cho giai đoạn cải tiến tiếp theo. Nhiệm vụ cần thực hiện trong tuần này: Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 1 - Chuẩn bị bộ dữ liệu huấn luyện cuối cùng và kiểm tra tính nhất quán về định dạng\n+ Kiểm tra lại các khoảng giá trị normalization\n+ Đảm bảo không thiếu các đoạn (segments) dữ liệu 04/11/2025 05/11/2025 - 2 - Bắt đầu chạy huấn luyện đầy đủ cho mô hình prototype\n+ Chạy nhiều epoch để quan sát hội tụ ban đầu\n+ Ghi log hành vi sớm để phân tích\n+ Ghi nhận các pattern loss không ổn định 05/11/2025 07/11/2025 - 3 - Xử lý training logs và rà soát diễn tiến loss để xác định các vấn đề cần điều chỉnh thêm 06/11/2025 06/11/2025 - 4 - Thực hiện các bài test ổn định sau huấn luyện\n- So sánh dự đoán sau huấn luyện với đầu ra baseline 08/11/2025 09/11/2025 - 5 - Tổng kết kết quả huấn luyện ban đầu và phác thảo các nội dung cần tối ưu trong giai đoạn fine-tuning tiếp theo 09/11/2025 10/11/2025 - Kết quả đạt được Tuần 9: Hoàn thành lần chạy huấn luyện đầy đủ đầu tiên của mô hình và quan sát hành vi của đường cong loss qua nhiều epoch.\nXác định một số pattern từ training logs để định hướng fine-tuning ở giai đoạn tiếp theo:\nLoss dao động mạnh ở giai đoạn đầu Các đoạn mà dự đoán bị lệch (diverge) Ghi chú về các phần cần tinh chỉnh cấu trúc/logic Hiểu rõ hơn cách mô hình đã huấn luyện phản ứng với các mẫu đầu vào khác nhau và mức độ chênh lệch hiện tại so với hành vi kỳ vọng trong thực tế.\nSo sánh dự đoán sau huấn luyện với baseline, ghi nhận các điểm cải thiện rõ rệt và các trường hợp mô hình vẫn gặp khó khăn.\nChuẩn bị danh sách các điểm cần fine-tuning cho giai đoạn tiếp theo:\nĐiều chỉnh lại logic bên trong mô hình Đánh giá lại tác động của các bước tiền xử lý Tinh chỉnh kiến trúc ở mức nhỏ cho vòng lặp kế tiếp "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/2-proposal/",
	"title": "Bản đề xuất",
	"tags": [],
	"description": "",
	"content": "NỀN TẢNG TRỰC TUYẾN ĐỂ THEO DÕI VÀ DỰ BÁO QUỸ ĐẠO BÃO Kỹ thuật tự nhận thức trắc địa (Geodesic-Aware Deep Learning) cho dự đoán hướng di chuyển chính xác: Một phương pháp tiếp cận dựa trên kêt hợp \u0026ldquo;mô phỏng vật lý\u0026rdquo; và \u0026ldquo;tăng cường dữ liệu\u0026rdquo; 1. Tóm tắt điều hành Dữ liệu chuỗi thời gian đóng vai trò là một trong những dạng biểu diễn thông tin quan trọng nhất trong các ứng dụng khoa học và công nghiệp hiện đại. Nó cần thiết để hiểu các quá trình động như xu hướng kinh tế, mô hình tiêu thụ năng lượng và sự thay đổi khí tượng theo thời gian. Đặc biệt, dự báo thời tiết phụ thuộc mạnh mẽ vào dữ liệu chuỗi thời gian để dự đoán điều kiện khí quyển trong tương lai, quỹ đạo bão và các biến động theo mùa dựa trên dữ liệu lịch sử.\nVới sự phát triển nhanh chóng trong nghiên cứu về học sâu và mạng nơ-ron, dự án của chúng tôi hướng đến việc phát triển một mô hình dự báo tiên tiến có khả năng dự đoán chính xác đường đi, cường độ và tổng quãng đường di chuyển của các cơn bão trong vài ngày tiếp theo. Các dự đoán này có thể hỗ trợ hệ thống cảnh báo sớm, giúp chính quyền và cư dân tại các khu vực bị ảnh hưởng có thể đưa ra biện pháp phòng tránh trước khi bão đổ bộ.\nĐể vượt qua những hạn chế của công nghệ và nghiên cứu hiện tại, nghiên cứu này giới thiệu một số kỹ thuật và thuật toán mới, bao gồm hai phương pháp tăng cường dữ liệu mới cho chuỗi thời gian trắc địa và một cơ chế mã hóa không gian được thiết kế để nâng cao hiệu suất dự đoán của điện toán tích chập. Hệ thống cuối cùng sẽ được tích hợp vào kiến trúc đám mây không máy chủ trên AWS, đảm bảo khả năng mở rộng, tính khả dụng cao và hiệu quả chi phí cho việc theo dõi và phân tích bão theo thời gian thực.\nKiến trúc hệ thống tận dụng một số dịch vụ AWS để tạo thành một quy trình xử lý và triển khai dữ liệu được quản lý hoàn toàn. Các hàm AWS Lambda đóng vai trò là là xương sống cho hoạt động tính toán không máy chủ, được kích hoạt tự động bởi Amazon EventBridge để thu thập và xử lý dữ liệu bão mới từ các nguồn khí tượng mở theo lịch trình. Dữ liệu đã xử lý được lưu trữ an toàn trên Amazon S3, trong khi AWS CodePipeline và CodeBuild tự động hóa việc tích hợp và triển khai liên tục các phiên bản mô hình mới. Mô hình sau khi huấn luyện được lưu trữ và cung cấp qua Amazon API Gateway, cho phép nền tảng dự báo trực tuyến thực hiện các yêu cầu suy luận nhẹ, theo thời gian thực. Mọi hoạt động của hệ thống được giám sát thông qua Amazon CloudWatch, cung cấp khả năng quan sát hoạt động, phát hiện lỗi và các số liệu hiệu suất.\nPhương pháp đầu tiên được đề xuất, Khung Tăng cường Dữ liệu Phai mờ Theo Thời Gian Theo Từng Bước (Stepwise Temporal Fading Augmentation - STFA), là một khuôn khổ tăng cường dữ liệu chuỗi thời gian mới, mô phỏng sự suy giảm tự nhiên của ảnh hưởng từ các quan sát trong quá khứ. Khác với các phương pháp tiếp cận truyền thống dựa trên việc gây nhiễu ngẫu nhiên hoặc bơm nhiễu, STFA áp dụng các trọng số phai mờ cho các bước thời gian trước đó trong khi vẫn bảo toàn thông tin gần đây. Quá trình này tạo ra các chuỗi tổng hợp đa dạng và chân thực, từ đó cải thiện độ mạnh mẽ và khả năng khái quát hóa của mô hình. Kỹ thuật này sẽ được đánh giá trên các tác vụ dự báo quỹ đạo bão, vốn dựa trên dữ liệu vĩ độ-kinh độ tuần tự.\nPhương pháp thứ hai, Tăng cường Hướng đi Trắc địa Hợp lý (Plausible Geodesic Bearing Augmentation - PGBA), giới thiệu một chiến lược tăng cường dựa trên phạm vi khả thi của hướng di chuyển và khoảng cách của bão. Bằng cách phân tích hướng đi trắc địa (geodesic bearing) và khoảng cách giữa các vị trí bão liên tiếp, PGBA xác định một biên giới chuyển động thực tế, trong đó các quỹ đạo tổng hợp mới được tạo ra. Cách tiếp cận này nâng cao khả năng của mô hình trong việc nắm bắt sự biến đổi không gian tự nhiên và độ bất định về hướng trong chuyển động của bão.\nNgoài ra, nghiên cứu này còn khám phá một biểu diễn không gian-thời gian (spatial-temporal representation) của dữ liệu chuỗi thời gian, cho phép áp dụng các mạng nơ-ron tích chập (Convolutional Neural Networks - CNNs) để nắm bắt cả sự phụ thuộc về không gian và thời gian. Biểu diễn này tận dụng thế mạnh của điện toán tích chập để mô hình hóa các tương tác cục bộ trên không gian và thời gian. Nó sẽ đóng vai trò là cơ sở (baseline) để so sánh với các mô hình Mạng Tích chập Thời gian (Temporal Convolutional Network - TCN) được huấn luyện bằng các phương pháp tăng cường dữ liệu đã được đề xuất.\nCác mạng nơ-ron truyền thống, dù được sử dụng cho mô hình hóa dữ liệu chuỗi thời gian hay dữ liệu ảnh, chủ yếu chỉ học các mẫu thống kê từ dữ liệu. Tuy nhiên, trong nhiều hệ thống vật lý thực tế, các mô hình hoàn toàn dựa trên dữ liệu như vậy có thể không tuân thủ các ràng buộc tự nhiên, chẳng hạn như các mối quan hệ trọng lực hoặc trắc địa. Để khắc phục hạn chế này, chúng tôi kết hợp các nguyên lý của Học máy Thông tin Vật lý (Physics-Informed Machine Learning - PIML) vào phương pháp tiếp cận của mình.khoảng cách trắc địa và phương vị được lấy từ dữ liệu vĩ độ-kinh độ và được tích hợp vào quá trình huấn luyện của mô hình như như những đặc trưng mang ý nghĩa vật lý. Hơn nữa, chúng tôi sử dụng công thức Haversine — công thức tính khoảng cách hình cầu giữa hai điểm - làm một số hạng mất mát phụ, bổ sung cho các số liệu sai số chuẩn như MSE, RMSE, MAE và MAPE.\nBằng cách kết hợp các phương pháp tăng cường dữ liệu được đề xuất, các nguyên tắc học máy thông tin vật lý (PIML) và một cơ sở hạ tầng học sâu không máy chủ được vận hành bởi các dịch vụ AWS, nghiên cứu này hướng đến mục tiêu phát triển một khuôn khổ dự báo quỹ đạo bão có khả năng mở rộng, mạnh mẽ và chính xác. Hệ thống thu được không chỉ nâng cao trình độ mô hình hóa chuỗi thời gian trắc địa mà còn chứng minh tính thực tiễn của việc triển khai các hệ thống dự báo môi trường dựa trên AI như các ứng dụng bản địa đám mây (cloud-native) bền bỉ, giúp tăng cường khả năng chuẩn bị và đảm bảo an toàn cho các khu vực dễ bị ảnh hưởng bởi bão.\n2. Tuyên bố vấn đề Vấn đề hiện tại? Để phát triển một nền tảng đáng tin cậy nhằm theo dõi và đưa ra cảnh báo về đường đi của bão trong tương lai, việc xây dựng một mô hình máy học vừa chính xác vừa có khả năng đưa ra các dự đoán đáng tin cậy là hết sức quan trọng. Thành phần theo dõi có thể được giải quyết bằng cách liên tục thu thập và cập nhật dữ liệu từ các nguồn khí tượng công cộng. Tuy nhiên, các tập dữ liệu này thường bị giới hạn về mặt địa lý và chứa nhiều thông tin trùng lặp hoặc không đầy đủ.\nNgược lại, thành phần dự báo lại có độ phức tạp cao hơn. Việc đạt được dự báo chuỗi thời gian chính xác trong bối cảnh này thường phải đối mặt với hai thách thức chính: (1) tính đa dạng và phạm vi bao phủ của dữ liệu có sẵn còn hạn chế, và (2) sự thiếu vắng các nguyên tắc vật lý nền tảng, điều này làm hạn chế khả năng của mô hình trong việc phản ánh các động lực địa vật lý cơ bản của hành vi bão.\nThiếu dữ liệu: Nhiều tác vụ dự báo chuỗi thời gian gặp hạn chế về lượng dữ liệu huấn luyện. Mặc dù có nhiều phương pháp tăng cường dữ liệu, nhưng rất ít phương pháp tập trung trực tiếp vào sự suy giảm tầm quan trọng của các giá trị trong quá khứ theo thời gian.\nBỏ qua yếu tố vật lý:: Hầu hết các mạng nơ-ron chỉ học từ dữ liệu thô mà không xét đến các ràng buộc vật lý trong thế giới thực. Trong các tác vụ dự đoán quỹ đạo (ví dụ: bão), điều này thường dẫn đến những kết quả phi thực tế.\nMục tiêu của nhóm :\nPhát triển một phương pháp tăng cường chuỗi thời gian mới (STFA) nhằm cải thiện độ bền vững và khả năng khái quát của mô hình. Tích hợp các ràng buộc dựa trên vật lý vào quá trình huấn luyện mô hình, thu hẹp khoảng cách giữa học máy dựa trên dữ liệu và động lực học của thế giới thực. Kiểm tra sức mạnh của mạng tích chập (convolution2D) trong việc dự báo quỹ đạo. Xây dựng một nền tảng trực tuyến cung cấp thông tin mới nhất về các cơn bão hiện tại và các dự đoán chính xác về quỹ đạo của chúng. Giải Pháp A - Tăng cường cường dữ liệu bằng Stepwise Temporal Fading (STFA) STFA biến đổi các chuỗi thời gian bằng cách giảm dần ảnh hưởng của các giá trị trong quá khứ. Khác với phương pháp thêm nhiễu ngẫu nhiên, nó áp dụng một cách có hệ thống các hệ số phai mờ theo từng bước vào các nhóm dữ liệu cũ.\nCho một chuỗi đơn biến là:\n$$ X = [x_0, x_1, \\ldots, x_{T-1}] $$\ntrong đó $T$ là độ dài của chuỗi $X$.\nCác tham số:\n$n$: khoảng giá trị gần nhất muốn giữ nguyên $S$: số dải được áp dụng phai dần $L = T - n$: độ dài của vùng phai dần $k = \\frac{L}{S}$: số giá trị trong mỗi dải $I_b$: tập chỉ số của dải thứ $b$ \\[ I_b = {, i \\mid L - b \\cdot k ;\\leq; i ;\\leq; L - (b-1)\\cdot k - 1 ,} \\]\nBiến đổi::\nKý hiệu chuỗi dữ liệu đã được tăng cường là:\n$$ X = [x_0, \\ldots, x_{T-1}] $$\nvới các quy tắc biến đổi sau:\n$$ x_t = \\begin{cases} x_t, \u0026amp; t \\in {T-n, \\ldots, T-1}, \\\\ m_b , x_t, \u0026amp; t \\in I_b, \\\\ m_{S+1} , x_t, \u0026amp; t \u0026lt; \\min(I_S), \\end{cases} $$\ntrong đó các hệ số nhân $m_b \\in (0,1)$ giảm dần đều từ các nhóm dữ liệu gần nhất tới các dữ liệu cũ hơn.\nCông thức này duy trì độ thực tế, chính xác của các dữ liệu gần đây trong khi kiểm soát chặt chẽ hơn ảnh hưởng của các giá trị xa trong chuỗi. Việc tăng cường này buộc mô hình tập trung vào các mẫu hình mạnh mẽ hơn thay vì chỉ phụ thuộc vào dữ liệu thô, đồng thời gia tăng tính đa dạng của dữ liệu theo các tham số được chọn.\nB - Tăng Cường theo Hướng Di Chuyển Hợp Lý (PGBA - Plausible Geodesic Bearing Augmentation) Kỹ thuật Tăng cường Hướng Di chuyển (dựa trên) Trắc địa Hợp lý (PGBA) nâng cao tính chân thực và khả năng kiểm soát của việc tạo ra quỹ đạo trong các tác vụ dự báo chuỗi thời gian không gian địa lý. Khác với các phương pháp nhiễu loạn ngẫu nhiên thông thường, PGBA đưa vào yếu tố ngẫu nhiên nhưng vẫn đảm bảo sự hợp lý trong các ràng buộc vật lý của chuyển động cơ bản. Các quỹ đạo được tạo ra bắt nguồn từ các mối quan hệ hình học của các quan sát trong quá khứ thay vì từ các bước hoàn toàn ngẫu nhiên, dẫn đến các đường đi mượt mà hơn và tính biến thiên có ý nghĩa trong tập dữ liệu huấn luyện. Kỹ thuật này được áp dụng cho mỗi bốn vị trí trong một chuỗi dữ liệu.\nPGBA đóng vai trò là phương pháp tăng cường bổ trợ cho STFA, làm phong phú thêm tính đa dạng của các mẫu huấn luyện trong khi vẫn bảo toàn cấu trúc động học ban đầu. Mục tiêu của nó là tạo ra các quỹ đạo dư thừa nhưng vẫn đảm bảo tính nhất quán vật lý, từ đó nắm bắt được các biến thể tiềm năng trong chuyển động của bão hoặc các hiện tượng không gian địa lý tương tự.\nCơ chế cốt lõi\nXét một quỹ đạo bão được biểu diễn bằng một chuỗi $n$ các vị trí địa lý theo thứ tự:\n$$ P = [P_1, P_2, \\ldots, P_n] $$\nChúng tôi chia chuỗi này thành các khối nhỏ gồm 4 điểm, với $P_i$ là điểm bắt đầu của mỗi khối, được xác định bởi vĩ độ và kinh độ:\n$$ P_i = (\\phi_i, \\lambda_i) $$\nỞ đây, $\\phi_i$ và $\\lambda_i$ lần lượt biểu thị cho vĩ độ và kinh độ theo đơn vị radian.\nKhoảng cách trắc địa $d_i$ và góc phương vị $\\theta_i$ giữa hai điểm liên tiếp $P_i$ và $P_{i+1}$ được xác định như sau:\n$$ d_i = \\text{Distance}(P_i, P_{i+1}), \\qquad \\theta_i = \\text{Bearing}(P_i, P_{i+1}) $$\nĐể tạo ra biến đổi hợp lý, PGBA làm nhiễu góc phương vị bằng cách thêm một nhiễu ngẫu nhiên nhỏ có phân phối đều $\\epsilon_i$:\n$$ \\theta_i^{\\text{aug}} = \\theta_i + \\epsilon_i, \\qquad \\epsilon_i \\sim \\text{Uniform}(-\\delta, \\delta) $$\ntrong đó $\\delta$ là giới hạn góc có thể điều chỉnh để kiểm soát phạm vi lệch.\nHai điểm đầu tiên luôn được giữ nguyên và được sử dụng để tính toán khoảng cách và hướng di chuyển. Điểm tiếp theo được tăng cường sau đó được tính toán bằng công thức định vị trắc địa, giữ nguyên khoảng cách trong khi cho phép hướng di chuyển thay đổi trong phạm vi nhiễu ngẫu nhiên:\n$$ P_{i+2}^{\\text{aug}} = \\text{Destination}(P_i, d_i, \\theta_i^{\\text{aug}}) $$\nQuá trình này bảo toàn khoảng cách giữa các điểm $d_i$ trong khi làm lệch hướng một cách nhẹ để tạo ra các độ lệch hợp lý về mặt vật lý.\nLàm mượt và Hiệu chỉnh Đa Bước\nĐể nâng cao độ mượt và tạo ra các quỹ đạo cong tự nhiên, PGBA áp dụng một hiệu chỉnh tại mỗi điểm thứ tư. Gọi $P_{i+3}^{\\text{aug}}$ là điểm thứ tư. Nó được tính toán lại sao cho góc phương vị $\\theta_{i+3}^{\\text{corr}}$ của nó tối thiểu hóa độ lệch so với điểm gốc $P_{i+3}$:\n$$ \\theta_{i+3}^{\\text{corr}} = \\arg\\min_{\\theta}; \\text{Distance}\\Big( \\text{Destination}(P_{i+2}^{\\text{aug}}, d_{i+2}, \\theta),; P_{i+3} \\Big) $$\nSau đó, điểm tăng cường đã hiệu chỉnh được xác định như sau:\n$$ P_{i+3}^{\\text{aug}} = \\text{Destination}(P_{i+2}^{\\text{aug}}, d_{i+2}, \\theta_{i+3}^{\\text{corr}}) $$\nBước này đảm bảo sự chuyển tiếp mượt mà giữa nhiều điểm trong khi vẫn duy trì tính hợp lý về mặt vật lý.\nLưu ý rằng hai điểm đầu tiên và điểm cuối cùng trong mỗi chuỗi thời gian trắc địa luôn được giữ nguyên không thay đổi.\nC - Học máy (Machine Lerning) dựa trên vật lý Các mô hình mạng nơ-ron như RNNs, CNNs, và Transformers không cần công thức hay quy tắc đặc thù cho từng tác vụ để đạt hiệu suất tốt, miễn là chúng được huấn luyện với đủ dữ liệu. Ví dụ, trong tác vụ dịch máy như dịch từ tiếng Đức sang tiếng Anh bằng RNN, không có quy tắc ngữ pháp nào được cung cấp trong quá trình huấn luyện (không tính stemming, letmat, pos tagging, \u0026hellip; chỉ nói đến quá trình huấn luyện). Tuy nhiên, mô hình vẫn có thể tạo ra bản dịch mạch lạc, thể hiện một trong những điểm mạnh chính của học sâu: khả năng học trực tiếp các mẫu phức tạp từ dữ liệu. Ngược lại, các phương pháp truyền thống — chẳng hạn như các hệ thống dịch dựa trên quy tắc (ví dụ: Google Translate trước những năm 2000) — phụ thuộc nhiều vào quy tắc ngữ pháp và từ điển. Mặc dù chính xác, nhưng các hệ thống này thường thiếu linh hoạt và thất bại khi gặp từ có nhiều nghĩa hoặc các cấu trúc phụ thuộc vào ngữ cảnh.\nLấy cảm hứng từ sự khác biệt đó, mục tiêu của chúng tôi là kết hợp sức mạnh của học sâu với các công thức do con người định nghĩa để đạt hiệu suất tốt hơn. Cụ thể trong lĩnh vực địa lý này, chúng tôi sẽ thử đưa các lợi ích từ công thức Haversine vào quá trình huấn luyện để tính toán khoảng cách và góc phương vị giữa hai vị trí trên một hình cầu. Những yếu tố này cung cấp cho mô hình một cấu trúc bổ sung và độ lệch quy nạp, định hướng việc học vượt ra ngoài các tương quan thuần túy thống kê.\nCông thức Haversine\nĐối với việc tính toán khoảng cách\nCông thức Haversine được sử dụng để tính khoảng cách cung lớn giữa hai điểm trên bề mặt hình cầu — tức là đường đi ngắn nhất trên bề mặt Trái Đất.\n$$ d = 2r , \\arcsin!\\left( \\sqrt{ \\sin^2!\\left(\\frac{\\Delta \\varphi}{2}\\right) + \\cos(\\varphi_1)\\cos(\\varphi_2) \\sin^2!\\left(\\frac{\\Delta \\lambda}{2}\\right) } \\right) $$\nTrong đó:\n$$(\\varphi_1, \\lambda_1)$$ và $$(\\varphi_2, \\lambda_2)$$ là vĩ độ và kinh độ của hai điểm (tính bằng raidian). $\\Delta \\varphi = \\varphi_2 - \\varphi_1$ $\\Delta \\lambda = \\lambda_2 - \\lambda_1$ $r$ là bán kính Trái Đất (≈ 6,371 km). Trong khuôn khổ của chúng tôi, thay vì chỉ dựa vào các hàm mất mát tiêu chuẩn như MSE, RMSE hay MAPE, chúng tôi đề xuất sử dụng công thức Haversine để tính khoảng cách làm hàm mất mát chính. Khi mô hình xuất ra tọa độ vĩ độ và kinh độ cho vị trí bão tiếp theo, công thức Haversine trực tiếp đo lường khoảng cách giữa điểm dự đoán và điểm thực tế. Khoảng cách gần 0 cho thấy dự đoán có độ chính xác cao, trong khi khoảng cách lớn báo hiệu một sai số đáng kể.\nĐối với việc tính toán góc phương vị (bearing)\nCông thức tính góc phương vị được suy ra từ Công thức Haversine, cho biết hướng đi từ một điểm địa lý này đến một điểm khác dọc theo đường tròn lớn:\n$$\\theta = \\text{atan2}!\\left(\\sin(\\Delta \\lambda)\\cos(\\varphi_2),, \\cos(\\varphi_1)\\sin(\\varphi_2) - \\sin(\\varphi_1)\\cos(\\varphi_2)\\cos(\\Delta \\lambda)\\right)$$\nTrong đó:\n$(\\varphi_1, \\lambda_1)$ là điểm xuất phát. $(\\varphi_2, \\lambda_2)$ là điểm kết thúc. $\\Delta \\lambda$ là hiệu số kinh độ. Kết quả $\\theta$ đại diện cho góc phương vị ban đầu (azimuth) được đo theo chiều kim đồng hồ từ hướng Bắc thực.\nTrong quá trình triển khai, chúng tôi tận dụng triệt để việc sử dụng Công thức Haversine để tính toán hai đặc trưng bổ sung — \u0026ldquo;khoảng cách\u0026rdquo; và \u0026ldquo;góc phương vị\u0026rdquo; — được thêm vào tập dữ liệu. Các đặc trưng này cung cấp cho mô hình thông tin phong phú hơn về quỹ đạo bão, đồng thời vẫn duy trì mục tiêu cốt lõi là dự đoán vị trí địa lý tiếp theo.\nD - Tổng quan ngắn về những hiểu lầm phổ biến trong các phương pháp tiếp cận mô hình hóa chuỗi Trong lĩnh vực mô hình hóa chuỗi thuộc học sâu, các kiến trúc dạng hồi quy như RNN, LSTM và GRU thường được coi là giải pháp mặc định. Nhận thức này đã khiến nhiều người thực hành và nhà nghiên cứu bỏ qua các kiến trúc thay thế, đặc biệt là Mạng Nơ-ron Tích chập (CNN) - vốn thường được liên hệ chủ yếu với các tác vụ xử lý hình ảnh. Sách giáo khoa và các khóa học thường phân loại các tác vụ như mô hình hóa ngôn ngữ, dịch máy, hoặc các dự đoán tuần tự khác là lĩnh vực của các mạng hồi quy, trong khi các mạng tích chập thường chỉ được giới thiệu trong bối cảnh dữ liệu không gian như hình ảnh. Kết quả là, tiềm năng của CNN cho mô hình hóa chuỗi thường bị đánh giá thấp hoặc bị bỏ qua.\nCác mạng tích chập (Convolutional networks) mang lại một số ưu điểm khiến chúng phù hợp cho dữ liệu tuần tự. Tính song song vốn có của chúng cho phép huấn luyện nhanh hơn đáng kể so với các mô hình tuần tự nghiêm ngặt. Ngoài ra, CNN có hiệu quả cao trong việc nắm bắt các quan hệ không gian và thời gian cục bộ - một đặc tính có thể được tận dụng trong dự báo chuỗi thời gian và các tác vụ tuần tự khác. Tuy vậy, CNN thường bị hiểu nhầm là không phù hợp cho chuỗi do thiếu cơ chế bộ nhớ rõ ràng và không có thứ tự thời gian nội tại.\nTrong nghiên cứu của chúng tôi, chúng tôi tập trung vào dự báo quỹ đạo bão, nơi dữ liệu bao gồm các bản ghi chuỗi thời gian của tọa độ vĩ độ và kinh độ. Chúng tôi chứng minh rằng loại dữ liệu tuần tự này có thể được mô hình hóa hiệu quả bằng cách sử dụng CNN, tận dụng hiệu suất tính toán và khả năng nắm bắt các mẫu hình không-thời gian cục bộ của chúng. Để hỗ trợ điều này, chúng tôi mã hóa các vị trí thành một biểu diễn ma trận 2D, cho phép mạng tích chập trích xuất và học các mẫu hình từ dữ liệu hiệu quả hơn. Mỗi mục trong ma trận tương ứng với một \u0026ldquo;điểm ảnh\u0026rdquo; của một hình ảnh, một cách tiếp cận mà chúng tôi gọi là Quỹ đạo-dưới-dạng-Hình ảnh (Trajectory-as-Image). Phương pháp của chúng tôi sử dụng một CNN tiêu chuẩn làm cơ sở, sau đó so sánh nó với các mô hình chuỗi chuyên biệt hơn, bao gồm TCN, LSTM và RNN, đồng thời kết hợp các kỹ thuật tăng cường dữ liệu khác nhau, bao gồm hai phương pháp mới được đề xuất trong công trình này.\nThông qua quá trình thử nghiệm và đánh giá có hệ thống, chúng tôi mong muốn thách thức quan điểm phổ biến cho rằng kiến trúc tích chập không phù hợp với dữ liệu chuỗi. Bằng cách làm nổi bật hiệu quả của CNN trong mô hình hóa chuỗi, chúng tôi hy vọng mở rộng góc nhìn của cộng đồng nghiên cứu và thực hành, khuyến khích họ xem xét điện toán tích chập như một hướng tiếp cận khả thi và cạnh tranh trong dự báo chuỗi thời gian cũng như các tác vụ dự đoán tuần tự khác.\nLợi ích và Hiệu quả đầu tư Tăng hiệu suất: STFA + PGBA tạo ra các chuỗi tổng hợp có cấu trúc giúp tăng cường độ mạnh mẽ của mô hình, giảm hiện tượng quá khớp và cải thiện khả năng khái quát hóa trên các quỹ đạo bão chưa từng thấy.\nNhận thức vật lý: Việc tích hợp các nguyên lý địa lý như khoảng cách và góc phương vị giúp tăng khả năng diễn giải và đảm bảo kết quả dự đoán phù hợp với các ràng buộc vật lý.\nHướng nghiên cứu mới: Thiết lập hai mô hình mới cho việc tăng cường chuỗi thời gian dựa trên sự phai mờ liên quan theo thời gian, mở rộng bộ công cụ phương pháp luận cho học máy chuỗi.\nScalability and Reusability: Khuôn khổ kết hợp STFA + PGBA + PIML có thể được mở rộng sang các lĩnh vực dự báo chuỗi khác như nhu cầu năng lượng, lưu lượng giao thông và xu hướng tài chính.\nTác động tổng thể: Bằng cách cải thiện độ ổn định và khả năng diễn giải của mô hình trong khi vẫn duy trì tính mở rộng, phương pháp được đề xuất mang lại cả giá trị khoa học lẫn hiệu quả thực tiễn trong đầu tư tính toán.\n3. Kiến trúc giải pháp Nền tảng trực tuyến cung cấp cho người dùng thông tin cập nhật về các cơn bão gần đây và một công cụ mạnh mẽ để dự báo quỹ đạo bão. Người dùng có thể xem dữ liệu bão gần đây hoặc chạy các dự đoán bằng các mô hình ML. Kết quả được hiển thị trực quan trên bản đồ, cho thấy vị trí, thời gian và đường đi dự đoán của bão.\nNền tảng được xây dựng bằng kiến trúc không máy chủ trên AWS để giảm chi phí vận hành trong khi vẫn đảm bảo khả năng mở rộng và độ tin cậy. Nội dung frontend được lưu trữ trên Amazon S3 và phân phối toàn cầu thông qua CloudFront, đảm bảo truy cập với độ trễ thấp. Yêu cầu của người dùng được định tuyến qua API Gateway tới các hàm Lambda, nơi xử lý tính toán dự đoán và truy xuất dữ liệu. Các mô hình ML đã được huấn luyện trước và tập dữ liệu bão gần đây được lưu trữ an toàn trong S3, với các bản cập nhật hàng tuần được quản lý tự động bởi trình thu thập dữ liệu kích hoạt qua EventBridge. Các khóa API nhạy cảm được lưu trữ trong Secrets Manager, và hiệu suất hệ thống được giám sát thông qua nhật ký và số liệu trên CloudWatch. IAM thực thi quyền truy cập tối thiểu cho tất cả các dịch vụ.\nCác mô hình dự báo tận dụng các kỹ thuật STFA (Tăng cường Phai mờ Thời gian Theo từng Bước) và PGBA (Tăng cường Hướng đi Trắc địa Hợp lý) được đề xuất. Các phương pháp này tạo ra các quỹ đạo chuỗi thời gian tổng hợp chân thực, bảo toàn mức độ liên quan theo thời gian và tính nhất quán không gian, giúp cải thiện đáng kể độ mạnh mẽ và độ chính xác của mô hình. Bằng cách tích hợp STFA và PGBA, nền tảng cung cấp các dự báo đường đi bão chính xác hơn, giúp người dùng hiểu rõ hơn về hành vi của bão và đưa ra quyết định sáng suốt.\nKiến trúc này cho phép tạo ra một nền tảng phản hồi nhanh, hiệu quả về chi phí và bảo mật, nơi người dùng có thể trực quan hóa thông tin bão theo thời gian thực và khám phá các dự báo quỹ đạo bão với bản đồ tương tác, được hỗ trợ bởi các phương pháp tăng cường tiên tiến để nâng cao hiệu suất mô hình.\nHình 1 : Sơ đồ huấn luyện mô hình Machine Learning\rHình 2 : Kiến Trúc Platform\rCác dịch vụ AWS được sử dụng Amazon S3: Lưu trữ các tệp frontend tĩnh, các mô hình ML đã được huấn luyện trước và dữ liệu bão mới nhất. AWS Lambda: Chạy các mô hình dự đoán, truy xuất dữ liệu bão và thực thi tự động hóa thu thập dữ liệu web. Amazon API Gateway: Xử lý các yêu cầu từ frontend cho dự báo và dữ liệu bão. Amazon CloudFront: Phân phối nội dung tĩnh trên toàn cầu với độ trễ thấp. Amazon Route 53: Định tuyến lưu lượng người dùng đến CloudFront. Amazon EventBridge: Lập lịch trình thu thập dữ liệu hàng tuần. AWS Secrets Manager: Lưu trữ an toàn các khóa API từ bên ngoài. Amazon CloudWatch: Giám sát nhật ký Lambda, số liệu hiệu suất và tình trạng hệ thống. AWS IAM: Gắn quyền truy cập tối thiểu (least-privilege) cho tất cả các dịch vụ. Thiết kế thành phần Lớp Frontend: Được lưu trữ trên S3 và phân phối thông qua CloudFront. Lớp Backend: Xử lý dự báo và truy xuất dữ liệu bằng API Gateway và Lambda. lưu trữ Dữ liệu: Các mô hình ML được lưu trữ trong S3, dữ liệu bão mới nhất được cập nhật hàng tuần bởi trình thu thập dữ liệu. Tự động hóa: EventBridge kích hoạt Crawler Lambda hàng tuần để lấy dữ liệu bão từ các nguồn bên ngoài. Bảo mật \u0026amp; Giám sát: Các thông tin nhạy cảm được lưu trữ trong Secrets Manager, số liệu và nhật ký được thu thập thông qua CloudWatch. 4. Triển Khai Kỹ Thuật Các Giai Đoạn Triển Khai Dự án này có ba phần chính: xây dựng pipeline dự đoán, thiết lập thu thập dữ liệu và triển khai nền tảng web. Mỗi phần trải qua bốn giai đoạn:\nThiết Kế Kiến Trúc: Lên kế hoạch cho hệ thống serverless AWS, các hàm Lambda, cấu trúc S3. (Tuần 1-2) Ước Tính Chi Phí: Sử dụng AWS Pricing Calculator để đánh giá tính khả thi và điều chỉnh thiết kế (Tuần 1-2). Tối Ưu Hóa Kiến Trúc: Điều chỉnh bộ nhớ Lambda, cách sử dụng S3 và bộ nhớ đệm để giảm chi phí (Tuần 2-4). Phát Triển, Kiểm Thử, Triển Khai: Triển khai các hàm Lambda, lập lịch sự kiện, tích hợp mô hình ML và frontend web với Next.js (Tuần 4-8). Yêu Cầu Kỹ Thuật\nCác Mô Hình ML: Các mô hình quỹ đạo được đào tạo sẵn lưu trữ trong S3 (.h5/.pth), được tải bởi hàm Lambda dự đoán. Dữ Liệu Bão: Các tệp JSON được cập nhật hàng tuần, lưu trữ trong S3, được sử dụng để hiển thị frontend và xác thực dự đoán. Cơ Sở Hạ Tầng Serverless: Lambda cho dự đoán, truy xuất và thu thập; API Gateway cho các yêu cầu frontend; CloudFront/S3 để phân phối nội dung. Bảo Mật: Secrets Manager cho khóa API, IAM cho quyền truy cập tối thiểu. Giám Sát: CloudWatch để ghi log và các số liệu Lambda Insights. 5. Tiến Độ \u0026amp; Các Mốc Quan Trọng Dòng Thời Gian Dự Án\nTrước Kỳ Thực Tập (Tuần 1): Lập kế hoạch, nghiên cứu các API thời tiết bên ngoài và chuẩn bị mô hình ML.\nTrong Kỳ Thực Tập (Tuần 1-8):\nTuần 1-2: Nghiên cứu AWS, thiết kế kiến trúc và ước tính chi phí. Tuần 2-4: Tối ưu hóa kiến trúc, cấu hình quy trình không máy chủ và tích hợp các mô hình ML. Tuần 4-8: Triển khai các hàm Lambda, thiết lập frontend, kiểm thử hệ thống và triển khai lên môi trường production. Sau Khi Ra Mắt: Thu thập dữ liệu liên tục và giám sát trong tối đa 1 năm.\n6. Ước Tính Ngân Sách Khu vực: ap-southeast-1 (Singapore)\nChi phí ước tính hàng tháng để vận hành nền tảng dự đoán bão trên AWS như sau:\nA. Frontend \u0026amp; Phân Phối Nội Dung\nAmazon S3 (Tệp Tĩnh): Lưu trữ 5 GB tệp frontend (HTML, CSS, JS) và xử lý 10 GB chuyển dữ liệu mỗi tháng. Chi phí ≈ $0.54/tháng. Amazon CloudFront: Xử lý 50 GB chuyển dữ liệu và lên đến 1 triệu yêu cầu (trong phạm vi miễn phí). Chi phí ≈ $6.00/tháng. Amazon Route 53: 1 hosted zone và 1 triệu truy vấn DNS mỗi tháng. Chi phí ≈ $0.90/tháng. AWS Certificate Manager (ACM): Cung cấp chứng chỉ TLS cho truy cập HTTPS bảo mật. Miễn phí. Tổng cho Frontend \u0026amp; CDN: ≈ $7.4/tháng\nB. Backend (Xử Lý API \u0026amp; ML)\nAmazon API Gateway: Xử lý 1 triệu yêu cầu HTTP API mỗi tháng, mỗi yêu cầu có kích thước khoảng 1 MB. Chi phí ≈ $2.5/tháng. Lambda (Dự đoán Bão): Dự đoán quỹ đạo bão sử dụng các mô hình ML. Chạy ~1,000 lần mỗi ngày với 512 MB bộ nhớ được cấp phát và 1 GB bộ nhớ tạm thời. Mỗi lần thực thi kéo dài ~5 giây. Chi phí ≈ $2.54/tháng. Lambda (Lấy Dữ Liệu Bão Gần Đây): Lấy dữ liệu bão từ S3 cho frontend. Chạy ~20,000 lần mỗi ngày với 512 MB bộ nhớ và 512 MB bộ nhớ tạm thời trong 1 giây mỗi lần thực thi. Chi phí ≈ $0.00/tháng (nằm trong free tier). Tổng cho Backend: ≈ $4.54/tháng\nC. Tự Động Hóa \u0026amp; Thu Thập Dữ Liệu\nAmazon EventBridge: Lập lịch thu thập dữ liệu bão hàng tuần (1 cron trigger mỗi ngày). Miễn phí theo AWS free tier. Lambda (Trình Thu Thập Web): Lấy dữ liệu từ các API bên ngoài hàng tuần. Sử dụng 128 MB bộ nhớ và 512 MB bộ nhớ tạm thời, ~30 giây mỗi lần thực thi. Chi phí ≈ $0.00/tháng (free tier). AWS Secrets Manager: Lưu trữ 5 khóa API để truy cập an toàn vào các dịch vụ thời tiết bên ngoài. Chi phí ≈ $2.00/tháng. Tổng cho Tự Động Hóa \u0026amp; Thu Thập Dữ Liệu: ≈ $2.0/tháng\nD. Giám Sát \u0026amp; Ghi Log\nAmazon CloudWatch Logs: Thu thập logs từ tất cả các hàm Lambda và chuyển đến S3 với thời gian lưu giữ 1 tháng. Khoảng 2 GB logs mỗi tháng. Chi phí ≈ $0.57/tháng. CloudWatch Metrics (Lambda Insights): Giám sát 8 metrics trên các hàm Lambda. 10 metrics đầu tiên miễn phí, và chỉ ghi lại cho dự đoán và dữ liệu thu thập. Chi phí ≈ $0.00/tháng. Tổng cho Giám Sát \u0026amp; Ghi Log: ≈ $0.57/tháng\nE. Lưu Trữ \u0026amp; Truyền Dữ Liệu\nS3 (Bucket Mô Hình): Lưu trữ các mô hình ML (~1 GB) và xử lý ~60,000 yêu cầu GET mỗi tháng. Chi phí ≈ $0.05/tháng. S3 (Bucket Dữ Liệu Bão Gần Đây): Lưu trữ dữ liệu bão gần đây (~1 GB) với ~60,000 yêu cầu GET và 30 yêu cầu PUT mỗi tháng. Chi phí ≈ $0.27/tháng. Tổng cho Lưu Trữ \u0026amp; Truyền Dữ Liệu: ≈ $0.32/tháng\nF. Di chuyển lên AWS\nDùng AWS CodePipeline và CodeBuild cho 10 phút chỉnh sửa mỗi tháng ≈ $0.90/month. Tổng Chi Phí Hàng Tháng Ước Tính\nFrontend \u0026amp; CDN: $7.4 Backend (API + ML): $4.54 Tự động hóa (Crawler + Secrets): $2.0 Giám sát \u0026amp; Ghi log: $0.57 Lưu trữ \u0026amp; Truyền dữ liệu: $0.32 Phí di chuyển kỹ thuật : $0.90 TỔNG CỘNG ≈ $15.73/tháng\n7. Đánh Giá Rủi Ro Các Rủi Ro Chính\nSự cố Mạng: Mức độ ảnh hưởng trung bình, khả năng xảy ra trung bình. Nguồn Dữ liệu Không Khả Dụng: Mức độ ảnh hưởng trung bình, khả năng xảy ra thấp. Lỗi Mô Hình ML: Mức độ ảnh hưởng cao, khả năng xảy ra thấp. Vượt Quá Chi Phí: Mức độ ảnh hưởng trung bình, khả năng xảy ra thấp. Chiến Lược Giảm Thiểu\nMạng: Lưu vào bộ nhớ đệm (cache) dữ liệu bão gần đây trong S3 để cho phép hiển thị frontend trong thời gian xảy ra sự cố. Nguồn Dữ liệu: Lưu trữ dữ liệu bão lịch sử để dự phòng. Mô Hình ML: Xác thực và kiểm tra mô hình thường xuyên. Chi Phí: Giám sát mức sử dụng AWS và thiết lập cảnh báo ngân sách. Kế Hoạch Dự Phòng\nChuyển sang cập nhật thủ công nếu API bên ngoài thất bại. Khôi phục (rollback) về mô hình ML trước đó bằng cách sử dụng tính năng versioning của S3 nếu mô hình mới thất bại. 8. Kết Quả Kỳ Vọng Cải Tiến Kỹ Thuật:\nDự đoán quỹ đạo bão thời gian thực với các đường đi được hiển thị hóa. Hệ thống không máy chủ có khả năng mở rộng, xử lý hàng nghìn yêu cầu/ngày. Giá Trị Lâu Dài:\nDữ liệu bão tập trung cho nghiên cứu và phân tích. Khung (framework) có thể tái sử dụng cho các tác vụ dự đoán không gian địa lý khác. Chi phí vận hành hàng tháng thấp (\u0026lt; $20/tháng). "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/5-workshop/5.2-data-preparation/",
	"title": "Chuẩn Bị Dữ Liệu",
	"tags": [],
	"description": "",
	"content": "Thu Thập và Xử Lý Dữ Liệu Dữ liệu là một thành phần quan trọng trong dự án. Nó không chỉ cung cấp nguồn kiến thức cho mô hình machine learning mà còn được hiển thị trực tiếp cho người dùng cuối để theo dõi các cơn bão mới nhất tại khu vực Tây Thái Bình Dương. Vì dữ liệu có mục đích kép — huấn luyện mô hình và trực quan hóa theo thời gian thực — chúng em đã xem xét kỹ lưỡng nhiều nguồn data source đáng tin cậy và có thẩm quyền trước khi chọn một bộ dữ liệu đáp ứng đầy đủ các yêu cầu, đó là: Dữ liệu bão của NOAA.\nNOAA (National Oceanic and Atmospheric Administration) là cơ quan khoa học thuộc Bộ Thương mại Hoa Kỳ. NOAA cung cấp dữ liệu môi trường có độ chính xác cao phục vụ nghiên cứu, bao gồm quan sát thời tiết toàn cầu, ảnh vệ tinh và thông tin về các xoáy thuận nhiệt đới. Với nhiều thập kỷ đầu tư vào công nghệ tiên tiến như vệ tinh địa tĩnh, hệ thống radar và mạng lưới giám sát khí hậu, NOAA được xem là một trong những nguồn cung cấp dữ liệu bão đáng tin cậy nhất trên thế giới.\nTrong dự án này, chúng em sử dụng dữ liệu từ International Best Track Archive for Climate Stewardship (IBTrACS) — một dự án do NOAA khởi xướng và là bộ dữ liệu xoáy thuận nhiệt đới toàn diện nhất thế giới. IBTrACS tổng hợp dữ liệu đường đi của bão trong lịch sử từ nhiều cơ quan khí tượng (ví dụ: JTWC, JMA, CMA, NHC). Bằng cách hợp nhất các nguồn vào một định dạng thống nhất, IBTrACS cải thiện khả năng so sánh giữa các cơ quan và đảm bảo các nhà nghiên cứu trên toàn thế giới có quyền truy cập dữ liệu chất lượng cao nhất.\nPhiên bản mới nhất của bộ dữ liệu này chứa 226.153 dòng ghi nhận quan sát bão. Mỗi dòng bao gồm nhiều thuộc tính giá trị như:\nsid – mã cơn bão number – số thứ tự bão basin / subbasin – phân loại khu vực nature – loại bão (ví dụ: áp thấp, bão nhiệt đới, siêu bão) iso_time – thời gian lat / lon – tọa độ tâm bão … và nhiều thông số khí tượng học khác Tuy nhiên, đối với mô hình machine learning, chúng em chỉ tập trung vào bốn cột chính: sid, iso_time, lat, và lon. Đây là chuỗi thời gian cơ bản dùng để dự đoán quỹ đạo di chuyển của bão.\nBộ dữ liệu bao gồm các cơn bão từ 1870 đến 2025, được lọc chỉ giữ lại các cơn bão trong khu vực Tây Thái Bình Dương — phạm vi địa lý mà dự án hướng đến. Bộ dữ liệu gốc được công khai tại: https://data.humdata.org/dataset/vnm-ibtracs-tropical-storm-tracks#\nLàm sạch dữ liệu và Trích xuất đặc trưng dựa trên quy luật vật lý Một ưu điểm của IBTrACS là dữ liệu đã được bảo trì tốt và có tính nhất quán cao. Việc tiền xử lý chỉ yêu cầu các bước tối thiểu, chủ yếu là loại bỏ giá trị thiếu.\nSau khi làm sạch, chúng em áp dụng bước đầu tiên của machine learning dựa trên vật lý (physics-informed ML) — kỹ thuật đưa kiến thức vật lý trực tiếp vào pipeline dữ liệu. Từ tọa độ vĩ độ – kinh độ, nhóm tính thêm hai đặc trưng bằng công thức Haversine:\nKhoảng cách giữa hai điểm bão liên tiếp Góc phương vị (bearing) — hướng di chuyển Các đặc trưng này mang ý nghĩa vật lý: chúng phản ánh quy luật chuyển động thực tế, thay vì những biến đổi tùy ý. Nhờ đó, chúng tăng cường bối cảnh về quán tính và hướng di chuyển của bão, giúp mô hình học hiệu quả hơn và dự đoán chính xác hơn.\nHình 1 : Mô tả dữ liệu\rDữ Liệu Hiển Thị Trên Nền Tảng Dữ liệu dùng để hiển thị trên nền tảng khác với dữ liệu dùng để huấn luyện, dù cả hai đều xuất phát từ NOAA. Dữ liệu huấn luyện là tĩnh, còn dữ liệu hiển thị phải cập nhật theo tình hình bão hiện tại.\nĐể xử lý yêu cầu này, nhóm đã triển khai một AWS Lambda chạy theo lịch, tự động lấy dữ liệu đường đi bão mới nhất vào cuối mỗi ngày. Điều này đảm bảo nền tảng luôn hiển thị thông tin mới, chính xác và kịp thời cho người dùng.\nDữ liệu hiển thị sau xử lý được lưu dưới dạng file JSON trong S3. Khi người dùng truy cập website:\nFrontend gửi yêu cầu tới API Gateway API Gateway kích hoạt Lambda tương ứng Lambda lấy file JSON từ S3 Dữ liệu được trả về cho người dùng để hiển thị trực quan Pipeline này đảm bảo khả năng cập nhật thời gian thực, serverless và tiết kiệm chi phí.\nBộ dữ liệu bão có thể truy cập tại: https://ncics.org/ibtracs/\nHình 2 : Web để crawl dữ liệu\r"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/1-worklog/1.10-week10/",
	"title": "Worklog Tuần 10",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 10: Rà soát kết quả huấn luyện ban đầu để xác định hướng fine-tuning. Điều chỉnh các thành phần mô hình quan trọng nhằm cải thiện độ ổn định của forward-pass và tính nhất quán của dự đoán. Thực hiện huấn luyện fine-tuning để loss giảm mượt và đáng tin cậy hơn. Đánh giá dự đoán sau fine-tuning để xác nhận mức cải thiện so với giai đoạn huấn luyện ban đầu. Hoàn tất các phần học AWS còn lại cần thiết cho các hoạt động triển khai về sau. Nhiệm vụ cần thực hiện trong tuần này: Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 1 - Rà soát đầu ra huấn luyện ban đầu để xác định hướng fine-tuning\n+ Kiểm tra các dự đoán bị lệch (misaligned)\n+ Đánh dấu các trường hợp có hành vi không ổn định 11/11/2025 12/11/2025 - 2 - Điều chỉnh các thành phần mô hình để fine-tuning\n+ Tinh chỉnh cấu hình activation/hidden\n+ Làm sạch logic forward-pass\n+ Cập nhật các ảnh hưởng từ bước tiền xử lý 12/11/2025 14/11/2025 - 3 - Thực hiện huấn luyện fine-tuning và theo dõi diễn tiến loss được cải thiện 14/11/2025 15/11/2025 - 4 - Thực hiện các bài đánh giá sau fine-tuning\n- So sánh dự đoán sau cập nhật với kết quả huấn luyện Tuần 9 15/11/2025 16/11/2025 - 5 - Hoàn tất các hạng mục học AWS cuối cùng và tổng kết phần nghiên cứu liên quan môi trường (environment) cần thiết cho triển khai về sau 16/11/2025 17/11/2025 AWS Docs Kết quả đạt được Tuần 10: Cải thiện mô hình thông qua fine-tuning và nhận thấy loss giảm mượt hơn so với giai đoạn huấn luyện ban đầu.\nXác định các cải thiện quan trọng sau khi điều chỉnh một số thành phần của mô hình:\nForward-pass nhất quán và rõ ràng hơn Hành vi dự đoán ít “nhảy loạn” hơn Độ khớp tốt hơn giữa quỹ đạo dự đoán và quỹ đạo kỳ vọng Hoàn thành các phần học AWS còn lại phục vụ các bước triển khai sắp tới, bao gồm chuẩn bị môi trường và các lưu ý về logging.\nĐánh giá sau fine-tuning cho thấy đầu ra ổn định hơn trên nhiều cơn bão, thể hiện khả năng khái quát hóa (generalization) được cải thiện.\nTổng hợp ghi chú đã tinh chỉnh cho giai đoạn tiếp theo, tập trung vào cách mô hình sau fine-tuning hoạt động ở các tình huống “edge-case”:\nKiểm tra hiệu năng với outlier Các điều chỉnh tiền xử lý có thể cần xem lại Một số yếu tố kiến trúc đáng cân nhắc revisiting về sau "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/1-worklog/1.11-week11/",
	"title": "Worklog Tuần 11",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 11: Rà soát các chỉ số (metrics) huấn luyện để hiểu rõ hơn hành vi mô hình và xác định các điểm cần phân tích sâu hơn. Đánh giá dự đoán của mô hình trên nhiều mẫu bão để phát hiện các sai lệch lớn và các mẫu lỗi lặp lại. Thực hiện phân tích lỗi mở rộng để hiểu điểm yếu của mô hình theo từng loại bão khác nhau. Bắt đầu tích hợp mô hình đã huấn luyện vào tầng API và kiểm tra định dạng đầu ra. Chạy các bài test triển khai ban đầu để đảm bảo dự đoán hiển thị đúng trên giao diện web. Nhiệm vụ cần thực hiện trong tuần này: Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 1 - Rà soát kết quả huấn luyện tổng quan để xác định metric nào cần phân tích sâu hơn\n- Kiểm tra xu hướng loss qua nhiều epoch 18/11/2025 19/11/2025 - 2 - Đánh giá dự đoán mô hình trên một số mẫu bão\n+ So sánh quỹ đạo dự đoán với ground truth\n+ Đánh dấu các chuỗi có độ lệch lớn 19/11/2025 21/11/2025 - 3 - Thực hiện phân tích lỗi mở rộng để phát hiện các vấn đề lặp lại\n- Rà soát sự khác biệt hiệu năng giữa các loại bão 21/11/2025 22/11/2025 - 4 - Bắt đầu tích hợp mô hình đã huấn luyện vào cấu trúc API\n+ Chuẩn bị hàm inference để trả output cho API\n+ Kiểm tra/đối chiếu định dạng response để web có thể sử dụng 22/11/2025 23/11/2025 - 5 - Thực hiện các bài test triển khai ban đầu trên giao diện web bằng cách gọi API endpoint của mô hình\n- Kiểm tra việc render dự đoán trên web UI có đúng không 23/11/2025 24/11/2025 - Kết quả đạt được Tuần 11: Hiểu rõ hơn hành vi mô hình sau khi rà soát các training metrics và phân tích cách loss thay đổi theo epoch.\nĐánh giá mô hình trên nhiều mẫu bão và nhận diện các pattern về loại bão thường gây sai lệch lớn nhất:\nCác trường hợp quỹ đạo có đoạn rẽ ngoặt (turning trajectories) Tình huống tốc độ thay đổi đột ngột Các outlier có hướng di chuyển không nhất quán Thực hiện phân tích lỗi sâu hơn, giúp làm rõ các điểm yếu lặp lại và định hướng cho các bước tinh chỉnh tiếp theo.\nChạy nhiều bài test inference trên local và xác nhận định dạng output đủ ổn định để các thành phần downstream sử dụng.\nBắt đầu tích hợp mô hình vào tầng API một cách thành công:\nChuẩn bị logic trả về kết quả inference Đảm bảo API trả JSON nhất quán Kiểm tra tương thích với giao diện web Xác minh triển khai API bản đầu hoạt động trên phiên bản web và quỹ đạo dự đoán có thể render đúng trên UI.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/1-worklog/1.12-week12/",
	"title": "Worklog Tuần 12",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 12: Hoàn thiện toàn bộ tài liệu dự án để nộp. Hoàn thành báo cáo kỹ thuật bao gồm tiền xử lý, đánh giá mô hình và luồng hoạt động của hệ thống. Chuẩn bị và tinh chỉnh slide thuyết trình cuối cùng cho phần demo dự án. Thực hiện kiểm thử end-to-end đầy đủ để đảm bảo hệ thống triển khai hoạt động ổn định và đáng tin cậy. Tổ chức lại repository và tạo runbook rõ ràng để bàn giao dự án. Nhiệm vụ cần thực hiện trong tuần này: Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 1 - Bắt đầu chuẩn bị tài liệu dự án cuối cùng\n+ Viết phần tổng quan hệ thống (system overview)\n+ Mô tả kiến trúc mô hình và tóm tắt quá trình huấn luyện 25/11/2025 26/11/2025 - 2 - Tiếp tục viết báo cáo kỹ thuật để nộp cuối kỳ\n- Hoàn thành các phần về tiền xử lý, các chỉ số đánh giá và phân tích lỗi 26/11/2025 27/11/2025 - 3 - Bắt đầu xây dựng slide thuyết trình cuối cùng\n+ Thêm sơ đồ kiến trúc (architecture diagrams)\n+ Bổ sung chi tiết về API flow và tích hợp với web 27/11/2025 28/11/2025 - 4 - Thực hiện kiểm thử demo end-to-end cuối cùng trên hệ thống đã triển khai\n- Kiểm tra luồng API → web và xác nhận dự đoán render đúng, không gặp lỗi 28/11/2025 29/11/2025 - 5 - Dọn dẹp repository và viết runbook bàn giao dự án 29/11/2025 29/11/2025 - Kết quả đạt được Tuần 12: Hoàn thành tài liệu dự án cuối cùng với phần giải thích rõ ràng về workflow hệ thống và hành vi mô hình.\nHoàn tất báo cáo kỹ thuật bao gồm các bước tiền xử lý, đánh giá mô hình và phân tích lỗi, giúp dự án dễ review và dễ bảo trì hơn.\nTạo slide thuyết trình cuối cùng cho buổi trình bày tổng kết:\nThêm sơ đồ pipeline của mô hình và tích hợp AWS Đưa ví dụ phản hồi từ API Nêu bật các cải thiện so với các tuần trước Xác thực thành công toàn bộ hệ thống end-to-end thông qua kiểm thử demo cuối, đảm bảo API và web interface hoạt động ổn định cùng nhau.\nTổ chức lại repository và chuẩn bị runbook để bàn giao:\nBao gồm hướng dẫn triển khai (deployment instructions) Ghi rõ các bước chạy lại tiền xử lý và inference Thêm ghi chú cho các hướng cải tiến trong tương lai "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/3-blogstranslated/",
	"title": "Các bài blogs đã dịch",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Trexlix đạt 35% mức độ tiết kiệm chi phí và tăng cường bảo mật nhờ vào dịch vụ OpenSearch của Amazon Bài viết này giới thiệu về Trelix 1 nhà lãnh đạo hàng đầu về các giải pháp an toàn thông tin,nổi lên vào năm 2022 từ sự hợp nhất của McAfee Enterprise và FireEye, phục vụ cho hơn 40,000 doanh nghiệp khách hàng trên khắp thế giới,Trelix cung cấp nền tảng bảo mật AI toàn diện, cởi mở nhất và quen thuộc cho các doanh nghiệp.Các biện pháp của họ giúp các tổ chức xây dựng khả năng phục hồi để chống lại các mối đe dọa nâng cao thông qua tự động phát hiện,điều tra và phản ứng\nBlog 2 - Tính năng cấp quyền truy cập dữ liệu trong các hệ thống RAG Blog này giới thiệu các tổ chức đang áp dụng phương pháp Retrieval-Augmented Generation (RAG) nhằm kết hợp dữ liệu độc quyền, kiến thức chuyên ngành và tài liệu nội bộ để cung cấp các phản hồi chính xác và có ngữ cảnh hơn. Với RAG, các LLM sử dụng một kho tri thức bên ngoài dựa trên vector store để kết hợp các dữ liệu kiến thức cụ thể trước khi tạo phản hồi.\nBlog 3 - Công bố tính năng ingestion đa tài khoản cho Amazon OpenSearch Service Blog này giới thiệu từ một tài khoản logging trung tâm với hai tài khoản phát triển khác (A và B). Tài khoản logging trung tâm có thể tạo một pipeline OpenSearch Ingestion bằng cách sử dụng một nguồn dạng push, ví dụ như HTTP. Sau khi tạo pipeline, một thành viên của nhóm logging trung tâm có thể cấp quyền truy cập cho các nhóm khác. Họ có thể sử dụng một resource policy cung cấp quyền cho hai tài khoản nhóm còn lại để tạo các endpoint của pipeline. Sau khi thực hiện thay đổi này, pipeline OpenSearch Ingestion sẽ có sẵn để các nhóm khác sử dụng.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/5-workshop/5.3-ml-model/",
	"title": "Mô Hình Học Máy",
	"tags": [],
	"description": "",
	"content": "HUẤN LUYỆN MÔ HÌNH Mục sau đây trình bày về quá trình phát triển mô hình dự đoán được thiết kế để dự đoán vị trí địa lý tiếp theo của một cơn bão, dựa trên dữ liệu quan sát từ quỹ đạo di chuyển trước đó. Nói cách đơn giản, nhóm sử dụng chuỗi các giá trị vĩ độ và kinh độ trong quá khứ để dự đoán vĩ độ và kinh độ tại bước thời gian kế tiếp.\nTrích Xuất Đặc Trưng Sau khi hoàn tất bước tiền xử lý dữ liệu, nhóm tiến hành chia bộ dữ liệu thành 70% để huấn luyện, 10% để kiểm định, và 20% để kiểm tra. Việc chia này được thực hiện theo mã định danh cơn bão (storm ID), đảm bảo không có cơn bão nào xuất hiện đồng thời trong cả tập huấn luyện và tập kiểm định/kiểm tra. Điều này giúp ngăn rò rỉ dữ liệu và đảm bảo độ tin cậy của quá trình đánh giá.\nTrong quá trình huấn luyện mô hình, mỗi mẫu đầu vào bao gồm chuỗi 4 bước thời gian liên tiếp, trong đó mỗi bước cách nhau 3 giờ. Như vậy, một chuỗi đầu vào tương ứng với quãng di chuyển 9 giờ của cơn bão.\nFigure 1 : Dataset Distribution\rÁp dụng Kỹ thuật Stepwise Temporal Fading Augmentation (STFA) Để tăng độ đa dạng của dữ liệu huấn luyện, nhóm áp dụng phương pháp tự đề xuất — Stepwise Temporal Fading Augmentation (STFA) — lên 50% tập huấn luyện, được lựa chọn dựa trên từng storm ID riêng biệt. Các chuỗi gốc của những cơn bão này sẽ được thay thế bằng chuỗi đã được tăng cường, đảm bảo kích thước tập huấn luyện cuối cùng vẫn giữ nguyên (xấp xỉ 100% kích thước ban đầu).\nNhư đã đề cập trong phần đề xuất mô hình, STFA thay đổi các điểm cũ hơn trong chuỗi, trong khi giữ các quan sát mới nhất không đổi. Với mỗi chuỗi 4 bước:\n2 bước mới nhất được giữ nguyên 2 bước cũ hơn được nhân với hệ số giảm dần: [0.98, 0.99] Mặc dù những giá trị này có vẻ nhỏ, nhưng vĩ độ và kinh độ cực kỳ nhạy cảm. Một thay đổi nhỏ — ví dụ từ 6.7 lên 6.8 — có thể tương ứng với hàng chục kilomet dịch chuyển ngoài thực tế. Do đó, mức điều chỉnh nhỏ như vậy là hợp lý và phù hợp với quy luật vật lý, giúp dữ liệu tăng cường vẫn mang tính chân thực.\nVí dụ STFA trên một chuỗi 4 bước thời gian Row Original (lat, lon) Augmented (lat, lon) Operation 1 [-6.8, 107.5] [-6.66, 105.35] nhân với 0.98 2 [-7.0, 107.1] [-6.93, 106.03] nhân với 0.99 3 [-7.3, 106.7] [-7.3, 106.7] giữ nguyên 4 [-7.5, 106.4] [-7.5, 106.4] giữ nguyên Quy trình trên làm giảm giá trị của các quan sát cũ, đồng thời giữ nguyên các bước mới. Việc tăng cường này giúp mô hình có thêm biến thiên có kiểm soát, cải thiện khả năng tổng quát hóa trong dự báo quỹ đạo.\nTrước đó, nhóm đã sử dụng machine learning dựa trên quy luật vật lý để tính khoảng cách và góc phương vị bằng công thức Haversine. Sau khi STFA được áp dụng, các giá trị này sẽ được tính lại dựa trên tọa độ đã tăng cường để đảm bảo các đặc trưng vật lý vẫn chính xác và nhất quán.\nFigure 2 : Comparison of Augmentation Techniques on Storm Trajectories\rThiết lập Mô hình 1. Hàm Loss dựa trên quy luật vật lý Việc ứng dụng công thức Haversine không chỉ dừng lại ở bước trích xuất đặc trưng. Ngoài việc tạo ra các giá trị khoảng cách và hướng, nhóm còn tích hợp Công thức Haversine như một hàm loss tùy chỉnh, sử dụng cùng với các hàm loss truyền thống như MSE, RMSE và MAPE.\nCông thức Haversine đo khoảng cách địa lý thực giữa hai điểm, nên đây là metric tự nhiên để đánh giá sai số dự đoán vị trí bão. Khoảng cách Haversine càng lớn nghĩa là dự đoán càng sai; ngược lại, giá trị gần 0 km cho thấy mô hình hoạt động tốt.\nVí dụ:\nDự đoán: [-6.72, 107.1] Giá trị thật: [-6.8, 107.5] Haversine loss: 45.06 km Giá trị 45.06 km phản ánh chính xác sai số vị trí ngoài thực tế, giúp việc giải thích mô hình dễ dàng và ý nghĩa hơn.\n2. Kiến trúc mô hình Các bài toán mô hình hóa chuỗi thường được xử lý bằng RNN, LSTM hoặc GRU, nhưng các nghiên cứu gần đây cho thấy mô hình dựa trên tích chập (CNN) có thể vượt trội hơn trong nhiều tác vụ time-series.\nDo đó, nhóm sử dụng kiến trúc CNN — cụ thể là Temporal Convolutional Network (TCN).\nTCN sử dụng tích chập giãn (dilated convolution), giúp mô hình có receptive field rộng mà không cần dùng mạng hồi quy.\nTCN kết hợp được cả:\nkhả năng học phụ thuộc dài hạn tốc độ huấn luyện nhanh gradient ổn định Nên rất phù hợp cho bài toán dự báo quỹ đạo bão.\n3. Các siêu tham số mô hình Input: 4 đặc trưng (lat, lon, distance, bearing) Hidden units: 1024 Số lớp TCN: 2 Learning rate: 1e-4 Epochs: 80 Optimizer: Adam Early stopping: patience = 6 4. Hàm Loss tổng hợp Loss chính của mô hình được kết hợp từ:\nMSE của lat/lon MSE của distance/bearing Haversine loss (dựa trên vật lý) Trong đó:\nλ_aux = 0.5 λ_hav = 0.3 Cách thiết kế này giúp mô hình:\ngiảm sai số tọa độ tôn trọng quy luật dịch chuyển vật lý tránh overfit vào một loại đặc trưng cụ thể Figure 3 : Training Process\rEvaluation Đánh giá mô hình Sau khi mô hình hoàn tất quá trình huấn luyện và dừng sớm (early stopping), nhóm tiến hành đánh giá trên tập kiểm tra để xác định khả năng tổng quát hóa và mức độ sẵn sàng triển khai thực tế.\nKết quả đánh giá:\nTotal Loss: 74.3849 MSE: 0.0832 RMSE: 0.2772 MAPE: 0.60% Haversine: 30.75 km Sai số vị trí trung bình khoảng 30 km — mức hoàn toàn chấp nhận được đối với hệ thống có quy mô hàng trăm đến hàng nghìn kilomet như bão nhiệt đới. MSE nhỏ (0.08) cho thấy khả năng dự đoán tốt và ổn định.\nKết quả này cũng chứng minh rằng các mô hình convolution có thể hoạt động xuất sắc trong bài toán mô hình hóa chuỗi, không chỉ trong xử lý ảnh.\nSau khi xác thực mô hình, bước tiếp theo là tải mô hình lên Amazon S3 và sử dụng AWS Lambda để thực thi mô hình khi người dùng gửi yêu cầu dự đoán.\nFigure 4 : Evaluation Metrics\r"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/4-eventparticipated/",
	"title": "Các events đã tham gia",
	"tags": [],
	"description": "",
	"content": "Trong quá trình thực tập, em đã tham gia hai sự kiện. Mỗi sự kiện đều là một trải nghiệm đáng nhớ, mang lại những kiến thức mới mẻ, thú vị và hữu ích, cùng với quà tặng và những khoảnh khắc tuyệt vời. Sự kiện 1 Tên sự kiện: Workshop “AI-Powered Cloud Solutions \u0026amp; Amazon Bedrock”\nThời gian: 08:00, ngày 15 tháng 11 năm 2025\nĐịa điểm: Tầng 26, Tòa nhà Bitexco, 02 Hai Triều, Phường Sài Gòn, TP. Hồ Chí Minh\nVai trò: Người tham dự\nTổng quan sự kiện: Sự kiện giới thiệu các giải pháp cloud hiện đại ứng dụng AI với Amazon Bedrock, bao gồm Foundation Models, prompt engineering, RAG và phát triển AI agent. Các diễn giả trình bày những use case thực tế với AWS AI Services và giải thích cách doanh nghiệp ứng dụng AI để hiện đại hóa quy trình làm việc, đồng thời xây dựng các sản phẩm cloud có khả năng mở rộng.\nKết quả đạt được: Tôi hiểu rõ hơn về Foundation Models, các kỹ thuật prompt engineering và quy trình RAG. Đồng thời, tôi cũng học được cách các công cụ của AWS như Bedrock, AgentCore và các AI Services giúp tăng tốc phát triển, nâng cao độ chính xác và hỗ trợ xây dựng các sản phẩm AI thực tế — hữu ích cho cả dự án cá nhân lẫn đóng góp trong công việc nhóm sau này.\nSự kiện 2 Tên sự kiện: AWS Cloud, AI \u0026amp; Innovation Summit\nThời gian: 09:00, ngày 19 tháng 9 năm 2025\nĐịa điểm: Tầng 26, Tòa nhà Bitexco, 02 Hai Triều, Phường Sài Gòn, TP. Hồ Chí Minh\nVai trò: Người tham dự\nTổng quan sự kiện: Sự kiện nhấn mạnh chiến lược phát triển cloud và AI của Việt Nam, mối quan hệ hợp tác ngày càng mở rộng với Hoa Kỳ, và tác động thực tiễn của AI cùng blockchain trong giáo dục, y tế và công nghiệp. Các diễn giả cũng giới thiệu các chương trình của AWS, các thực hành AI có trách nhiệm (Responsible AI) và các công cụ như SageMaker, Amazon Q và QuickSight để hỗ trợ phát triển theo hướng AI-driven.\nKết quả đạt được: Tôi có cái nhìn rõ ràng hơn về cách cloud, AI và blockchain thúc đẩy chuyển đổi số quốc gia, đồng thời tích lũy thêm kiến thức thực hành về quy trình phát triển AI, các lưu ý về bảo mật và những dịch vụ AWS có thể giúp tăng năng suất và hỗ trợ các dự án trong tương lai.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/5-workshop/5.4-frontback-end/5.4.1-frontend-architecture/",
	"title": "Kiến Trúc Frontend",
	"tags": [],
	"description": "",
	"content": "Kiến trúc Frontend – Ứng dụng Web Dự báo Bão Tổng quan Dưới đây là tài liệu chi tiết về quá trình phát triển front-end của nhóm: Một ứng dụng web xây dựng bằng React và TypeScript dùng để theo dõi và dự đoán quỹ đạo bão\nKiến trúc dịch vụ AWS ┌─────────────────────────────────────────────────────────────┐\r│ TRÌNH DUYỆT NGƯỜI DÙNG │\r└────────────────────────┬────────────────────────────────────┘\r│\r▼\r┌─────────────────────────────────────────────────────────────┐\r│ CloudFront CDN │\r│ - Distribution: d3lj47ilp0fgxy.cloudfront.net │\r│ - SSL/TLS: HTTPS │\r│ - Cache: Tài nguyên tĩnh + dữ liệu JSON │\r│ - Truy cập Origin: OAI/OAC (bảo mật) │\r└────────────────────────┬────────────────────────────────────┘\r│\r▼\r┌─────────────────────────────────────────────────────────────┐\r│ S3 Bucket (Riêng tư) │\r│ - Bucket: storm-frontend-hosting-duc-2025 │\r│ - Static Website Hosting: TẮT │\r│ - Quyền truy cập: Chỉ CloudFront được phép qua REST API │\r│ - Nội dung: HTML, CSS, JS, hình ảnh, recent_storms.json │\r└─────────────────────────────────────────────────────────────┘\r┌─────────────────────────────────────────────────────────────┐\r│ Các hàm Lambda │\r│ ┌─────────────────────────────────────────────────────┐ │\r│ │ Lambda #1: Dự báo bão │ │\r│ │ - URL: vill3povlzqxdyxm7ubldizobu0kdgbi... │ │\r│ │ - Phương thức: POST /predict │ │\r│ │ - Xác thực: KHÔNG (công khai) │ │\r│ │ - Container: ECR (Docker) │ │\r│ │ - Mô hình: LSTM + TCN │ │\r│ └─────────────────────────────────────────────────────┘ │\r│ ┌─────────────────────────────────────────────────────┐ │\r│ │ Lambda #2: Thu thập dữ liệu bão (mới) │ │\r│ │ - Kích hoạt: EventBridge (hàng tuần) │ │\r│ │ - Chức năng: Thu thập dữ liệu IBTrACS │ │\r│ │ - Đầu ra: recent_storms.json → S3 │ │\r│ └─────────────────────────────────────────────────────┘ │\r└─────────────────────────────────────────────────────────────┘\r┌─────────────────────────────────────────────────────────────┐\r│ EventBridge │\r│ - Rule: storm-data-crawler-weekly-trigger │\r│ - Lịch chạy: Mỗi Chủ nhật 00:00 UTC (07:00 giờ Việt Nam) │\r│ - Đích: Lambda #2 (Thu thập dữ liệu bão) │\r└─────────────────────────────────────────────────────────────┘ Cấu trúc thư mục Frontend frontend/ ├── src/ │ ├── components/ # Các component React │ │ ├── ui/ # Component từ shadcn/ui (button, card, input, ...) │ │ ├── storm/ # Component chuyên cho nghiệp vụ bão │ │ ├── timeline/ # Điều khiển timeline │ │ ├── wind/ # Trực quan hóa gió │ │ ├── StormPredictionForm.tsx # Form nhập tọa độ bão │ │ ├── WeatherMap.tsx # Bản đồ Leaflet chính │ │ ├── StormTracker.tsx # Danh sách bão │ │ ├── StormInfo.tsx # Thông tin chi tiết bão │ │ ├── StormAnimation.tsx # Marker hoạt ảnh │ │ ├── WeatherOverlay.tsx # Lớp phủ Nhiệt độ/Gió │ │ ├── WeatherLayerControl.tsx # Lớp Satellite/Radar │ │ ├── WeatherLayerControlPanel.tsx # UI bảng điều khiển lớp dữ liệu │ │ ├── WeatherValueTooltip.tsx # Tooltip khi rê chuột │ │ ├── WindyLayer.tsx # Tích hợp Windy.com │ │ ├── ProvinceLayer.tsx # Lớp ranh giới tỉnh/thành Việt Nam │ │ ├── OptimizedTemperatureLayer.tsx │ │ ├── TemperatureHeatMapLayer.tsx │ │ ├── ThemeToggle.tsx # Chế độ Sáng/Tối │ │ ├── PreferencesModal.tsx # Tùy chọn người dùng │ │ ├── RightSidebar.tsx # Panel bên phải │ │ └── WeeklyForecast.tsx # Dự báo 7 ngày │ │ │ ├── pages/ │ │ ├── Index.tsx # Trang chính │ │ └── NotFound.tsx # Trang 404 │ │ │ ├── lib/ # Logic nghiệp vụ \u0026amp; tiện ích │ │ ├── api/ # Client gọi API │ │ ├── __tests__/ # Unit test │ │ ├── stormData.ts # Kiểu dữ liệu \u0026amp; interface │ │ ├── stormAnimations.ts # Logic hoạt ảnh │ │ ├── stormIntensityChanges.ts │ │ ├── stormPerformance.ts │ │ ├── stormValidation.ts │ │ ├── windData.ts │ │ ├── windStrengthCalculations.ts │ │ ├── windyStatePersistence.ts │ │ ├── windyUrlState.ts │ │ ├── mapUtils.ts # Tiện ích hỗ trợ bản đồ │ │ ├── openWeatherMapClient.ts │ │ ├── dataWorker.ts # Web Worker │ │ ├── utils.ts │ │ └── colorInterpolation.ts │ │ │ ├── hooks/ # Custom React hooks │ │ ├── use-toast.ts │ │ ├── use-theme.tsx │ │ ├── use-mobile.tsx │ │ ├── useTimelineState.ts │ │ ├── useWindyStateSync.ts │ │ └── useSimplifiedTooltip.ts │ │ │ ├── contexts/ # React Context │ │ └── WindyStateContext.tsx │ │ │ ├── api/ │ │ └── weatherApi.ts # Các hàm gọi API │ │ │ ├── utils/ │ │ └── colorInterpolation.ts │ │ │ ├── styles/ │ │ └── accessibility.css # Style tuân thủ WCAG │ │ │ ├── test/ # Bộ test │ │ ├── accessibility.test.ts │ │ ├── accessibility-audit.test.ts │ │ ├── wcag-compliance.test.ts │ │ ├── performance.test.ts │ │ ├── cross-browser.test.ts │ │ └── setup.ts │ │ │ ├── assets/ # Ảnh, icon │ ├── App.tsx │ ├── main.tsx │ └── index.css │ ├── public/ # Tài nguyên tĩnh ├── dist/ # Output sau khi build (npm run build) ├── .env.production # Cấu hình môi trường production ├── .env.example ├── package.json ├── vite.config.ts ├── vitest.config.ts # Cấu hình test ├── tailwind.config.ts ├── tsconfig.json └── components.json # Cấu hình shadcn/ui Biến môi trường .env.production # OpenWeather API VITE_OPENWEATHER_API_KEY=8ff7f009d2bd420c86845c6bcf6de4a9 # CloudFront URL - Dùng để lấy dữ liệu bão VITE_CLOUDFRONT_URL=https://d3lj47ilp0fgxy.cloudfront.net # Lambda Function URL - API dự đoán bão VITE_PREDICTION_API_URL=https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws Quy trình Build \u0026amp; Deploy 1. Build bản Production cd frontend npm run build Đầu ra: thư mục dist/ bao gồm:\nindex.html assets/index-[hash].js assets/index-[hash].css 2. Upload lên S3 aws s3 sync dist/ s3://storm-frontend-hosting-duc-2025/ --delete Important Notes:\nS3 bucket ở chế độ riêng tư (không public) CloudFront dùng REST API endpoint, không dùng website endpoint Origin: storm-frontend-hosting-duc-2025.s3.ap-southeast-1.amazonaws.com 3. Invalidate cache của CloudFront aws cloudfront create-invalidation \\ --distribution-id E1234567890ABC \\ --paths \u0026#34;/*\u0026#34; Luồng dữ liệu A. Tải dữ liệu bão (khi khởi động ứng dụng) Trình duyệt → CloudFront → S3\r↓\rGET /recent_storms.json\r↓\rParse JSON → Hiển thị lên bản đồ File: src/pages/Index.tsx (dòng ~40)\nconst CLOUDFRONT_URL = import.meta.env.VITE_CLOUDFRONT_URL; const FETCH_URL = `${CLOUDFRONT_URL}/recent_storms.json?t=${Date.now()}`; B. Dự báo bão (khi người dùng thao tác) Người dùng điền form → Nhấn \u0026#34;Run Prediction\u0026#34;\r↓\rPOST /predict tới Lambda Function URL\r↓\r{\r\u0026#34;history\u0026#34;: [{lat, lng}, ...],\r\u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34;\r}\r↓\rLambda xử lý → Trả về dự báo\r↓\rHiển thị đường đi dự đoán lên bản đồ File: src/components/StormPredictionForm.tsx (dòng ~80)\nconst API_URL = `${import.meta.env.VITE_PREDICTION_API_URL}/predict`; const response = await fetch(API_URL, { method: \u0026#34;POST\u0026#34;, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }, body: JSON.stringify({ history, storm_name }) }); Các thành phần chính 1. Thành phần cốt lõi StormPredictionForm File: src/components/StormPredictionForm.tsx\nTính năng:\nForm nhập tọa độ bão (tối thiểu 9 điểm) Kiểm tra dữ liệu đầu vào (lat/lng hợp lệ) Gọi Lambda API để dự báo Hiển thị kết quả lên bản đồ Danh sách vị trí có thể cuộn, hỗ trợ thêm/xóa điểm thuộc tính truyền:\ninterface StormPredictionFormProps { onPredictionResult: (result: PredictionResult) =\u0026gt; void; setIsLoading: (isLoading: boolean) =\u0026gt; void; } WeatherMap File: src/components/WeatherMap.tsx\nTính năng:\nHiển thị bản đồ Leaflet Vẽ quỹ đạo bão (lịch sử + dự báo) Vẽ đường dự đoán (màu tím, nét đứt) Lớp phủ thời tiết (nhiệt độ, gió, radar) Hiển thị nhiều cơn bão cùng lúc Tự động zoom tới cơn bão đang được chọn Dùng các pane tùy chỉnh để quản lý thứ tự lớp (z-index) Props:\ninterface WeatherMapProps { storms: Storm[]; selectedStorm?: Storm; customPrediction?: PredictionResult | null; mapFocusBounds?: LatLngBounds | null; onMapFocusComplete?: () =\u0026gt; void; } Mục cần chụp màn hình:\nWeb UI → Bản đồ có quỹ đạo bão (xanh/đỏ) Web UI → Đường dự đoán tùy chỉnh (màu tím, nét đứt) Web UI → Lớp phủ thời tiết (nhiệt độ/gió) Index (Trang chính) File: src/pages/Index.tsx\nTính năng:\nBố cục chính với header/footer Quản lý state (storms, selectedStorm, customPrediction) Sidebar có tab (Current Storms / Predict Storm) Đồng bộ trạng thái timeline Xử lý loading và lỗi Skip link hỗ trợ truy cập (accessibility) 2. Các component về bão StormTracker File: src/components/StormTracker.tsx\nDanh sách các cơn bão hiện tại Lọc theo trạng thái (active/developing/dissipated) Bấm để chọn cơn bão StormInfo File: src/components/StormInfo.tsx\nThông tin chi tiết về bão Tốc độ gió, áp suất, phân loại Dữ liệu lịch sử Timeline dự báo StormAnimation File: src/components/StormAnimation.tsx\nMarker động cho các vị trí của bão Hiệu ứng nhấp nháy/pulsing Màu sắc theo cấp độ bão 3. Các component lớp thời tiết WeatherOverlay File: src/components/WeatherOverlay.tsx\nLớp phủ heatmap nhiệt độ Trực quan hóa tốc độ gió Dữ liệu thời gian thực từ OpenWeather API Rê chuột để xem giá trị WeatherLayerControl File: src/components/WeatherLayerControl.tsx\nLớp ảnh vệ tinh (satellite) Lớp radar Lớp nhiệt độ Quản lý các tile layer WeatherLayerControlPanel File: src/components/WeatherLayerControlPanel.tsx\nĐiều khiển UI cho các lớp thời tiết Thanh chỉnh độ trong suốt (opacity) Nút bật/tắt layer Bật/tắt animation cho nhiệt độ OptimizedTemperatureLayer \u0026amp; TemperatureHeatMapLayer Files: src/components/OptimizedTemperatureLayer.tsx, TemperatureHeatMapLayer.tsx\nRender nhiệt độ tối ưu hiệu năng Nội suy màu (color interpolation) Heatmap dạng lưới (grid-based) 4. Các component về gió WindyLayer File: src/components/WindyLayer.tsx\nTích hợp Windy.com bằng iframe Lớp phủ animation gió Đồng bộ trạng thái với bản đồ chính Context: src/contexts/WindyStateContext.tsx\nState toàn cục cho lớp Windy Lưu trạng thái vào URL Đồng bộ giữa các component 5. Component nâng cấp bản đồ ProvinceLayer File: src/components/ProvinceLayer.tsx\nRanh giới tỉnh/thành Việt Nam Render GeoJSON Nhãn tên tỉnh/thành WeatherValueTooltip File: src/components/WeatherValueTooltip.tsx\nTooltip hiển thị giá trị thời tiết khi rê chuột Nhiệt độ, tốc độ gió, áp suất Tooltip định vị theo vị trí con trỏ 6. Các component UI ThemeToggle File: src/components/ThemeToggle.tsx\nChuyển chế độ Sáng/Tối Lưu lại tùy chọn người dùng Tự nhận theme theo hệ thống PreferencesModal File: src/components/PreferencesModal.tsx\nThiết lập tùy chọn người dùng Tùy chọn bản đồ Tùy chọn hiển thị RightSidebar File: src/components/RightSidebar.tsx\nPanel thông tin bổ sung Sidebar có thể thu gọn WeeklyForecast File: src/components/WeeklyForecast.tsx\nDự báo thời tiết 7 ngày Xu hướng nhiệt độ Icon thời tiết 7. Các component timeline Thư mục: src/components/timeline/\nĐiều khiển timeline cho hoạt ảnh bão Chức năng Play/Pause Kéo để tua thời gian (scrubbing) Điều chỉnh tốc độ Kiểu dữ liệu PredictionResult File: src/lib/stormData.ts\nexport interface PredictionResult { storm_id: string; storm_name: string; prediction_time: string; totalDistance: number; // km actualDistance: number; // km lifespan: number; // giờ forecastHours: number; // giờ forecast: StormPoint[]; // Các điểm vị trí dự đoán path?: StormPoint[]; // Hỗ trợ tương thích (legacy) } StormPoint export interface StormPoint { timestamp: number; // Unix timestamp (ms) lat: number; lng: number; windSpeed: number; // km/h pressure: number; // hPa category: string; // \u0026#34;Typhoon\u0026#34;, \u0026#34;Super Typhoon\u0026#34;, ... } `md\nQuyền IAM/AWS cần thiết S3 Bucket Policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::storm-frontend-hosting-duc-2025/*\u0026#34; } ] } CloudFront Origin Access Origin: S3 bucket Origin Access: Public (hoặc OAI nếu dùng) Kiểm thử Local Development npm run dev # Mở http://localhost:5173 Kiểm thử bản build production npm run build npm run preview # Mở http://localhost:4173 Các lỗi thường gặp 1. Lỗi CORS khi gọi Lambda Đặc điểm: lỗi Access-Control-Allow-Origin\nCách xử lý: Lambda cần trả về header CORS:\nreturn { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } 2. CloudFront bị cache cũ Đặc điểm: Code mới không hiển thị\nCách xử lý: Invalidate cache\naws cloudfront create-invalidation --distribution-id E... --paths \u0026#34;/*\u0026#34; 3. Biến môi trường không được load Triệu chứng: undefined khi truy cập import.meta.env.VITE_*\nCách xử lý:\nĐảm bảo có file .env.production Build lại: npm run build Biến phải bắt đầu bằng VITE_ Checklist triển khai Cập nhật .env.production với đúng URL npm run build chạy thành công Upload dist/ lên S3 Invalidate CloudFront cache Test trên URL production Kiểm tra Lambda API hoạt động Kiểm tra dữ liệu bão tải được Test form dự đoán với 9+ điểm tọa độ API Endpoints 1. Lấy dữ liệu bão GET https://d3lj47ilp0fgxy.cloudfront.net/recent_storms.json Phản hồi: mảng các object Storm\n2. Dự đoán đường đi bão POST https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict Body: { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, ... ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } Response: { \u0026#34;storm_id\u0026#34;: \u0026#34;unknown\u0026#34;, \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34;, \u0026#34;totalDistance\u0026#34;: 500.5, \u0026#34;lifespan\u0026#34;: 72, \u0026#34;forecast\u0026#34;: [...] } Tối ưu hiệu năng 1. Tối ưu mã nguồn Code Splitting: Vite tự động tách chunk theo routes Tree Shaking: Loại bỏ code không dùng Minification: Build production tự nén JS/CSS Lazy Loading: Component được tải khi cần 2. Tối ưu dữ liệu Web Workers: Tính toán nặng chạy trong worker (dataWorker.ts) Memoization: Dùng React.memo cho component tốn tài nguyên Debouncing: Debounce cho handler input Caching: Lưu cache tùy chọn bằng LocalStorage 3. Tối ưu render Virtual Scrolling: Danh sách lớn dùng virtual scrolling Optimized Layers: OptimizedTemperatureLayer tối ưu hiệu năng Canvas Rendering: Heatmap render bằng canvas thay vì DOM Pane Management: Tạo Leaflet pane riêng để tối ưu z-index 4. Tối ưu mạng CDN Caching: CloudFront cache tài nguyên tĩnh Image Optimization: WebP, lazy loading API Caching: Cache dữ liệu bão kèm timestamp Compression: Nén Gzip/Brotli 5. Tối ưu cho khả năng truy cập (Accessibility) Skip Links: Phím tắt điều hướng bằng bàn phím ARIA Labels: Dùng nhãn ARIA và HTML ngữ nghĩa đúng Focus Management: Quản lý focus, đảm bảo thứ tự tab hợp lý Screen Reader: Tối ưu để hoạt động tốt với trình đọc màn hình Thư viện \u0026amp; tiện ích Business Logic (lib/) Quản lý bão stormData.ts: Kiểu dữ liệu, interface, định nghĩa Storm/StormPoint stormAnimations.ts: Logic animation cho marker bão stormIntensityChanges.ts: Tính toán thay đổi cường độ bão stormPerformance.ts: Tối ưu hiệu năng render stormValidation.ts: Kiểm tra/validate dữ liệu bão Hệ thống gió windData.ts: Cấu trúc dữ liệu gió windStrengthCalculations.ts: Tính toán cường độ gió windyStatePersistence.ts: Lưu trạng thái lớp Windy windyUrlState.ts: Quản lý trạng thái theo URL cho Windy Bản đồ \u0026amp; thời tiết mapUtils.ts: Tiện ích bản đồ (center, zoom, tính bounds) openWeatherMapClient.ts: Client gọi OpenWeather API colorInterpolation.ts: Tính toán gradient/nội suy màu Hiệu năng dataWorker.ts: Web Worker cho tác vụ tính toán nặng utils.ts: Tiện ích dùng chung Custom Hooks (hooks/) use-toast.ts: Hệ thống thông báo toast use-theme.tsx: Quản lý theme sáng/tối use-mobile.tsx: Nhận diện thiết bị mobile useTimelineState.ts: Đồng bộ trạng thái timeline useWindyStateSync.ts: Đồng bộ trạng thái lớp Windy useSimplifiedTooltip.ts: Logic tooltip rút gọn Context (contexts/) WindyStateContext.tsx: State toàn cục cho tích hợp lớp Windy Testing (test/) accessibility.test.ts: Kiểm thử accessibility accessibility-audit.test.ts: Audit theo WCAG wcag-compliance.test.ts: Kiểm thử tuân thủ WCAG 2.1 performance.test.ts: Benchmark hiệu năng cross-browser.test.ts: Kiểm thử tương thích đa trình duyệt setup.ts: Thiết lập môi trường test Dependencies Core React 18 TypeScript Vite (công cụ build) Vitest (framework test) UI Framework Tailwind CSS shadcn/ui (thư viện component) Lucide Icons Radix UI (primitives) Map \u0026amp; Visualization Leaflet React-Leaflet Hỗ trợ GeoJSON API \u0026amp; Data Fetch API (native) OpenWeather API AWS Lambda Function URL Quản lý state React Context API State theo URL (query params) Lưu trạng thái bằng LocalStorage Hiệu năng Web Workers Code splitting (Vite) Lazy loading Ảnh chụp tài liệu CloudFront Phân phối (Distribution) Hình 1\rCài đặt Origin (Origin Settings) Hình 1\rInvalidations Hình 2\rstorm-frontend-hosting-duc-2025 Hình 3\rPhân quyền (Permissions) Hình 4\rstorm-ai-models-2025 Hình 5\rstorm-data-store-2025 Hình 6\rTrang chính (Main Page) Hình 7\rTính năng theo dõi bão (Storm Tracking Features) Hình 8\rChi tiết cơn bão (Storm Details) Hình 9\rTính năng dự đoán (Predict Feature) Hình 10\rHình 11\rHình 12\r"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/",
	"title": "Kiến trúc Lambda",
	"tags": [],
	"description": "",
	"content": "Kiến trúc Lambda - Dịch vụ AI Dự báo Bão Tổng quan Hàm Lambda là một thành phần quan trọng trong kiến trúc serverless. Chúng đặc biệt hữu ích nhờ chi phí vận hành thấp và khả năng triển khai dễ dàng—những yếu tố rất phù hợp với nền tảng dự đoán bão của nhóm.\nPhần này trình bày chi tiết cách chúng tôi thiết kế và xây dựng kiến trúc Lambda.\nCác hàm Lambda của chúng em chạy mô hình PyTorch để dự đoán quỹ đạo bão và được triển khai thông qua Docker container image.\nKiến trúc các dịch vụ AWS ┌─────────────────────────────────────────────────────────────┐ │ Frontend (Trình duyệt) │ └────────────────────────┬────────────────────────────────────┘ │ POST /predict ▼ ┌─────────────────────────────────────────────────────────────┐ │ Lambda Function URL (Công khai) │ │ URL: https://vill3povlzqxdyxm7ubldizobu0kdgbi... │ │ Auth: NONE (không xác thực) │ │ Method: POST │ └────────────────────────┬────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────┐ │ Lambda Function │ │ Tên: storm-prediction │ │ Runtime: Python 3.10 (Container) │ │ Bộ nhớ: 3008 MB │ │ Timeout: 120 giây │ │ Kiến trúc: x86_64 │ └────────────────────────┬────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────┐ │ ECR Repository │ │ Account: 339570693867 │ │ Region: ap-southeast-1 │ │ Repo: storm-prediction │ │ Image: latest │ │ Size: ~2 GB │ └────────────────────────┬────────────────────────────────────┘ ┌─────────────────────────────────────────────────────────────┐ │ S3 Buckets │ │ 1. storm-frontend-hosting-duc-2025 │ │ - models/lstm_totald_256_4.pt (tùy chọn) │ │ - predictions/[storm_id]_[timestamp].json │ │ │ │ 2. storm-ai-models (khuyến nghị) │ │ - models/lstm_totald_256_4.pt │ │ - models/tcn_model.pth (backup) │ └─────────────────────────────────────────────────────────────┘ Cấu trúc thư mục storm_prediction/ storm_prediction/ ├── app.py # Lambda handler (mã chính) ├── Dockerfile # Định nghĩa container ├── requirements.txt # Thư viện Python phụ thuộc ├── cropping_storm_7304_2l.pth # Mô hình TCN (đóng kèm trong image) │ ├── DEPLOY_NOW.md # Hướng dẫn deploy nhanh ├── DEPLOY_CONSOLE_STEP_BY_STEP.md # Hướng dẫn AWS Console từng bước ├── LAMBDA_DEPLOYMENT_GUIDE.md # Hướng dẫn triển khai chi tiết ├── AWS_CONSOLE_DEPLOYMENT_GUIDE.md ├── FIX_ECR_PUSH_ERROR.md # Tài liệu xử lý lỗi ECR ├── FIX_UNICODE_ERROR.md # Sửa lỗi UnicodeDecodeError ├── FIX_UNICODE_ERROR_SOLUTION.md # Chi tiết giải pháp └── REBUILD_AND_DEPLOY.sh # Script tự động build \u0026amp; deploy Cấu trúc Docker Image Dockerfile FROM public.ecr.aws/lambda/python:3.10 # Cài đặt dependencies COPY requirements.txt . RUN pip3 install -r requirements.txt \\ --target \u0026#34;${LAMBDA_TASK_ROOT}\u0026#34; \\ --extra-index-url https://download.pytorch.org/whl/cpu # Copy Lambda handler COPY app.py ${LAMBDA_TASK_ROOT} # Copy mô hình TCN vào thư mục con (tránh nhầm file .pth) RUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ # Đặt handler CMD [ \u0026#34;app.handler\u0026#34; ] Các lớp (Image Layers) Layer 1: AWS Lambda Python 3.10 base (~500 MB) Layer 2: PyTorch CPU + dependencies (~1.2 GB) Layer 3: app.py + mô hình TCN (~300 MB) ───────────────────────────────────────────── Tổng dung lượng: ~2 GB Các mô hình AI 1. Mô hình TCN (Dự đoán quỹ đạo) File: cropping_storm_7304_2l.pth\nVị trí: Bên trong Docker image tại /var/task/models/\nDung lượng: ~300 MB\nMục đích: Dự đoán bước tiếp theo (lat, lng) của quỹ đạo bão\nKiến trúc:\nclass StormTCN(nn.Module): def __init__(self, input_dim=4, hidden_units=1024, num_layers=2): self.tcn = TCN(...) self.head_latlon = nn.Linear(hidden_units, 2) # Dự đoán lat, lng self.head_aux = nn.Linear(hidden_units, 2) # Dự đoán đặc trưng phụ Input: [batch, sequence, 4] - (lat, lng, distance, bearing)\nOutput:\npred_latlon: (lat, lng) kế tiếp pred_aux: đặc trưng phụ 2. Mô hình LSTM (Dự đoán tổng quãng đường) File: lstm_totald_256_4.pt\nVị trí: S3 bucket (tải về ở lần chạy đầu tiên)\nDung lượng: ~50 MB\nMục đích: Dự đoán tổng quãng đường bão sẽ di chuyển\nKiến trúc:\nclass StormLSTM(nn.Module): def __init__(self, input_size=4, hidden_size=256, num_layers=2): self.lstm = nn.LSTM(...) self.fc = nn.Sequential( nn.Linear(hidden_size, hidden_size // 2), nn.ReLU(), nn.Linear(hidden_size // 2, 1) # Dự đoán tổng quãng đường ) Input: Tổng hợp theo ngày [batch, days, 4] - (day, daily_dist, avg_speed, motion_type)\nOutput: Tổng quãng đường (km)\nLuồng xử lý request 1. Nhận request POST /predict Content-Type: application/json { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, ... // Tối thiểu 9 điểm ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34;, \u0026#34;storm_id\u0026#34;: \u0026#34;TEST001\u0026#34; // Tùy chọn } 2. Tải mô hình (chỉ lần gọi đầu tiên) def load_models(): global LSTM_MODEL, TCN_MODEL # Tải LSTM từ S3 (nếu có) if not os.path.exists(\u0026#39;/tmp/lstm_model.pt\u0026#39;): s3_client.download_file( MODEL_BUCKET, \u0026#39;models/lstm_totald_256_4.pt\u0026#39;, \u0026#39;/tmp/lstm_model.pt\u0026#39; ) LSTM_MODEL = StormLSTM(...) LSTM_MODEL.load_state_dict(torch.load(\u0026#39;/tmp/lstm_model.pt\u0026#39;)) # Tải TCN từ local (đã có sẵn trong image) TCN_MODEL = StormTCN(...) TCN_MODEL.load_state_dict( torch.load(\u0026#39;/var/task/models/cropping_storm_7304_2l.pth\u0026#39;) ) 4. Dự đoán tổng quãng đường (LSTM) def predict_total_distance(record_tensor): if LSTM_MODEL is None: # Dự phòng (fallback): avg_distance * 24 bước return fallback_distance # Gom theo ngày (9 điểm/ngày) # Chạy dự đoán bằng LSTM with torch.no_grad(): pred = LSTM_MODEL(summary_tensor, lengths) return pred.item() # km 5. Dự đoán đường đi (TCN) def predict_storm_path(record_tensor, total_distance, history): seq = record_tensor.clone() gone_distance = 0 predicted_points = [] while gone_distance \u0026lt; total_distance: # Dự đoán vị trí tiếp theo pred_latlon, pred_aux = TCN_MODEL(seq) new_lat = pred_latlon[0, -1, 0].item() new_lng = pred_latlon[0, -1, 1].item() # Tính khoảng cách \u0026amp; hướng di chuyển step_distance = haversine(last_lat, last_lng, new_lat, new_lng) # Ước lượng tốc độ gió (giảm dần theo thời gian) estimated_wind = max(avg_wind * (0.98 ** step), 30) predicted_points.append({ \u0026#39;lat\u0026#39;: new_lat, \u0026#39;lng\u0026#39;: new_lng, \u0026#39;timestamp\u0026#39;: base_timestamp + (step * 3 * 3600 * 1000), \u0026#39;windSpeed\u0026#39;: estimated_wind, \u0026#39;pressure\u0026#39;: 980.0, \u0026#39;category\u0026#39;: calculate_category(estimated_wind) }) # Cập nhật chuỗi (sliding window) seq = torch.cat([seq[:, 1:, :], next_point.unsqueeze(1)], dim=1) gone_distance += step_distance step += 1 return predicted_points 6. Trả về response result = { \u0026#39;storm_id\u0026#39;: storm_id, \u0026#39;storm_name\u0026#39;: storm_name, \u0026#39;prediction_time\u0026#39;: datetime.now().isoformat(), \u0026#39;totalDistance\u0026#39;: 500.5, \u0026#39;actualDistance\u0026#39;: 520.3, \u0026#39;lifespan\u0026#39;: 72, \u0026#39;forecastHours\u0026#39;: 72, \u0026#39;forecast\u0026#39;: [ { \u0026#39;lat\u0026#39;: 15.1, \u0026#39;lng\u0026#39;: 106.99, \u0026#39;timestamp\u0026#39;: 1765015351626, \u0026#39;windSpeed\u0026#39;: 65, \u0026#39;pressure\u0026#39;: 980, \u0026#39;category\u0026#39;: \u0026#39;Typhoon\u0026#39; }, ... ] } return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } Quy trình Build \u0026amp; Deploy Bước 1: Build Docker Image cd storm_prediction docker build \\ --provenance=false \\ --platform linux/amd64 \\ -t storm-prediction-model . Giải thích flags:\n--provenance=false: Giảm kích thước image (không kèm metadata build) --platform linux/amd64: Lambda chỉ hỗ trợ x86_64 -t storm-prediction-model: Tên tag của image Bước 2: Tag để push lên ECR docker tag storm-prediction-model:latest \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Bước 3: Đăng nhập ECR aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com Bước 4: Push image lên ECR docker push \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Thời gian: ~5–10 phút (upload ~2GB)\nBước 5: Cập nhật Lambda Function Trên AWS Console:\nLambda → storm-prediction Tab Image → Deploy new image Chọn image latest Nhấn Save Cấu hình Lambda Thiết lập Function Name: storm-prediction Runtime: Container image Architecture: x86_64 Memory: 3008 MB Timeout: 120 seconds Ephemeral storage: 512 MB Biến môi trường MODEL_BUCKET=storm-frontend-hosting-duc-2025 DATA_BUCKET=storm-frontend-hosting-duc-2025 Function URL URL: https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws Auth type: NONE CORS: Enabled - Allow origins: * - Allow methods: POST, OPTIONS - Allow headers: Content-Type Quyền IAM Role { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::storm-frontend-hosting-duc-2025/*\u0026#34;, \u0026#34;arn:aws:s3:::storm-ai-models/*\u0026#34; ] } ] } Giám sát \u0026amp; Logs CloudWatch Logs Log Group: /aws/lambda/storm-prediction\nCác log quan trọng:\nLoading LSTM model... Downloaded LSTM from S3 LSTM loaded successfully Loading TCN model... Checking: /var/task/models/cropping_storm_7304_2l.pth Found TCN at /var/task/models/cropping_storm_7304_2l.pth TCN loaded successfully Processing: Test Storm (TEST001) Input points: 9 Predicted total distance: 500.50 km Generated 24 predictions (72 hours) Saved to S3: predictions/TEST001_1733486400.json Metrics CloudWatch Metrics:\nInvocations Duration (trung bình ~5–10 giây) Errors Throttles Memory used (~500–800 MB) Lỗi thường gặp \u0026amp; cách xử lý 1. UnicodeDecodeError: \u0026lsquo;utf-8\u0026rsquo; codec can\u0026rsquo;t decode byte 0x80 Triệu chứng:\nUnicodeDecodeError: \u0026#39;utf-8\u0026#39; codec can\u0026#39;t decode byte 0x80 in position 64 Nguyên nhân: file model .pth ở thư mục gốc bị Lambda hiểu nhầm như file cấu hình Python\nCách xử lý: chuyển model vào thư mục con\nRUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ 2. 502 Bad Gateway Triệu chứng: Frontend nhận lỗi 502\nNguyên nhân có thể:\nLambda timeout (quá 120s) Lambda crash (hết bộ nhớ) Load model thất bại Cách xử lý:\nKiểm tra CloudWatch Logs Tăng memory nếu cần Tăng timeout nếu cần 3. LSTM Fallback Đặc điểm: log có \u0026quot; Using fallback distance\u0026quot;\nNguyên nhân: chưa có model LSTM trên S3\nCách xử lý: upload lstm_totald_256_4.pt lên S3\naws s3 cp lstm_totald_256_4.pt \\ s3://storm-frontend-hosting-duc-2025/models/ 4. ECR Push 403 Forbidden Đặc điểm: 403 Forbidden khi push image\nNguyên nhân có thể:\nHết hạn đăng nhập ECR Sai account ID Repo chưa tồn tại Cách xử lý:\n# Đăng nhập lại aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com # Tạo repository nếu cần aws ecr create-repository \\ --repository-name storm-prediction \\ --region ap-southeast-1 Kiểm thử Test local (nếu có thể) # Chạy local python app.py # Test event test_event = { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, ... ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } result = handler(test_event, None) print(result) Test trực tiếp trên Lambda Trên AWS Console:\nLambda → tab Test Tạo test event: { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, {\u0026#34;lat\u0026#34;: 15.2, \u0026#34;lng\u0026#34;: 120.2}, {\u0026#34;lat\u0026#34;: 15.3, \u0026#34;lng\u0026#34;: 120.3}, {\u0026#34;lat\u0026#34;: 15.4, \u0026#34;lng\u0026#34;: 120.4}, {\u0026#34;lat\u0026#34;: 15.5, \u0026#34;lng\u0026#34;: 120.5}, {\u0026#34;lat\u0026#34;: 15.6, \u0026#34;lng\u0026#34;: 120.6}, {\u0026#34;lat\u0026#34;: 15.7, \u0026#34;lng\u0026#34;: 120.7}, {\u0026#34;lat\u0026#34;: 15.8, \u0026#34;lng\u0026#34;: 120.8} ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } Nhấn Test Kiểm tra response Test bằng cURL bash\rcurl -X POST \\\r\u0026quot;https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict\u0026quot; \\\r-H \u0026quot;Content-Type: application/json\u0026quot; \\\r-d '{\r\u0026quot;history\u0026quot;: [\r{\u0026quot;lat\u0026quot;: 15.0, \u0026quot;lng\u0026quot;: 120.0},\r{\u0026quot;lat\u0026quot;: 15.1, \u0026quot;lng\u0026quot;: 120.1},\r{\u0026quot;lat\u0026quot;: 15.2, \u0026quot;lng\u0026quot;: 120.2},\r{\u0026quot;lat\u0026quot;: 15.3, \u0026quot;lng\u0026quot;: 120.3},\r{\u0026quot;lat\u0026quot;: 15.4, \u0026quot;lng\u0026quot;: 120.4},\r{\u0026quot;lat\u0026quot;: 15.5, \u0026quot;lng\u0026quot;: 120.5},\r{\u0026quot;lat\u0026quot;: 15.6, \u0026quot;lng\u0026quot;: 120.6},\r{\u0026quot;lat\u0026quot;: 15.7, \u0026quot;lng\u0026quot;: 120.7},\r{\u0026quot;lat\u0026quot;: 15.8, \u0026quot;lng\u0026quot;: 120.8}\r],\r\u0026quot;storm_name\u0026quot;: \u0026quot;Test Storm\u0026quot;\r}'\rChecklist triển khai File model cropping_storm_7304_2l.pth tồn tại (Tùy chọn) Upload model LSTM lên S3 Build Docker image thành công Tag image đúng account ID (339570693867) Login ECR thành công Push image lên ECR Update Lambda function với image mới Kiểm tra cấu hình Lambda (memory, timeout) Test Lambda với test event Test qua Function URL bằng cURL Test từ frontend Kiểm tra CloudWatch Logs Xác nhận kết quả dự đoán hiển thị trên bản đồ Ảnh chụp Hình 1\rCấu hình (Configuration) Hình 2\rBiến môi trường (Environment Variables) Hình 3\rECR Kho lưu trữ (Repository) Hình 4\rHình 5\r"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.2-fix-unicode-error-solution/",
	"title": "Sửa lỗi Unicode",
	"tags": [],
	"description": "",
	"content": "Giải quyết lỗi UnicodeDecodeError trong Lambda Đây là một lỗi quan trọng mà nhóm gặp phải trong quá trình xây dựng nền tảng và tích hợp model nên dành riêng một mục để nói về nó. Chi tiết xe ở bên dưới.\nVấn đề UnicodeDecodeError: \u0026#39;utf-8\u0026#39; codec can\u0026#39;t decode byte 0x80 in position 64: invalid start byte Nguyên nhân Model PyTorch có extension .pth (binary file) Python runtime cũng sử dụng .pth files cho path configuration (text files) Khi đặt model .pth trực tiếp trong LAMBDA_TASK_ROOT, Python cố đọc nó như text → lỗi Giải pháp đã áp dụng 1. Sửa Dockerfile Di chuyển model vào thư mục con models/:\nRUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ 2. Sửa app.py Update đường dẫn tìm model:\npossible_paths = [ \u0026#39;/var/task/models/cropping_storm_7304_2l.pth\u0026#39;, \u0026#39;models/cropping_storm_7304_2l.pth\u0026#39;, tcn_path ] Các bước rebuild và deploy Bước 1: Build Docker image cd storm_prediction docker build --provenance=false --platform linux/amd64 -t storm-prediction-model . Lưu ý: --provenance=false giúp giảm kích thước image để push lên ECR\nBước 2: Tag image docker tag storm-prediction-model:latest 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Bước 3: Login ECR aws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com Bước 4: Push to ECR docker push 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Bước 5: Update Lambda Vào AWS Console → Lambda → storm-prediction Click tab Image Click Deploy new image Chọn image mới nhất Click Save Bước 6: Test curl -X POST \u0026#34;https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 14.5, \u0026#34;lng\u0026#34;: 121.0}, {\u0026#34;lat\u0026#34;: 14.6, \u0026#34;lng\u0026#34;: 121.1}, {\u0026#34;lat\u0026#34;: 14.7, \u0026#34;lng\u0026#34;: 121.2}, {\u0026#34;lat\u0026#34;: 14.8, \u0026#34;lng\u0026#34;: 121.3}, {\u0026#34;lat\u0026#34;: 14.9, \u0026#34;lng\u0026#34;: 121.4}, {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 121.5}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 121.6}, {\u0026#34;lat\u0026#34;: 15.2, \u0026#34;lng\u0026#34;: 121.7}, {\u0026#34;lat\u0026#34;: 15.3, \u0026#34;lng\u0026#34;: 121.8} ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; }\u0026#39; Kiểm tra logs sau khi deploy aws logs tail /aws/lambda/storm-prediction --region ap-southeast-1 --follow Tóm tắt Trước: Model .pth ở root → Python nhầm là config file → UnicodeDecodeError Sau: Model .pth ở models/ → Python bỏ qua → Lambda hoạt động ✅ "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.1-ai-model-integration/",
	"title": "Tích Hợp AI",
	"tags": [],
	"description": "",
	"content": "Tiến hành tích hợp AI Model từ Lambda Tổng quan Dưới đây xin trình bày về cách mà nhóm đã tích hợp mô hình AI dự đoán bão từ lambda để dùng theo từng bước.\nCác file đang dùng Mock Data 1. WeatherOverlay.tsx (QUAN TRỌNG) Vị trí: frontend/src/components/WeatherOverlay.tsx Mock data: Temperature và Wind overlay data Function: generateWeatherData() Cần sửa: Thay thế bằng API call đến Lambda 2. WeeklyForecast.tsx Vị trí: frontend/src/components/WeeklyForecast.tsx Mock data: mockForecast array Cần sửa: Fetch từ backend API 3. windData.ts Vị trí: frontend/src/lib/windData.ts Mock data: mockWindData Cần sửa: Fetch từ OpenWeatherMap hoặc Lambda 4. WindFieldManager.ts Vị trí: frontend/src/components/wind/WindFieldManager.ts Mock data: Fallback khi không có API key Đã OK: Có logic fetch từ OpenWeatherMap, chỉ cần config API key Cách tích hợp AI Model từ Lambda Bước 1: Thêm API endpoint cho Storm Prediction Trong file frontend/src/api/weatherApi.ts, thêm:\nexport interface StormPrediction { stormId: string; name: string; nameVi: string; currentPosition: { lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; }; historicalTrack: Array\u0026lt;{ lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; }\u0026gt;; forecastTrack: Array\u0026lt;{ lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; confidence?: number; // Độ tin cậy từ AI model }\u0026gt;; } export const weatherApi = { // ... existing methods ... // Lấy dự đoán bão từ Lambda AI model getStormPredictions: async (): Promise\u0026lt;StormPrediction[]\u0026gt; =\u0026gt; { const response = await api.get\u0026lt;StormPrediction[]\u0026gt;(\u0026#39;/storms/predictions\u0026#39;); return response.data; }, // Lấy chi tiết một cơn bão cụ thể getStormById: async (stormId: string): Promise\u0026lt;StormPrediction\u0026gt; =\u0026gt; { const response = await api.get\u0026lt;StormPrediction\u0026gt;(`/storms/${stormId}`); return response.data; }, }; Bước 2: Cập nhật Backend để gọi Lambda Trong backend C# (backend/Controllers/WeatherController.cs), thêm endpoint:\n[HttpGet(\u0026#34;storms/predictions\u0026#34;)] public async Task\u0026lt;IActionResult\u0026gt; GetStormPredictions() { try { // Gọi Lambda function var lambdaClient = new AmazonLambdaClient(); var request = new InvokeRequest { FunctionName = \u0026#34;storm-prediction-function\u0026#34;, InvocationType = InvocationType.RequestResponse, Payload = \u0026#34;{}\u0026#34; // Hoặc parameters nếu cần }; var response = await lambdaClient.InvokeAsync(request); using var reader = new StreamReader(response.Payload); var result = await reader.ReadToEndAsync(); return Ok(JsonSerializer.Deserialize\u0026lt;List\u0026lt;StormPrediction\u0026gt;\u0026gt;(result)); } catch (Exception ex) { return StatusCode(500, new { error = ex.Message }); } } Bước 3: Cập nhật Frontend để dùng API thật Trong frontend/src/pages/Index.tsx hoặc nơi fetch storm data:\nimport { weatherApi } from \u0026#39;../api/weatherApi\u0026#39;; import { useQuery } from \u0026#39;@tanstack/react-query\u0026#39;; // Thay vì dùng mock data const { data: storms, isLoading } = useQuery({ queryKey: [\u0026#39;storms\u0026#39;], queryFn: () =\u0026gt; weatherApi.getStormPredictions(), refetchInterval: 5 * 60 * 1000, // Refresh mỗi 5 phút }); Bước 4: Cấu hình Environment Variables Frontend (.env.production):\nVITE_API_BASE_URL=https://your-backend-api.com/api/weather Backend (appsettings.json):\n{ \u0026#34;AWS\u0026#34;: { \u0026#34;Region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;LambdaFunctionName\u0026#34;: \u0026#34;storm-prediction-function\u0026#34; } } Checklist Deploy Deploy AI model lên Lambda Test Lambda function với sample input Thêm API endpoint trong backend C# Test backend endpoint Cập nhật weatherApi.ts với endpoints mới Thay thế mock data bằng API calls Test frontend với data thật Cập nhật .env.production với URL production Build và deploy frontend Monitor logs và errors Files không cần sửa (chỉ là examples) Các file này chỉ là demo/example, không ảnh hưởng production:\n*.example.tsx */__tests__/* */GUIDE.md "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.3-lambda-deployment/",
	"title": "Triển Khai Lamda",
	"tags": [],
	"description": "",
	"content": "Các bước Deploy PyTorch Model dự đoán bão lên AWS Lambda Việc triển khai Lambda là một phần quan trọng trong quy trình phát triển website của nhóm. Mục sau đây sẽ giải thích các bước chúng em đã thực hiện để hoàn thành quy trình này.\nChuẩn bị Code 1.1. Sửa app.py\nimport json import torch import numpy as np from typing import List, Dict # Load model khi Lambda khởi động (reuse across invocations) MODEL_PATH = \u0026#34;model.pth\u0026#34; device = torch.device(\u0026#34;cpu\u0026#34;) # Lambda không có GPU model = None def load_model(): global model if model is None: print(f\u0026#34;Loading model from {MODEL_PATH}...\u0026#34;) model = torch.load(MODEL_PATH, map_location=device) model.eval() print(\u0026#34;Model loaded successfully!\u0026#34;) return model def prepare_features(history: List[Dict]) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34; Chuyển đổi history thành tensor cho model history: [{\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 107.0}, ...] \u0026#34;\u0026#34;\u0026#34; # TODO: Implement feature engineering theo model của bạn lats = [p[\u0026#34;lat\u0026#34;] for p in history] lngs = [p[\u0026#34;lng\u0026#34;] for p in history] # Ví dụ: normalize và reshape features = np.array([lats + lngs]) # Shape: (1, 18) return torch.tensor(features, dtype=torch.float32) def format_predictions(predictions: torch.Tensor, storm_name: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Format output theo cấu trúc frontend cần \u0026#34;\u0026#34;\u0026#34; # TODO: Implement theo output của model pred_array = predictions.detach().cpu().numpy()[0] # Giả sử model predict 10 điểm tiếp theo (lat, lng) forecast = [] base_timestamp = int(time.time() * 1000) for i in range(0, len(pred_array), 2): if i + 1 \u0026lt; len(pred_array): forecast.append({ \u0026#34;lat\u0026#34;: float(pred_array[i]), \u0026#34;lng\u0026#34;: float(pred_array[i + 1]), \u0026#34;timestamp\u0026#34;: base_timestamp + (i // 2) * 3600000, # +1 hour each \u0026#34;windSpeed\u0026#34;: 120.0, # TODO: Predict từ model \u0026#34;pressure\u0026#34;: 980.0, # TODO: Predict từ model \u0026#34;category\u0026#34;: \u0026#34;Category 3\u0026#34;, # TODO: Classify từ windSpeed \u0026#34;confidence\u0026#34;: 0.85 }) return { \u0026#34;storm_name\u0026#34;: storm_name, \u0026#34;forecast\u0026#34;: forecast } def handler(event, context): \u0026#34;\u0026#34;\u0026#34; Lambda handler function \u0026#34;\u0026#34;\u0026#34; try: # Parse input body = json.loads(event.get(\u0026#39;body\u0026#39;, \u0026#39;{}\u0026#39;)) history = body.get(\u0026#39;history\u0026#39;, []) storm_name = body.get(\u0026#39;storm_name\u0026#39;, \u0026#39;Unknown Storm\u0026#39;) # Validate input if len(history) \u0026lt; 9: return { \u0026#39;statusCode\u0026#39;: 400, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: f\u0026#39;Need at least 9 positions, got {len(history)}\u0026#39; }) } # Load model model = load_model() # Prepare features X = prepare_features(history) # Predict with torch.no_grad(): predictions = model(X) # Format output result = format_predictions(predictions, storm_name) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } except Exception as e: print(f\u0026#34;Error: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: str(e) }) } 1.2. Sửa Dockerfile\nFROM public.ecr.aws/lambda/python:3.11 # Copy requirements và install COPY requirements.txt ${LAMBDA_TASK_ROOT} RUN pip install --no-cache-dir -r requirements.txt # Copy model (đổi tên thành model.pth) COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/model.pth # Copy code COPY app.py ${LAMBDA_TASK_ROOT} # Set handler CMD [\u0026#34;app.handler\u0026#34;] 1.3. Kiểm tra requirements.txt\ntorch==2.1.0 numpy==1.24.3 Bước 2: Build Docker Image cd storm_prediction # Build image docker build -t storm-prediction-model . # Test local (optional) docker run -p 9000:8080 storm-prediction-model # Test với curl curl -X POST \u0026#34;http://localhost:9000/2015-03-31/functions/function/invocations\u0026#34; \\ -d \u0026#39;{ \u0026#34;body\u0026#34;: \u0026#34;{\\\u0026#34;history\\\u0026#34;: [{\\\u0026#34;lat\\\u0026#34;: 15.0, \\\u0026#34;lng\\\u0026#34;: 107.0}, {\\\u0026#34;lat\\\u0026#34;: 15.1, \\\u0026#34;lng\\\u0026#34;: 107.1}, {\\\u0026#34;lat\\\u0026#34;: 15.2, \\\u0026#34;lng\\\u0026#34;: 107.2}, {\\\u0026#34;lat\\\u0026#34;: 15.3, \\\u0026#34;lng\\\u0026#34;: 107.3}, {\\\u0026#34;lat\\\u0026#34;: 15.4, \\\u0026#34;lng\\\u0026#34;: 107.4}, {\\\u0026#34;lat\\\u0026#34;: 15.5, \\\u0026#34;lng\\\u0026#34;: 107.5}, {\\\u0026#34;lat\\\u0026#34;: 15.6, \\\u0026#34;lng\\\u0026#34;: 107.6}, {\\\u0026#34;lat\\\u0026#34;: 15.7, \\\u0026#34;lng\\\u0026#34;: 107.7}, {\\\u0026#34;lat\\\u0026#34;: 15.8, \\\u0026#34;lng\\\u0026#34;: 107.8}], \\\u0026#34;storm_name\\\u0026#34;: \\\u0026#34;Test Storm\\\u0026#34;}\u0026#34; }\u0026#39; Bước 3: Upload lên AWS ECR # 1. Tạo ECR repository aws ecr create-repository \\ --repository-name storm-prediction-model \\ --region ap-southeast-1 # 2. Đăng nhập Docker vào ECR aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com # 3. Tag image docker tag storm-prediction-model:latest \\ \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest # 4. Push image docker push \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest Lưu ý: Thay \u0026lt;account-id\u0026gt; bằng AWS Account ID của bạn.\nBước 4: Tạo Lambda Function 4.1. Tạo Lambda từ Console\nVào AWS Lambda Console Click \u0026ldquo;Create function\u0026rdquo; Chọn \u0026ldquo;Container image\u0026rdquo; Function name: storm-prediction Container image URI: Chọn image vừa push lên ECR Architecture: x86_64 Click \u0026ldquo;Create function\u0026rdquo; 4.2. Cấu hình Lambda\n# Hoặc dùng AWS CLI aws lambda create-function \\ --function-name storm-prediction \\ --package-type Image \\ --code ImageUri=\u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest \\ --role arn:aws:iam::\u0026lt;account-id\u0026gt;:role/lambda-execution-role \\ --timeout 60 \\ --memory-size 3008 \\ --region ap-southeast-1 Cấu hình quan trọng:\nMemory: 3008 MB (PyTorch model cần nhiều RAM) Timeout: 60 seconds (model inference có thể mất 10-30s) Ephemeral storage: 512 MB (default, tăng nếu cần) 4.3. Tạo IAM Role\nLambda cần role với permissions:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:GetDownloadUrlForLayer\u0026#34;, \u0026#34;ecr:BatchGetImage\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Bước 5: Tạo API Gateway # 1. Tạo REST API aws apigateway create-rest-api \\ --name storm-prediction-api \\ --region ap-southeast-1 # 2. Lấy API ID và Root Resource ID API_ID=\u0026lt;your-api-id\u0026gt; ROOT_ID=\u0026lt;your-root-resource-id\u0026gt; # 3. Tạo resource /predict aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part predict # 4. Tạo POST method RESOURCE_ID=\u0026lt;predict-resource-id\u0026gt; aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $RESOURCE_ID \\ --http-method POST \\ --authorization-type NONE # 5. Integrate với Lambda aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $RESOURCE_ID \\ --http-method POST \\ --type AWS_PROXY \\ --integration-http-method POST \\ --uri arn:aws:apigateway:ap-southeast-1:lambda:path/2015-03-31/functions/arn:aws:lambda:ap-southeast-1:\u0026lt;account-id\u0026gt;:function:storm-prediction/invocations # 6. Deploy API aws apigateway create-deployment \\ --rest-api-id $API_ID \\ --stage-name prod API URL: https://\u0026lt;api-id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/prod/predict\nBước 6: Cập nhật Frontend 6.1. Cập nhật .env.production\nVITE_PREDICTION_API_URL=https://\u0026lt;api-id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/prod 6.2. Build và deploy frontend\ncd frontend npm run build # Deploy dist/ lên S3/CloudFront Tối ưu hóa 1. Giảm Cold Start Provisioned Concurrency:\naws lambda put-provisioned-concurrency-config \\ --function-name storm-prediction \\ --provisioned-concurrent-executions 1 \\ --qualifier $LATEST 2. Giảm kích thước Image Dùng PyTorch CPU-only:\n# requirements.txt torch==2.1.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu numpy==1.24.3 Multi-stage build:\n# Stage 1: Build FROM python:3.11-slim as builder COPY requirements.txt . RUN pip install --target /packages -r requirements.txt # Stage 2: Runtime FROM public.ecr.aws/lambda/python:3.11 COPY --from=builder /packages ${LAMBDA_RUNTIME_DIR} COPY model.pth ${LAMBDA_TASK_ROOT}/ COPY app.py ${LAMBDA_TASK_ROOT}/ CMD [\u0026#34;app.handler\u0026#34;] 3. Cache Model trong /tmp import os MODEL_PATH = \u0026#34;/tmp/model.pth\u0026#34; if os.path.exists(\u0026#34;/tmp/model.pth\u0026#34;) else \u0026#34;model.pth\u0026#34; def load_model(): global model if model is None: # Copy to /tmp for faster access if not os.path.exists(\u0026#34;/tmp/model.pth\u0026#34;): import shutil shutil.copy(\u0026#34;model.pth\u0026#34;, \u0026#34;/tmp/model.pth\u0026#34;) model = torch.load(\u0026#34;/tmp/model.pth\u0026#34;, map_location=device) model.eval() return model Monitoring CloudWatch Logs # Xem logs aws logs tail /aws/lambda/storm-prediction --follow CloudWatch Metrics Invocations: Số lần gọi Duration: Thời gian chạy Errors: Số lỗi Throttles: Số lần bị throttle Alerts # Tạo alarm cho errors aws cloudwatch put-metric-alarm \\ --alarm-name storm-prediction-errors \\ --alarm-description \u0026#34;Alert when Lambda has errors\u0026#34; \\ --metric-name Errors \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 300 \\ --threshold 5 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=storm-prediction Troubleshooting Lỗi: \u0026ldquo;Task timed out after 3.00 seconds\u0026rdquo; Giải pháp: Tăng timeout lên 60s\nLỗi: \u0026ldquo;Runtime exited with error: signal: killed\u0026rdquo; Giải pháp: Tăng memory lên 3008 MB\nLỗi: \u0026ldquo;No module named \u0026rsquo;torch'\u0026rdquo; Giải pháp: Kiểm tra requirements.txt và rebuild image\nLỗi: Model không load được Giải pháp: Kiểm tra tên file model trong Dockerfile và app.py\nChi phí ước tính Lambda:\nFree tier: 1M requests/month, 400,000 GB-seconds Sau đó: $0.20 per 1M requests + $0.0000166667 per GB-second Ví dụ: 10,000 requests/month, mỗi request 10s, 3GB RAM\nCompute: 10,000 × 10s × 3GB × $0.0000166667 = $5/month Requests: 10,000 × $0.20/1M = $0.002/month Total: ~$5/month API Gateway:\n$3.50 per million requests 10,000 requests = $0.035/month ECR:\n$0.10 per GB/month storage Image ~2GB = $0.20/month Total ước tính: ~$5.25/month cho 10,000 predictions\nChecklist cuối cùng Sửa tên file model trong app.py hoặc Dockerfile Test Docker image locally Push image lên ECR Tạo Lambda function với memory 3008MB, timeout 60s Tạo API Gateway và integrate với Lambda Test API với Postman/curl Cập nhật VITE_PREDICTION_API_URL trong frontend Build và deploy frontend Test form prediction trên web Setup CloudWatch alerts Monitor logs và performance "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "NỀN TẢNG THEO DÕI VÀ DỰ ĐOÁN BÃO Tổng Quan Bão là một trong những thảm họa thiên nhiên nguy hiểm nhất, có thể gây thiệt hại nghiêm trọng đến cơ sở hạ tầng và đe dọa tính mạng con người. Việc phát hiện sớm và đưa ra cảnh báo kịp thời là vô cùng quan trọng để người dân trong khu vực bị ảnh hưởng có đủ thời gian chuẩn bị và sơ tán an toàn.\nĐể đáp ứng nhu cầu này, dự án của chúng tôi hướng đến việc xây dựng một nền tảng trực tuyến cho phép người dùng truy cập miễn phí vào thông tin về những cơn bão mới nhất ở khu vực Tây Thái Bình Dương, sử dụng dữ liệu từ NOAA (Cơ quan Quản lý Khí quyển và Đại dương Quốc gia Hoa Kỳ) — một nguồn dữ liệu đáng tin cậy. Bên cạnh đó, sinh viên, nhà khí tượng hoặc bất kỳ ai quan tâm đến động lực học của bão đều có thể tương tác với hệ thống bằng cách cung cấp quỹ đạo đầu vào của riêng họ và nhận về dự đoán được tạo bởi mô hình học máy của chúng tôi.\nWorkshop này trình bày toàn bộ quy trình xây dựng mô hình dự báo bão, bao gồm nhiều kỹ thuật chuỗi thời gian mới — Stepwise Temporal Fading và Plausible Geodesic-Aware Augmentation — cùng với phần giải thích chi tiết từng bước về cách chúng tôi xây dựng và triển khai nền tảng từ con số 0.\nVới sự hỗ trợ của các dịch vụ AWS như Amazon S3, AWS Lambda, API Gateway và CloudFront, chúng tôi xây dựng một kiến trúc hoàn toàn serverless. Giải pháp này mang lại sự đơn giản, khả năng mở rộng linh hoạt và hiệu quả chi phí dài hạn, đồng thời đảm bảo hiệu suất ổn định và phản hồi nhanh.\nKiến trúc Nền tảng\rNền tảng cuối cùng cung cấp hai chức năng cốt lõi:\nXem Thông Tin Bão Người dùng có thể khám phá thông tin mới nhất về các cơn bão ở Tây Thái Bình Dương, bao gồm quỹ đạo lịch sử, tốc độ gió, nhiệt độ và các thông số liên quan khác.\nDự Đoán Quỹ Đạo Bão Người dùng có thể nhập quỹ đạo một phần của cơn bão và nhận về dự đoán quãng đường tiếp theo được tạo bởi mô hình đã huấn luyện.\nNội dung Tổng quan về workshop Chuẩn bị dữ liệu Kiến tạo mô hình ML Kiến trúc Front\u0026amp;Back-end API "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/5-workshop/5.5-platform-api/",
	"title": "Nền tảng API",
	"tags": [],
	"description": "",
	"content": "MÔ TẢ CHI TIẾT BACK-END API Mục lục Giới thiệu Kiến trúc hệ thống Tính năng cốt lõi Công nghệ sử dụng Cấu trúc dự án API Endpoints 1. Giới thiệu Weather Backend API là một dịch vụ RESTful cung cấp thông tin thời tiết bằng cách tích hợp với OpenWeatherMap API. Backend hoạt động như lớp trung gian giữa ứng dụng frontend và các nguồn dữ liệu thời tiết bên ngoài.\nHình 1\r2. Kiến trúc hệ thống Kiến trúc ┌─────────────────────────────────────────────────────────────┐\r│ Ứng dụng Frontend │\r│ (React, Mobile, Web Clients) │\r└─────────────────────────────────────────────────────────────┘\r│\r│ HTTPS / REST API\r▼\r┌─────────────────────────────────────────────────────────────┐\r│ Weather Backend API │\r│ (.NET 9.0 - ASP.NET Core) │\r├─────────────────────────────────────────────────────────────┤\r│ ┌────────────────┐ ┌────────────────┐ ┌────────────────┐\r│ │ Bộ điều khiển │ │ Dịch vụ │ │ Program.cs │\r│ │ │ │ │ │ - Khởi động app│\r│ │ - WeatherCtrl │ │ - WeatherSvc │ │ - Ghi log │\r│ │ - ForecastCtrl │ │ - Lớp cache │ │ - Thiết lập DI │\r│ └────────────────┘ └────────────────┘ └────────────────┘\r└─────────────────────────────────────────────────────────────┘\r│\r│ HTTPS / REST API (Bên ngoài)\r▼\r┌─────────────────────────────────────────────────────────────┐\r│ Dịch vụ thời tiết bên ngoài │\r├─────────────────────────────────────────────────────────────┤\r│ • OpenWeatherMap API │\r│ • Redis Caching Layer │\r│ • Giới hạn tốc độ \u0026amp; Giám sát │\r└─────────────────────────────────────────────────────────────┘ 3. Tính năng cốt lõi Mô tả: Lấy dữ liệu thời tiết hiện tại của bất kỳ thành phố nào trên thế giới.\nTính năng:\nTìm kiếm theo tên thành phố (ví dụ: \u0026ldquo;Hà Nội\u0026rdquo;, \u0026ldquo;TP Hồ Chí Minh\u0026rdquo;) Tùy chọn mã quốc gia để xác định vị trí chính xác Hỗ trợ nhiều hệ thống đơn vị (metric, imperial, standard) Hỗ trợ đa ngôn ngữ cho phần mô tả thời tiết Phản hồi được lưu cache để tăng hiệu suất Tham số API:\ncityName (bắt buộc): Tên thành phố countryCode (tùy chọn): Mã quốc gia ISO 3166 units (tùy chọn): metric, imperial, standard language (tùy chọn): en, vi, fr, \u0026hellip; Ví dụ phản hồi:\nResponse body Download { \u0026#34;localDate\u0026#34;: \u0026#34;2025-12-06 19:57:02\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Hà Nội\u0026#34;, \u0026#34;coord\u0026#34;: { \u0026#34;lon\u0026#34;: 105.8412, \u0026#34;lat\u0026#34;: 21.0245 }, \u0026#34;weather\u0026#34;: [ { \u0026#34;id\u0026#34;: 804, \u0026#34;main\u0026#34;: \u0026#34;Clouds\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;mây đen u ám\u0026#34;, \u0026#34;icon\u0026#34;: \u0026#34;04n\u0026#34; } ], \u0026#34;main\u0026#34;: { \u0026#34;temp\u0026#34;: 22, \u0026#34;feels_like\u0026#34;: 22.11, \u0026#34;temp_min\u0026#34;: 22, \u0026#34;temp_max\u0026#34;: 22, \u0026#34;pressure\u0026#34;: 1018, \u0026#34;humidity\u0026#34;: 71, \u0026#34;sea_level\u0026#34;: 1018, \u0026#34;grnd_level\u0026#34;: 1017 }, \u0026#34;wind\u0026#34;: { \u0026#34;speed\u0026#34;: 4.14, \u0026#34;deg\u0026#34;: 136, \u0026#34;gust\u0026#34;: 6.84 }, \u0026#34;sys\u0026#34;: { \u0026#34;type\u0026#34;: 1, \u0026#34;id\u0026#34;: 9308, \u0026#34;country\u0026#34;: \u0026#34;VN\u0026#34;, \u0026#34;sunrise\u0026#34;: 1764976827, \u0026#34;sunset\u0026#34;: 1765016103 } } Hình 2\r4. Công nghệ sử dụng Backend Framework .NET 9.0 – Runtime .NET mới nhất ASP.NET Core – Framework xây dựng Web API C# 12 – Ngôn ngữ lập trình chính Tích hợp API HttpClientFactory – Quản lý sử dụng HTTP client Polly – Chính sách thử lại \u0026amp; xử lý lỗi tạm thời Newtonsoft.Json / System.Text.Json – Tuần tự hóa JSON Cache \u0026amp; Hiệu năng MemoryCache – Cache trong bộ nhớ Redis (tùy chọn) – Cache phân tán ResponseCompression – Nén Gzip / Brotli Công cụ phát triển Visual Studio 2022 / VS Code - IDE / Trình soạn thảo mã Swagger / OpenAPI – Tài liệu API Git – Kiểm soát phiên bản Docker – Container hóa 5. Cấu trúc dự án WeatherBackend/ │ ├── WeatherBackend.csproj # Tập tin dự án ├── Program.cs # Điểm vào ứng dụng ├── WeatherBackend.http # Tập tin kiểm tra yêu cầu HTTP │ ├── appsettings.json # Cài đặt cấu hình │ ├── Controllers/ # Bộ điều khiển API │ └── WeatherController.cs # Điểm cuối thời tiết chính │ ├── Services/ # Dịch vụ logic nghiệp vụ │ └── WeatherService/ # Hợp đồng \u0026amp; triển khai dịch vụ 6. API Endpoints Base URL https://localhost:7042/swagger/index.html 6.1 GET /api/weather/current Mô tả:\nLấy dữ liệu thời tiết hiện tại theo tên thành phố.\nVí dụ CURL:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather?city=hanoi\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; URL yêu cầu:\nhttps://localhost:7042/api/Weather?city=hanoi 6.2 GET /api/weather/forecast Mô tả: Lấy dự báo thời tiết 5 ngày cho một thành phố được chọn.\nVí dụ CURL:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/forecast?city=hochiminh\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; URL yêu cầu:\nhttps://localhost:7042/api/Weather/forecast?city=hochiminh 6.3 GET /api/weather/coordinates Mô tả: Lấy thông tin thời tiết bằng vĩ độ và kinh độ.\nVí dụ CURL:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/by-coord?lat=21.0245\u0026amp;lon=105.8412\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; URL yêu cầu:\nhttps://localhost:7042/api/Weather/by-coord?lat=21.0245\u0026amp;lon=105.8412 6.4 GET /api/weather/location Mô tả: Lấy thông tin thời tiết theo vị trí của người dùng (yêu cầu thiết bị người dùng gửi tọa độ).\nVí dụ CURL:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/global\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; URL yêu cầu:\nhttps://localhost:7042/api/Weather/global Hình 3\rCập nhập lần cuối: 2025-12-09\nPhiên bản: 1.0.0\nBảo trì bởi: SKYNET\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/6-self-evaluation/",
	"title": "Tự đánh giá",
	"tags": [],
	"description": "",
	"content": "Trong thời gian thực tập tại Amazon Web Services (AWS) từ 08/09/2025 đến 29/11/2025, tôi có cơ hội học hỏi, thực hành và vận dụng những kiến thức đã được học ở trường vào môi trường làm việc thực tế.\nTôi tham gia dự án dự đoán quỹ đạo bão bằng deep learning (mô hình chuỗi thời gian dựa trên Transformer) và triển khai web theo hướng serverless, qua đó nâng cao kỹ năng về tiền xử lý dữ liệu, feature engineering, mô hình hóa chuỗi thời gian, huấn luyện \u0026amp; fine-tuning, đánh giá mô hình, tích hợp API, kiến thức cơ bản về AWS serverless, viết tài liệu kỹ thuật, và làm việc nhóm/giao tiếp.\nTrong 12 tuần, tôi tiến triển dần từ việc nắm các kiến thức nền tảng về AWS và đọc các nghiên cứu liên quan, đến xây dựng bộ dữ liệu bão và các mô hình baseline, lựa chọn hướng mô hình (LSTM/RNN/Transformer), phát triển prototype, chuẩn bị cho training, chạy huấn luyện đầy đủ và fine-tuning, đánh giá mô hình, và cuối cùng tích hợp mô hình đã huấn luyện vào API để phục vụ dự đoán trên giao diện web.\nTôi cũng cải thiện khả năng viết báo cáo kỹ thuật, xây dựng slide thuyết trình, và tạo runbook phục vụ bàn giao dự án.\nVề tác phong làm việc, tôi luôn cố gắng hoàn thành nhiệm vụ tốt nhất có thể, tuân thủ các quy định nơi làm việc, và chủ động trao đổi với đồng nghiệp để nâng cao hiệu quả công việc.\nĐể phản ánh khách quan quá trình thực tập, tôi tự đánh giá bản thân theo các tiêu chí sau:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức \u0026amp; kỹ năng chuyên môn Mức độ hiểu lĩnh vực, vận dụng kiến thức vào thực tế, thành thạo công cụ, chất lượng công việc ✅ ☐ ☐ 2 Khả năng học hỏi Khả năng tiếp thu kiến thức mới và học nhanh ✅ ☐ ☐ 3 Tính chủ động Chủ động tìm việc và thực hiện nhiệm vụ mà không chờ hướng dẫn ✅ ☐ ☐ 4 Tinh thần trách nhiệm Hoàn thành đúng hạn và đảm bảo chất lượng ✅ ☐ ☐ 5 Tính kỷ luật Tuân thủ lịch trình, quy định và quy trình làm việc ☐ ✅ ☐ 6 Tinh thần cầu tiến Sẵn sàng nhận góp ý và cải thiện bản thân ✅ ☐ ☐ 7 Giao tiếp Trình bày ý tưởng và báo cáo công việc rõ ràng ☐ ☐ ✅ 8 Làm việc nhóm Phối hợp hiệu quả với đồng nghiệp và tham gia hoạt động nhóm ✅ ☐ ☐ 9 Tác phong chuyên nghiệp Tôn trọng đồng nghiệp/đối tác và môi trường làm việc ✅ ☐ ☐ 10 Kỹ năng giải quyết vấn đề Nhận diện vấn đề, đề xuất giải pháp và thể hiện tính sáng tạo ☐ ✅ ☐ 11 Mức độ đóng góp cho dự án/nhóm Hiệu quả công việc, ý tưởng cải tiến, mức độ được nhóm ghi nhận ✅ ☐ ☐ 12 Tổng quan Đánh giá chung về toàn bộ thời gian thực tập ✅ ☐ ☐ Cần cải thiện Cải thiện quản lý thời gian và lập kế hoạch hằng ngày để duy trì năng suất ổn định, tránh dồn việc gấp khi nhiều nhiệm vụ bị chồng chéo. Tự tin hơn trong việc chủ động trao đổi và hỏi rõ yêu cầu từ sớm, đặc biệt khi yêu cầu chưa thật sự rõ ràng. Nâng cao thói quen giao tiếp chuyên nghiệp: cập nhật ngắn gọn, báo cáo trạng thái rõ ràng hơn và chủ động follow-up với các thành viên trong nhóm. "
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/7-feedback/",
	"title": "Chia sẻ, đóng góp ý kiến",
	"tags": [],
	"description": "",
	"content": "Đánh giá tổng quan 1. Môi trường làm việc\nMôi trường làm việc chuyên nghiệp nhưng vẫn thân thiện và dễ hòa nhập. Nhóm duy trì không gian làm việc yên tĩnh, ngăn nắp, giúp tôi tập trung và giữ năng suất tốt. Tôi cũng đánh giá cao sự linh hoạt và cởi mở, nơi thực tập sinh có thể trao đổi thoải mái với các anh/chị nhân sự chính thức. Nếu có thể, việc tổ chức thêm một vài hoạt động mang tính “kết nối nhẹ” (ví dụ: coffee chat ngắn theo nhóm hoặc các buổi internal sharing) sẽ giúp tăng sự gắn kết giữa các thành viên.\n2. Sự hỗ trợ từ Mentor / Team Admin\nTrong suốt kỳ thực tập, tôi nhận được sự hỗ trợ rất tốt từ mentor và đội ngũ admin. Mentor đưa ra định hướng rõ ràng, phản hồi kịp thời và giúp tôi bám sát tiến độ, đồng thời vẫn khuyến khích tôi chủ động tự giải quyết vấn đề. Đội ngũ admin hỗ trợ nhanh chóng và chu đáo, đảm bảo các phần liên quan đến thủ tục, giấy tờ và quy trình nội bộ được xử lý trơn tru để tôi có thể tập trung vào công việc chuyên môn.\n3. Mức độ liên quan của công việc với chuyên ngành\nCác nhiệm vụ được giao có mức độ liên quan cao với chuyên ngành học của tôi, giúp tôi vận dụng kiến thức nền tảng vào bối cảnh thực tế. Bên cạnh đó, công việc cũng giúp tôi tiếp cận thêm các công cụ và quy trình làm việc thực tiễn mà chương trình đại học chưa thể bao quát đầy đủ. Sự kết hợp này giúp tôi kết nối tốt hơn giữa lý thuyết và triển khai, đồng thời hiểu rõ hơn kỳ vọng của môi trường doanh nghiệp.\n4. Cơ hội học hỏi \u0026amp; phát triển kỹ năng\nKỳ thực tập mang lại nhiều cơ hội phát triển không chỉ về kỹ thuật. Tôi học được cách tổ chức công việc có hệ thống hơn, ghi chép/đánh dấu tiến độ và cập nhật trao đổi một cách chuyên nghiệp. Tôi cũng có thêm trải nghiệm phối hợp với nhiều vai trò khác nhau trong nhóm dự án, từ đó cải thiện khả năng thích nghi và làm việc hiệu quả trong môi trường công ty.\n5. Văn hóa công ty \u0026amp; tinh thần đội nhóm\nVăn hóa đội nhóm mang tính hợp tác và tôn trọng. Mọi người làm việc với tinh thần trách nhiệm cao nhưng vẫn gần gũi và sẵn sàng hỗ trợ. Khi phát sinh khó khăn, các thành viên chủ động chia sẻ kiến thức và giúp đỡ, tạo nên một môi trường học tập rất tích cực. Điều này giúp tôi dễ hòa nhập và có thêm động lực để đóng góp.\n6. Chính sách / quyền lợi thực tập\nChính sách thực tập rõ ràng và có tính hỗ trợ. Trợ cấp và sự linh hoạt về thời gian giúp tôi cân bằng giữa học hỏi và hiệu suất làm việc. Tôi cũng thấy rất hữu ích khi thực tập sinh được tiếp cận các tài liệu học tập nội bộ và cơ hội đào tạo, tạo thêm giá trị thiết thực bên cạnh công việc hằng ngày.\n"
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://minhduc123234.github.io/Document-AWS/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]